Testing bot: NonGreedyCachedEntropyWordleBot
Initializing bot (this may take a few minutes for optimized bots)...
Loading letter frequencies...
Loaded letter frequency cache with 26 letters
Loaded cache with 34,411 entropy values
Loaded cache with 44,565,000 feedback patterns
Bot initialization completed in 19.84 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙁𝙇𝙄𝘾𝙆 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜⬜⬜
Actual Info Gain: 3.8615 bits
Posterior entropy: 9.997179480937621
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 9.997179480937621, 'actual_info_gain': 3.8614815037851553, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
1022 candidate words remaining.
Top 10 guesses: [('COLIN', '5.938'), ('NOILY', '5.864'), ('DINLO', '5.841'), ('PIONY', '5.791'), ('LINGO', '5.749'), ('PILON', '5.742'), ('PINOL', '5.704'), ('LOGIN', '5.699'), ('DOILY', '5.681'), ('NICOL', '5.680')]
Top entropy choice: 𝘾𝙊𝙇𝙄𝙉 with entropy: 5.9379
Guess count low, choosing the word with highest entropy: 𝘾𝙊𝙇𝙄𝙉 with entropy: 5.9379

Guess: 𝘾𝙊𝙇𝙄𝙉, Feedback: 🟨⬜🟨🟨⬜
Actual Info Gain: 7.6753 bits
Posterior entropy: 2.321928094887362
entropy_info: {'prior_entropy': 9.997179480937621, 'posterior_entropy': 2.321928094887362, 'actual_info_gain': 7.67525138605026, 'expected_info_gain': 5.937896650379372}
----------
The bot is making a guess...
5 candidate words remaining.
Top 10 guesses: [('BIFID', '2.322'), ('FAUCH', '2.322'), ('FICIN', '2.322'), ('FIERI', '2.322'), ('FIFIS', '2.322'), ('FILCH', '2.322'), ('FILII', '2.322'), ('FILMI', '2.322'), ('FINCH', '2.322'), ('FINIS', '2.322')]
Top entropy choice: 𝘽𝙄𝙁𝙄𝘿 with entropy: 2.3219
Using common exploratory word: 𝙁𝙄𝙉𝘾𝙃 with entropy: 2.3219

Guess: 𝙁𝙄𝙉𝘾𝙃, Feedback: 🟩🟨⬜🟩⬜
Actual Info Gain: 2.3219 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 2.321928094887362, 'posterior_entropy': 0.0, 'actual_info_gain': 2.321928094887362, 'expected_info_gain': 5.937896650379372}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('FLICK', '0.000')]
Top entropy choice: 𝙁𝙇𝙄𝘾𝙆 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝙁𝙇𝙄𝘾𝙆 with entropy: 0.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜⬜⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 3.861 bits, Posterior Entropy: 9.997, 
Guess: 𝘾𝙊𝙇𝙄𝙉, Feedback: 🟨⬜🟨🟨⬜, Prior Entropy: 9.997, Expected Info Gain: 5.938 bits, Actual Info Gain: 7.675 bits, Posterior Entropy: 2.322, 
Guess: 𝙁𝙄𝙉𝘾𝙃, Feedback: 🟩🟨⬜🟩⬜, Prior Entropy: 2.322, Expected Info Gain: 5.938 bits, Actual Info Gain: 2.322 bits, Posterior Entropy: 0.000, 
Guess: 𝙁𝙇𝙄𝘾𝙆, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'FLICK': 31.1340 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙁𝙄𝘽𝙍𝙀 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜🟨🟨⬜
Actual Info Gain: 5.7140 bits
Posterior entropy: 8.144658242831882
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 8.144658242831882, 'actual_info_gain': 5.714002741890894, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
283 candidate words remaining.
Top 10 guesses: [('DEICE', '4.837'), ('PEINE', '4.750'), ('GEODE', '4.744'), ('REINE', '4.730'), ('GENIE', '4.727'), ('RIONE', '4.719'), ('DIENE', '4.706'), ('PENIE', '4.690'), ('NIECE', '4.628'), ('REGIE', '4.628')]
Top entropy choice: 𝘿𝙀𝙄𝘾𝙀 with entropy: 4.8373
Guess count low, choosing the word with highest entropy: 𝘿𝙀𝙄𝘾𝙀 with entropy: 4.8373

Guess: 𝘿𝙀𝙄𝘾𝙀, Feedback: ⬜⬜🟨⬜🟩
Actual Info Gain: 4.4442 bits
Posterior entropy: 3.700439718141092
entropy_info: {'prior_entropy': 8.144658242831882, 'posterior_entropy': 3.700439718141092, 'actual_info_gain': 4.44421852469079, 'expected_info_gain': 4.8373180440628385}
----------
The bot is making a guess...
13 candidate words remaining.
Top 10 guesses: [('FILON', '3.393'), ('DINLO', '3.239'), ('FONLY', '3.239'), ('INORB', '3.239'), ('LINGO', '3.239'), ('LINOS', '3.239'), ('NOBLE', '3.239'), ('NOBLY', '3.239'), ('RIGOL', '3.239'), ('FRORN', '3.181')]
Top entropy choice: 𝙁𝙄𝙇𝙊𝙉 with entropy: 3.3927
Using common exploratory word: 𝙇𝙄𝙉𝙂𝙊 with entropy: 3.2389

Guess: 𝙇𝙄𝙉𝙂𝙊, Feedback: ⬜🟩⬜⬜⬜
Actual Info Gain: 2.7004 bits
Posterior entropy: 1.0
entropy_info: {'prior_entropy': 3.700439718141092, 'posterior_entropy': 1.0, 'actual_info_gain': 2.700439718141092, 'expected_info_gain': 4.8373180440628385}
----------
The bot is making a guess...
2 candidate words remaining.
Top 10 guesses: [('FIBRE', '1.000'), ('VIVRE', '1.000')]
Top entropy choice: 𝙁𝙄𝘽𝙍𝙀 with entropy: 1.0000
Guess count high, choosing a candidate with the highest entropy: 𝙁𝙄𝘽𝙍𝙀 with entropy: 1.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜🟨🟨⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 5.714 bits, Posterior Entropy: 8.145, 
Guess: 𝘿𝙀𝙄𝘾𝙀, Feedback: ⬜⬜🟨⬜🟩, Prior Entropy: 8.145, Expected Info Gain: 4.837 bits, Actual Info Gain: 4.444 bits, Posterior Entropy: 3.700, 
Guess: 𝙇𝙄𝙉𝙂𝙊, Feedback: ⬜🟩⬜⬜⬜, Prior Entropy: 3.700, Expected Info Gain: 4.837 bits, Actual Info Gain: 2.700 bits, Posterior Entropy: 1.000, 
Guess: 𝙁𝙄𝘽𝙍𝙀, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'FIBRE': 8.9434 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙇𝘼𝙎𝙀𝙍 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟩🟨🟩🟨
Actual Info Gain: 9.8587 bits
Posterior entropy: 4.0
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 4.0, 'actual_info_gain': 9.858660984722777, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
16 candidate words remaining.
Top 10 guesses: [('BASKS', '2.061'), ('BASSY', '2.061'), ('BISKS', '2.061'), ('BOSKS', '2.061'), ('BOSSY', '2.061'), ('BUSKS', '2.061'), ('BYSSI', '2.061'), ('SNABS', '2.061'), ('SNEBS', '2.061'), ('SNIBS', '2.061')]
Top entropy choice: 𝘽𝘼𝙎𝙆𝙎 with entropy: 2.0613
Guess count low, choosing the word with highest entropy: 𝘽𝘼𝙎𝙆𝙎 with entropy: 2.0613

Guess: 𝘽𝘼𝙎𝙆𝙎, Feedback: ⬜🟩🟩⬜⬜
Actual Info Gain: 1.4150 bits
Posterior entropy: 2.584962500721156
entropy_info: {'prior_entropy': 4.0, 'posterior_entropy': 2.584962500721156, 'actual_info_gain': 1.415037499278844, 'expected_info_gain': 2.061278124459133}
----------
The bot is making a guess...
6 candidate words remaining.
Top 10 guesses: [('LEMED', '2.252'), ('MEDLE', '2.252'), ('MERER', '2.252'), ('RAMAL', '1.918'), ('RAMEE', '1.918'), ('REALM', '1.918'), ('REAME', '1.918'), ('REBEC', '1.918'), ('REBEL', '1.918'), ('RECAL', '1.918')]
Top entropy choice: 𝙇𝙀𝙈𝙀𝘿 with entropy: 2.2516
Using common exploratory word: 𝙍𝙀𝘼𝙇𝙈 with entropy: 1.9183

Guess: 𝙍𝙀𝘼𝙇𝙈, Feedback: 🟨🟨🟨🟨⬜
Actual Info Gain: 2.5850 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 2.584962500721156, 'posterior_entropy': 0.0, 'actual_info_gain': 2.584962500721156, 'expected_info_gain': 2.061278124459133}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('LASER', '0.000')]
Top entropy choice: 𝙇𝘼𝙎𝙀𝙍 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝙇𝘼𝙎𝙀𝙍 with entropy: 0.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟩🟨🟩🟨, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 9.859 bits, Posterior Entropy: 4.000, 
Guess: 𝘽𝘼𝙎𝙆𝙎, Feedback: ⬜🟩🟩⬜⬜, Prior Entropy: 4.000, Expected Info Gain: 2.061 bits, Actual Info Gain: 1.415 bits, Posterior Entropy: 2.585, 
Guess: 𝙍𝙀𝘼𝙇𝙈, Feedback: 🟨🟨🟨🟨⬜, Prior Entropy: 2.585, Expected Info Gain: 2.061 bits, Actual Info Gain: 2.585 bits, Posterior Entropy: 0.000, 
Guess: 𝙇𝘼𝙎𝙀𝙍, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'LASER': 1.4198 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙁𝙇𝙀𝘾𝙆 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜🟨⬜
Actual Info Gain: 4.2869 bits
Posterior entropy: 9.571752643503546
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 9.571752643503546, 'actual_info_gain': 4.286908341219231, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
761 candidate words remaining.
Top 10 guesses: [('DINLO', '5.536'), ('COLIN', '5.491'), ('DOLIE', '5.487'), ('NOILY', '5.417'), ('MOILE', '5.403'), ('OLDIE', '5.394'), ('DOILY', '5.388'), ('MOLIE', '5.386'), ('LEONE', '5.379'), ('LOGIN', '5.371')]
Top entropy choice: 𝘿𝙄𝙉𝙇𝙊 with entropy: 5.5362
Guess count low, choosing the word with highest entropy: 𝘿𝙄𝙉𝙇𝙊 with entropy: 5.5362

Guess: 𝘿𝙄𝙉𝙇𝙊, Feedback: ⬜⬜⬜🟨⬜
Actual Info Gain: 4.1123 bits
Posterior entropy: 5.459431618637297
entropy_info: {'prior_entropy': 9.571752643503546, 'posterior_entropy': 5.459431618637297, 'actual_info_gain': 4.112321024866248, 'expected_info_gain': 5.536189971336713}
----------
The bot is making a guess...
44 candidate words remaining.
Top 10 guesses: [('PECKE', '4.268'), ('PHEME', '4.231'), ('BECKE', '4.134'), ('BEEFY', '4.125'), ('GEEKY', '4.115'), ('MEECH', '4.102'), ('CLEPE', '4.088'), ('PLEBE', '4.077'), ('PEEKY', '4.070'), ('GULPH', '4.061')]
Top entropy choice: 𝙋𝙀𝘾𝙆𝙀 with entropy: 4.2676
Using common exploratory word: 𝘽𝙀𝙀𝙁𝙔 with entropy: 4.1247

Guess: 𝘽𝙀𝙀𝙁𝙔, Feedback: ⬜⬜🟩🟨⬜
Actual Info Gain: 5.4594 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 5.459431618637297, 'posterior_entropy': 0.0, 'actual_info_gain': 5.459431618637297, 'expected_info_gain': 5.536189971336713}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('FLECK', '0.000')]
Top entropy choice: 𝙁𝙇𝙀𝘾𝙆 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝙁𝙇𝙀𝘾𝙆 with entropy: 0.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜🟨⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 4.287 bits, Posterior Entropy: 9.572, 
Guess: 𝘿𝙄𝙉𝙇𝙊, Feedback: ⬜⬜⬜🟨⬜, Prior Entropy: 9.572, Expected Info Gain: 5.536 bits, Actual Info Gain: 4.112 bits, Posterior Entropy: 5.459, 
Guess: 𝘽𝙀𝙀𝙁𝙔, Feedback: ⬜⬜🟩🟨⬜, Prior Entropy: 5.459, Expected Info Gain: 5.536 bits, Actual Info Gain: 5.459 bits, Posterior Entropy: 0.000, 
Guess: 𝙁𝙇𝙀𝘾𝙆, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'FLECK': 25.1293 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙀𝙈𝘽𝙀𝘿 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜🟩⬜
Actual Info Gain: 4.9914 bits
Posterior entropy: 8.867278739709661
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 8.867278739709661, 'actual_info_gain': 4.991382245013115, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
467 candidate words remaining.
Top 10 guesses: [('INDOL', '4.933'), ('DINLO', '4.859'), ('LOWND', '4.836'), ('BLOND', '4.832'), ('LOUND', '4.773'), ('LINDY', '4.749'), ('DOLCI', '4.730'), ('DOILY', '4.707'), ('NOULD', '4.703'), ('MODIN', '4.688')]
Top entropy choice: 𝙄𝙉𝘿𝙊𝙇 with entropy: 4.9332
Guess count low, choosing the word with highest entropy: 𝙄𝙉𝘿𝙊𝙇 with entropy: 4.9332

Guess: 𝙄𝙉𝘿𝙊𝙇, Feedback: ⬜⬜🟨⬜⬜
Actual Info Gain: 3.3437 bits
Posterior entropy: 5.523561956057013
entropy_info: {'prior_entropy': 8.867278739709661, 'posterior_entropy': 5.523561956057013, 'actual_info_gain': 3.3437167836526482, 'expected_info_gain': 4.933171607708794}
----------
The bot is making a guess...
46 candidate words remaining.
Top 10 guesses: [('DUMKY', '3.769'), ('BUMPY', '3.726'), ('KEMPY', '3.704'), ('DUMPY', '3.663'), ('PEEKY', '3.658'), ('HUMPY', '3.645'), ('MUCKY', '3.536'), ('GEEKY', '3.491'), ('DUCKY', '3.445'), ('HEMPY', '3.443')]
Top entropy choice: 𝘿𝙐𝙈𝙆𝙔 with entropy: 3.7687
Using common exploratory word: 𝘽𝙐𝙈𝙋𝙔 with entropy: 3.7264

Guess: 𝘽𝙐𝙈𝙋𝙔, Feedback: 🟨⬜🟨⬜⬜
Actual Info Gain: 5.5236 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 5.523561956057013, 'posterior_entropy': 0.0, 'actual_info_gain': 5.523561956057013, 'expected_info_gain': 4.933171607708794}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('EMBED', '0.000')]
Top entropy choice: 𝙀𝙈𝘽𝙀𝘿 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝙀𝙈𝘽𝙀𝘿 with entropy: 0.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜🟩⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 4.991 bits, Posterior Entropy: 8.867, 
Guess: 𝙄𝙉𝘿𝙊𝙇, Feedback: ⬜⬜🟨⬜⬜, Prior Entropy: 8.867, Expected Info Gain: 4.933 bits, Actual Info Gain: 3.344 bits, Posterior Entropy: 5.524, 
Guess: 𝘽𝙐𝙈𝙋𝙔, Feedback: 🟨⬜🟨⬜⬜, Prior Entropy: 5.524, Expected Info Gain: 4.933 bits, Actual Info Gain: 5.524 bits, Posterior Entropy: 0.000, 
Guess: 𝙀𝙈𝘽𝙀𝘿, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'EMBED': 16.1753 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙇𝘼𝙐𝙍𝘼 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟩🟨⬜⬜
Actual Info Gain: 6.9398 bits
Posterior entropy: 6.918863237274595
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 6.918863237274595, 'actual_info_gain': 6.939797747448182, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
121 candidate words remaining.
Top 10 guesses: [('RINDY', '4.228'), ('RYPIN', '4.212'), ('KYLIN', '4.200'), ('ROBIN', '4.135'), ('MODIN', '4.120'), ('LINDY', '4.119'), ('RUBIN', '4.087'), ('RANID', '4.047'), ('UNLID', '4.029'), ('FLYIN', '4.013')]
Top entropy choice: 𝙍𝙄𝙉𝘿𝙔 with entropy: 4.2285
Guess count low, choosing the word with highest entropy: 𝙍𝙄𝙉𝘿𝙔 with entropy: 4.2285

Guess: 𝙍𝙄𝙉𝘿𝙔, Feedback: 🟨⬜⬜⬜⬜
Actual Info Gain: 2.1640 bits
Posterior entropy: 4.754887502163468
entropy_info: {'prior_entropy': 6.918863237274595, 'posterior_entropy': 4.754887502163468, 'actual_info_gain': 2.1639757351111264, 'expected_info_gain': 4.2284561813662584}
----------
The bot is making a guess...
27 candidate words remaining.
Top 10 guesses: [('LABRA', '3.736'), ('BOLAR', '3.726'), ('LOBAR', '3.726'), ('KALAM', '3.690'), ('LUBRA', '3.680'), ('KAVAL', '3.648'), ('KALPA', '3.634'), ('GULAB', '3.630'), ('MALVA', '3.616'), ('LABOR', '3.606')]
Top entropy choice: 𝙇𝘼𝘽𝙍𝘼 with entropy: 3.7360
Using common exploratory word: 𝙇𝘼𝘽𝙊𝙍 with entropy: 3.6060

Guess: 𝙇𝘼𝘽𝙊𝙍, Feedback: 🟩🟩⬜⬜🟨
Actual Info Gain: 3.1699 bits
Posterior entropy: 1.584962500721156
entropy_info: {'prior_entropy': 4.754887502163468, 'posterior_entropy': 1.584962500721156, 'actual_info_gain': 3.169925001442312, 'expected_info_gain': 4.2284561813662584}
----------
The bot is making a guess...
3 candidate words remaining.
Top 10 guesses: [('LAURA', '1.585'), ('LAVRA', '1.585'), ('LAARF', '0.918')]
Top entropy choice: 𝙇𝘼𝙐𝙍𝘼 with entropy: 1.5850
Guess count high, choosing a candidate with the highest entropy: 𝙇𝘼𝙐𝙍𝘼 with entropy: 1.5850
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟩🟨⬜⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 6.940 bits, Posterior Entropy: 6.919, 
Guess: 𝙍𝙄𝙉𝘿𝙔, Feedback: 🟨⬜⬜⬜⬜, Prior Entropy: 6.919, Expected Info Gain: 4.228 bits, Actual Info Gain: 2.164 bits, Posterior Entropy: 4.755, 
Guess: 𝙇𝘼𝘽𝙊𝙍, Feedback: 🟩🟩⬜⬜🟨, Prior Entropy: 4.755, Expected Info Gain: 4.228 bits, Actual Info Gain: 3.170 bits, Posterior Entropy: 1.585, 
Guess: 𝙇𝘼𝙐𝙍𝘼, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'LAURA': 4.5625 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙆𝙉𝙀𝙇𝙏 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟨⬜⬜🟨⬜
Actual Info Gain: 6.3509 bits
Posterior entropy: 7.507794640198696
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 7.507794640198696, 'actual_info_gain': 6.35086634452408, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
182 candidate words remaining.
Top 10 guesses: [('ELITE', '4.694'), ('LEONE', '4.690'), ('HELIO', '4.644'), ('LOCIE', '4.643'), ('LITHE', '4.629'), ('LENTI', '4.618'), ('BELIE', '4.614'), ('LINTY', '4.598'), ('FLITT', '4.596'), ('LOIPE', '4.588')]
Top entropy choice: 𝙀𝙇𝙄𝙏𝙀 with entropy: 4.6939
Guess count low, choosing the word with highest entropy: 𝙀𝙇𝙄𝙏𝙀 with entropy: 4.6939

Guess: 𝙀𝙇𝙄𝙏𝙀, Feedback: 🟨🟨⬜🟨⬜
Actual Info Gain: 3.9228 bits
Posterior entropy: 3.584962500721156
entropy_info: {'prior_entropy': 7.507794640198696, 'posterior_entropy': 3.584962500721156, 'actual_info_gain': 3.9228321394775403, 'expected_info_gain': 4.69389435581856}
----------
The bot is making a guess...
12 candidate words remaining.
Top 10 guesses: [('DOLLY', '3.252'), ('HOWDY', '3.252'), ('FOLKY', '3.189'), ('MOLDY', '3.189'), ('DECKO', '3.085'), ('DHOLL', '3.085'), ('DOOKY', '3.085'), ('DORKY', '3.085'), ('KYUDO', '3.085'), ('WILCO', '3.085')]
Top entropy choice: 𝘿𝙊𝙇𝙇𝙔 with entropy: 3.2516
Using common exploratory word: 𝘿𝙊𝙇𝙇𝙔 with entropy: 3.2516

Guess: 𝘿𝙊𝙇𝙇𝙔, Feedback: ⬜⬜⬜🟩⬜
Actual Info Gain: 3.5850 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 3.584962500721156, 'posterior_entropy': 0.0, 'actual_info_gain': 3.584962500721156, 'expected_info_gain': 4.69389435581856}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('KNELT', '0.000')]
Top entropy choice: 𝙆𝙉𝙀𝙇𝙏 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝙆𝙉𝙀𝙇𝙏 with entropy: 0.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟨⬜⬜🟨⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 6.351 bits, Posterior Entropy: 7.508, 
Guess: 𝙀𝙇𝙄𝙏𝙀, Feedback: 🟨🟨⬜🟨⬜, Prior Entropy: 7.508, Expected Info Gain: 4.694 bits, Actual Info Gain: 3.923 bits, Posterior Entropy: 3.585, 
Guess: 𝘿𝙊𝙇𝙇𝙔, Feedback: ⬜⬜⬜🟩⬜, Prior Entropy: 3.585, Expected Info Gain: 4.694 bits, Actual Info Gain: 3.585 bits, Posterior Entropy: 0.000, 
Guess: 𝙆𝙉𝙀𝙇𝙏, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'KNELT': 6.2107 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙋𝙊𝙐𝘾𝙃 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜⬜⬜
Actual Info Gain: 3.8615 bits
Posterior entropy: 9.997179480937621
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 9.997179480937621, 'actual_info_gain': 3.8614815037851553, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
1022 candidate words remaining.
Top 10 guesses: [('COLIN', '5.938'), ('NOILY', '5.864'), ('DINLO', '5.841'), ('PIONY', '5.791'), ('LINGO', '5.749'), ('PILON', '5.742'), ('PINOL', '5.704'), ('LOGIN', '5.699'), ('DOILY', '5.681'), ('NICOL', '5.680')]
Top entropy choice: 𝘾𝙊𝙇𝙄𝙉 with entropy: 5.9379
Guess count low, choosing the word with highest entropy: 𝘾𝙊𝙇𝙄𝙉 with entropy: 5.9379

Guess: 𝘾𝙊𝙇𝙄𝙉, Feedback: 🟨🟩⬜⬜⬜
Actual Info Gain: 6.0903 bits
Posterior entropy: 3.9068905956085187
entropy_info: {'prior_entropy': 9.997179480937621, 'posterior_entropy': 3.9068905956085187, 'actual_info_gain': 6.090288885329103, 'expected_info_gain': 5.937896650379372}
----------
The bot is making a guess...
15 candidate words remaining.
Top 10 guesses: [('OOMPH', '3.640'), ('HUMPY', '3.507'), ('BUMPH', '3.323'), ('HUMPH', '3.323'), ('MOOCH', '3.323'), ('MUMPH', '3.323'), ('PHOOH', '3.323'), ('SUMPH', '3.323'), ('UMPHS', '3.323'), ('JUMPY', '3.240')]
Top entropy choice: 𝙊𝙊𝙈𝙋𝙃 with entropy: 3.6402
Using common exploratory word: 𝙊𝙊𝙈𝙋𝙃 with entropy: 3.6402

Guess: 𝙊𝙊𝙈𝙋𝙃, Feedback: ⬜🟩⬜🟨🟩
Actual Info Gain: 3.9069 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 3.9068905956085187, 'posterior_entropy': 0.0, 'actual_info_gain': 3.9068905956085187, 'expected_info_gain': 5.937896650379372}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('POUCH', '0.000')]
Top entropy choice: 𝙋𝙊𝙐𝘾𝙃 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝙋𝙊𝙐𝘾𝙃 with entropy: 0.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜⬜⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 3.861 bits, Posterior Entropy: 9.997, 
Guess: 𝘾𝙊𝙇𝙄𝙉, Feedback: 🟨🟩⬜⬜⬜, Prior Entropy: 9.997, Expected Info Gain: 5.938 bits, Actual Info Gain: 6.090 bits, Posterior Entropy: 3.907, 
Guess: 𝙊𝙊𝙈𝙋𝙃, Feedback: ⬜🟩⬜🟨🟩, Prior Entropy: 3.907, Expected Info Gain: 5.938 bits, Actual Info Gain: 3.907 bits, Posterior Entropy: 0.000, 
Guess: 𝙋𝙊𝙐𝘾𝙃, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'POUCH': 31.6725 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝘿𝙀𝙈𝙊𝙉 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜🟨⬜
Actual Info Gain: 4.2869 bits
Posterior entropy: 9.571752643503546
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 9.571752643503546, 'actual_info_gain': 4.286908341219231, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
761 candidate words remaining.
Top 10 guesses: [('DINLO', '5.536'), ('COLIN', '5.491'), ('DOLIE', '5.487'), ('NOILY', '5.417'), ('MOILE', '5.403'), ('OLDIE', '5.394'), ('DOILY', '5.388'), ('MOLIE', '5.386'), ('LEONE', '5.379'), ('LOGIN', '5.371')]
Top entropy choice: 𝘿𝙄𝙉𝙇𝙊 with entropy: 5.5362
Guess count low, choosing the word with highest entropy: 𝘿𝙄𝙉𝙇𝙊 with entropy: 5.5362

Guess: 𝘿𝙄𝙉𝙇𝙊, Feedback: 🟩⬜🟨⬜🟨
Actual Info Gain: 8.5718 bits
Posterior entropy: 1.0
entropy_info: {'prior_entropy': 9.571752643503546, 'posterior_entropy': 1.0, 'actual_info_gain': 8.571752643503546, 'expected_info_gain': 5.536189971336713}
----------
The bot is making a guess...
2 candidate words remaining.
Top 10 guesses: [('DEMON', '1.000'), ('DEVON', '1.000')]
Top entropy choice: 𝘿𝙀𝙈𝙊𝙉 with entropy: 1.0000
Few candidates left, going through them all to pick a common word...
Using common word: 𝘿𝙀𝙈𝙊𝙉 with entropy: 1.0000
You won! Amount of guesses: 3

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜🟨⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 4.287 bits, Posterior Entropy: 9.572, 
Guess: 𝘿𝙄𝙉𝙇𝙊, Feedback: 🟩⬜🟨⬜🟨, Prior Entropy: 9.572, Expected Info Gain: 5.536 bits, Actual Info Gain: 8.572 bits, Posterior Entropy: 1.000, 
Guess: 𝘿𝙀𝙈𝙊𝙉, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'DEMON': 23.8937 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙇𝙐𝙍𝙄𝘿 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜🟩⬜⬜
Actual Info Gain: 6.7926 bits
Posterior entropy: 7.066089190457772
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 7.066089190457772, 'actual_info_gain': 6.792571794265005, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
134 candidate words remaining.
Top 10 guesses: [('GOYIM', '4.762'), ('DINGO', '4.689'), ('MODIN', '4.675'), ('DINLO', '4.673'), ('MUGIL', '4.672'), ('MIGOD', '4.666'), ('LINGO', '4.633'), ('MUCID', '4.620'), ('LOGIN', '4.605'), ('YOGIN', '4.602')]
Top entropy choice: 𝙂𝙊𝙔𝙄𝙈 with entropy: 4.7622
Guess count low, choosing the word with highest entropy: 𝙂𝙊𝙔𝙄𝙈 with entropy: 4.7622

Guess: 𝙂𝙊𝙔𝙄𝙈, Feedback: ⬜⬜⬜🟩⬜
Actual Info Gain: 4.2587 bits
Posterior entropy: 2.807354922057604
entropy_info: {'prior_entropy': 7.066089190457772, 'posterior_entropy': 2.807354922057604, 'actual_info_gain': 4.258734268400168, 'expected_info_gain': 4.762246063934135}
----------
The bot is making a guess...
7 candidate words remaining.
Top 10 guesses: [('BIDON', '2.807'), ('BUDAS', '2.807'), ('BUDDY', '2.807'), ('BUDED', '2.807'), ('BUDES', '2.807'), ('BUDGE', '2.807'), ('BUDIS', '2.807'), ('BUDOS', '2.807'), ('BUILD', '2.807'), ('BUMPH', '2.807')]
Top entropy choice: 𝘽𝙄𝘿𝙊𝙉 with entropy: 2.8074
Using common exploratory word: 𝘽𝙐𝘿𝘿𝙔 with entropy: 2.8074

Guess: 𝘽𝙐𝘿𝘿𝙔, Feedback: ⬜🟩🟨⬜⬜
Actual Info Gain: 2.8074 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 2.807354922057604, 'posterior_entropy': 0.0, 'actual_info_gain': 2.807354922057604, 'expected_info_gain': 4.762246063934135}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('LURID', '0.000')]
Top entropy choice: 𝙇𝙐𝙍𝙄𝘿 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝙇𝙐𝙍𝙄𝘿 with entropy: 0.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜🟩⬜⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 6.793 bits, Posterior Entropy: 7.066, 
Guess: 𝙂𝙊𝙔𝙄𝙈, Feedback: ⬜⬜⬜🟩⬜, Prior Entropy: 7.066, Expected Info Gain: 4.762 bits, Actual Info Gain: 4.259 bits, Posterior Entropy: 2.807, 
Guess: 𝘽𝙐𝘿𝘿𝙔, Feedback: ⬜🟩🟨⬜⬜, Prior Entropy: 2.807, Expected Info Gain: 4.762 bits, Actual Info Gain: 2.807 bits, Posterior Entropy: 0.000, 
Guess: 𝙇𝙐𝙍𝙄𝘿, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'LURID': 4.6662 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙁𝙀𝙑𝙀𝙍 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜🟨🟩⬜
Actual Info Gain: 5.5966 bits
Posterior entropy: 8.262094845370179
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 8.262094845370179, 'actual_info_gain': 5.596566139352598, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
307 candidate words remaining.
Top 10 guesses: [('POIND', '4.424'), ('DINLO', '4.295'), ('NIDOR', '4.259'), ('BIPOD', '4.172'), ('PINOL', '4.170'), ('PILON', '4.152'), ('PRIOR', '4.152'), ('VIOLD', '4.149'), ('INDOL', '4.122'), ('BIDON', '4.115')]
Top entropy choice: 𝙋𝙊𝙄𝙉𝘿 with entropy: 4.4242
Guess count low, choosing the word with highest entropy: 𝙋𝙊𝙄𝙉𝘿 with entropy: 4.4242

Guess: 𝙋𝙊𝙄𝙉𝘿, Feedback: ⬜⬜⬜⬜⬜
Actual Info Gain: 2.3079 bits
Posterior entropy: 5.954196310386875
entropy_info: {'prior_entropy': 8.262094845370179, 'posterior_entropy': 5.954196310386875, 'actual_info_gain': 2.3078985349833037, 'expected_info_gain': 4.424196059405291}
----------
The bot is making a guess...
62 candidate words remaining.
Top 10 guesses: [('ELEGY', '4.058'), ('GEMEL', '4.005'), ('GLEBE', '3.992'), ('FUGLY', '3.959'), ('GEBUR', '3.953'), ('BULGY', '3.942'), ('BEEFY', '3.927'), ('FLEER', '3.918'), ('LEGER', '3.912'), ('BEVEL', '3.901')]
Top entropy choice: 𝙀𝙇𝙀𝙂𝙔 with entropy: 4.0580
Using common exploratory word: 𝙀𝙇𝙀𝙂𝙔 with entropy: 4.0580

Guess: 𝙀𝙇𝙀𝙂𝙔, Feedback: 🟨⬜🟨⬜⬜
Actual Info Gain: 2.4948 bits
Posterior entropy: 3.4594316186372973
entropy_info: {'prior_entropy': 5.954196310386875, 'posterior_entropy': 3.4594316186372973, 'actual_info_gain': 2.4947646917495776, 'expected_info_gain': 4.424196059405291}
----------
The bot is making a guess...
11 candidate words remaining.
Top 10 guesses: [('WHIRR', '2.845'), ('FIBER', '2.664'), ('FUBAR', '2.664'), ('RHOMB', '2.664'), ('RHUMB', '2.664'), ('FAVER', '2.595'), ('FAVOR', '2.595'), ('FEVER', '2.595'), ('FEWER', '2.595'), ('FIVER', '2.595')]
Top entropy choice: 𝙒𝙃𝙄𝙍𝙍 with entropy: 2.8454
Guess count high, choosing a candidate with the highest entropy: 𝙁𝙀𝙑𝙀𝙍 with entropy: 2.5949
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜🟨🟩⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 5.597 bits, Posterior Entropy: 8.262, 
Guess: 𝙋𝙊𝙄𝙉𝘿, Feedback: ⬜⬜⬜⬜⬜, Prior Entropy: 8.262, Expected Info Gain: 4.424 bits, Actual Info Gain: 2.308 bits, Posterior Entropy: 5.954, 
Guess: 𝙀𝙇𝙀𝙂𝙔, Feedback: 🟨⬜🟨⬜⬜, Prior Entropy: 5.954, Expected Info Gain: 4.424 bits, Actual Info Gain: 2.495 bits, Posterior Entropy: 3.459, 
Guess: 𝙁𝙀𝙑𝙀𝙍, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'FEVER': 11.0246 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝘿𝙊𝙋𝙀𝘿 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜🟩⬜
Actual Info Gain: 4.9914 bits
Posterior entropy: 8.867278739709661
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 8.867278739709661, 'actual_info_gain': 4.991382245013115, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
467 candidate words remaining.
Top 10 guesses: [('INDOL', '4.933'), ('DINLO', '4.859'), ('LOWND', '4.836'), ('BLOND', '4.832'), ('LOUND', '4.773'), ('LINDY', '4.749'), ('DOLCI', '4.730'), ('DOILY', '4.707'), ('NOULD', '4.703'), ('MODIN', '4.688')]
Top entropy choice: 𝙄𝙉𝘿𝙊𝙇 with entropy: 4.9332
Guess count low, choosing the word with highest entropy: 𝙄𝙉𝘿𝙊𝙇 with entropy: 4.9332

Guess: 𝙄𝙉𝘿𝙊𝙇, Feedback: ⬜⬜🟨🟨⬜
Actual Info Gain: 3.0859 bits
Posterior entropy: 5.78135971352466
entropy_info: {'prior_entropy': 8.867278739709661, 'posterior_entropy': 5.78135971352466, 'actual_info_gain': 3.0859190261850014, 'expected_info_gain': 4.933171607708794}
----------
The bot is making a guess...
55 candidate words remaining.
Top 10 guesses: [('POOHY', '3.054'), ('WOOPY', '2.988'), ('POOCH', '2.971'), ('POBOY', '2.967'), ('WHOMP', '2.965'), ('OOMPH', '2.944'), ('POCHO', '2.912'), ('WOMBY', '2.893'), ('POOVY', '2.876'), ('CHOMP', '2.868')]
Top entropy choice: 𝙋𝙊𝙊𝙃𝙔 with entropy: 3.0538
Using common exploratory word: 𝙋𝙊𝙊𝘾𝙃 with entropy: 2.9710

Guess: 𝙋𝙊𝙊𝘾𝙃, Feedback: 🟨🟩⬜⬜⬜
Actual Info Gain: 4.1964 bits
Posterior entropy: 1.584962500721156
entropy_info: {'prior_entropy': 5.78135971352466, 'posterior_entropy': 1.584962500721156, 'actual_info_gain': 4.196397212803504, 'expected_info_gain': 4.933171607708794}
----------
The bot is making a guess...
3 candidate words remaining.
Top 10 guesses: [('DOPED', '1.585'), ('DOPEY', '1.585'), ('MOPED', '1.585')]
Top entropy choice: 𝘿𝙊𝙋𝙀𝘿 with entropy: 1.5850
Guess count high, choosing a candidate with the highest entropy: 𝘿𝙊𝙋𝙀𝘿 with entropy: 1.5850
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜🟩⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 4.991 bits, Posterior Entropy: 8.867, 
Guess: 𝙄𝙉𝘿𝙊𝙇, Feedback: ⬜⬜🟨🟨⬜, Prior Entropy: 8.867, Expected Info Gain: 4.933 bits, Actual Info Gain: 3.086 bits, Posterior Entropy: 5.781, 
Guess: 𝙋𝙊𝙊𝘾𝙃, Feedback: 🟨🟩⬜⬜⬜, Prior Entropy: 5.781, Expected Info Gain: 4.933 bits, Actual Info Gain: 4.196 bits, Posterior Entropy: 1.585, 
Guess: 𝘿𝙊𝙋𝙀𝘿, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'DOPED': 16.4004 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙒𝙊𝙒𝙀𝘿 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜🟩⬜
Actual Info Gain: 4.9914 bits
Posterior entropy: 8.867278739709661
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 8.867278739709661, 'actual_info_gain': 4.991382245013115, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
467 candidate words remaining.
Top 10 guesses: [('INDOL', '4.933'), ('DINLO', '4.859'), ('LOWND', '4.836'), ('BLOND', '4.832'), ('LOUND', '4.773'), ('LINDY', '4.749'), ('DOLCI', '4.730'), ('DOILY', '4.707'), ('NOULD', '4.703'), ('MODIN', '4.688')]
Top entropy choice: 𝙄𝙉𝘿𝙊𝙇 with entropy: 4.9332
Guess count low, choosing the word with highest entropy: 𝙄𝙉𝘿𝙊𝙇 with entropy: 4.9332

Guess: 𝙄𝙉𝘿𝙊𝙇, Feedback: ⬜⬜🟨🟨⬜
Actual Info Gain: 3.0859 bits
Posterior entropy: 5.78135971352466
entropy_info: {'prior_entropy': 8.867278739709661, 'posterior_entropy': 5.78135971352466, 'actual_info_gain': 3.0859190261850014, 'expected_info_gain': 4.933171607708794}
----------
The bot is making a guess...
55 candidate words remaining.
Top 10 guesses: [('POOHY', '3.054'), ('WOOPY', '2.988'), ('POOCH', '2.971'), ('POBOY', '2.967'), ('WHOMP', '2.965'), ('OOMPH', '2.944'), ('POCHO', '2.912'), ('WOMBY', '2.893'), ('POOVY', '2.876'), ('CHOMP', '2.868')]
Top entropy choice: 𝙋𝙊𝙊𝙃𝙔 with entropy: 3.0538
Using common exploratory word: 𝙋𝙊𝙊𝘾𝙃 with entropy: 2.9710

Guess: 𝙋𝙊𝙊𝘾𝙃, Feedback: ⬜🟩⬜⬜⬜
Actual Info Gain: 1.1964 bits
Posterior entropy: 4.584962500721156
entropy_info: {'prior_entropy': 5.78135971352466, 'posterior_entropy': 4.584962500721156, 'actual_info_gain': 1.1963972128035039, 'expected_info_gain': 4.933171607708794}
----------
The bot is making a guess...
24 candidate words remaining.
Top 10 guesses: [('WOMBY', '3.376'), ('JUMBY', '3.022'), ('DWAMY', '3.011'), ('MAWKY', '2.918'), ('WEBBY', '2.813'), ('WIMPY', '2.667'), ('BAWDY', '2.663'), ('BAWTY', '2.663'), ('BEWDY', '2.663'), ('BLOWY', '2.663')]
Top entropy choice: 𝙒𝙊𝙈𝘽𝙔 with entropy: 3.3758
Guess count high, choosing a candidate with the highest entropy: 𝘽𝙊𝙒𝙀𝘿 with entropy: 2.0717

Guess: 𝘽𝙊𝙒𝙀𝘿, Feedback: ⬜🟩🟩🟩🟩
Actual Info Gain: 2.0000 bits
Posterior entropy: 2.584962500721156
entropy_info: {'prior_entropy': 4.584962500721156, 'posterior_entropy': 2.584962500721156, 'actual_info_gain': 2.0, 'expected_info_gain': 2.071695404408628}
----------
The bot is making a guess...
6 candidate words remaining.
Top 10 guesses: [('DAMPY', '1.792'), ('DIMLY', '1.792'), ('DIVEY', '1.792'), ('DIVVY', '1.792'), ('DOOMY', '1.792'), ('DORMY', '1.792'), ('DUMKY', '1.792'), ('DUMMY', '1.792'), ('DUMPY', '1.792'), ('DWAMY', '1.792')]
Top entropy choice: 𝘿𝘼𝙈𝙋𝙔 with entropy: 1.7925
Guess count high, choosing a candidate with the highest entropy: 𝙈𝙊𝙒𝙀𝘿 with entropy: 0.6500

Guess: 𝙈𝙊𝙒𝙀𝘿, Feedback: ⬜🟩🟩🟩🟩
Actual Info Gain: 0.2630 bits
Posterior entropy: 2.321928094887362
entropy_info: {'prior_entropy': 2.584962500721156, 'posterior_entropy': 2.321928094887362, 'actual_info_gain': 0.2630344058337939, 'expected_info_gain': 0.6500224216483541}
----------
The bot is making a guess...
5 candidate words remaining.
Top 10 guesses: [('DIVEY', '1.922'), ('DIVVY', '1.922'), ('JIVEY', '1.922'), ('VODDY', '1.922'), ('WADDY', '1.922'), ('WAVEY', '1.922'), ('WIDDY', '1.922'), ('WUDDY', '1.922'), ('AJIVA', '1.371'), ('AVYZE', '1.371')]
Top entropy choice: 𝘿𝙄𝙑𝙀𝙔 with entropy: 1.9219
Guess count high, choosing a candidate with the highest entropy: 𝙑𝙊𝙒𝙀𝘿 with entropy: 0.7219
Game Over! You ran out of guesses.

Guess: 𝙑𝙊𝙒𝙀𝘿, Feedback: ⬜🟩🟩🟩🟩
Actual Info Gain: 0.3219 bits
Posterior entropy: 2.0
entropy_info: {'prior_entropy': 2.321928094887362, 'posterior_entropy': 2.0, 'actual_info_gain': 0.3219280948873622, 'expected_info_gain': 0.7219280948873623}
----------

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜🟩⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 4.991 bits, Posterior Entropy: 8.867, 
Guess: 𝙄𝙉𝘿𝙊𝙇, Feedback: ⬜⬜🟨🟨⬜, Prior Entropy: 8.867, Expected Info Gain: 4.933 bits, Actual Info Gain: 3.086 bits, Posterior Entropy: 5.781, 
Guess: 𝙋𝙊𝙊𝘾𝙃, Feedback: ⬜🟩⬜⬜⬜, Prior Entropy: 5.781, Expected Info Gain: 4.933 bits, Actual Info Gain: 1.196 bits, Posterior Entropy: 4.585, 
Guess: 𝘽𝙊𝙒𝙀𝘿, Feedback: ⬜🟩🟩🟩🟩, Prior Entropy: 4.585, Expected Info Gain: 2.072 bits, Actual Info Gain: 2.000 bits, Posterior Entropy: 2.585, 
Guess: 𝙈𝙊𝙒𝙀𝘿, Feedback: ⬜🟩🟩🟩🟩, Prior Entropy: 2.585, Expected Info Gain: 0.650 bits, Actual Info Gain: 0.263 bits, Posterior Entropy: 2.322, 
Guess: 𝙑𝙊𝙒𝙀𝘿, Feedback: ⬜🟩🟩🟩🟩
===================================

Time taken for 'WOWED': 18.2643 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙃𝙊𝙐𝙂𝙃 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜⬜⬜
Actual Info Gain: 3.8615 bits
Posterior entropy: 9.997179480937621
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 9.997179480937621, 'actual_info_gain': 3.8614815037851553, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
1022 candidate words remaining.
Top 10 guesses: [('COLIN', '5.938'), ('NOILY', '5.864'), ('DINLO', '5.841'), ('PIONY', '5.791'), ('LINGO', '5.749'), ('PILON', '5.742'), ('PINOL', '5.704'), ('LOGIN', '5.699'), ('DOILY', '5.681'), ('NICOL', '5.680')]
Top entropy choice: 𝘾𝙊𝙇𝙄𝙉 with entropy: 5.9379
Guess count low, choosing the word with highest entropy: 𝘾𝙊𝙇𝙄𝙉 with entropy: 5.9379

Guess: 𝘾𝙊𝙇𝙄𝙉, Feedback: ⬜🟩⬜⬜⬜
Actual Info Gain: 3.4426 bits
Posterior entropy: 6.554588851677638
entropy_info: {'prior_entropy': 9.997179480937621, 'posterior_entropy': 6.554588851677638, 'actual_info_gain': 3.4425906292599837, 'expected_info_gain': 5.937896650379372}
----------
The bot is making a guess...
94 candidate words remaining.
Top 10 guesses: [('PUDGY', '4.344'), ('BOODY', '4.273'), ('BODGY', '4.267'), ('DUMPY', '4.225'), ('BUMPY', '4.212'), ('GOMBO', '4.207'), ('BUMPH', '4.177'), ('BODOH', '4.121'), ('GOODY', '4.093'), ('PODGY', '4.085')]
Top entropy choice: 𝙋𝙐𝘿𝙂𝙔 with entropy: 4.3443
Using common exploratory word: 𝘽𝙐𝙈𝙋𝙔 with entropy: 4.2124

Guess: 𝘽𝙐𝙈𝙋𝙔, Feedback: ⬜🟨⬜⬜⬜
Actual Info Gain: 3.9696 bits
Posterior entropy: 2.584962500721156
entropy_info: {'prior_entropy': 6.554588851677638, 'posterior_entropy': 2.584962500721156, 'actual_info_gain': 3.9696263509564815, 'expected_info_gain': 5.937896650379372}
----------
The bot is making a guess...
6 candidate words remaining.
Top 10 guesses: [('AGUED', '2.585'), ('DEFOG', '2.585'), ('DOUGH', '2.585'), ('DRUGS', '2.585'), ('FADGE', '2.585'), ('FAULD', '2.585'), ('FAURD', '2.585'), ('FEUDS', '2.585'), ('FEUED', '2.585'), ('FIDGE', '2.585')]
Top entropy choice: 𝘼𝙂𝙐𝙀𝘿 with entropy: 2.5850
Guess count high, choosing a candidate with the highest entropy: 𝘿𝙊𝙐𝙂𝙃 with entropy: 2.5850

Guess: 𝘿𝙊𝙐𝙂𝙃, Feedback: ⬜🟩🟩🟩🟩
Actual Info Gain: 2.5850 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 2.584962500721156, 'posterior_entropy': 0.0, 'actual_info_gain': 2.584962500721156, 'expected_info_gain': 2.584962500721156}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('HOUGH', '0.000')]
Top entropy choice: 𝙃𝙊𝙐𝙂𝙃 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝙃𝙊𝙐𝙂𝙃 with entropy: 0.0000
You won! Amount of guesses: 5

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜⬜⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 3.861 bits, Posterior Entropy: 9.997, 
Guess: 𝘾𝙊𝙇𝙄𝙉, Feedback: ⬜🟩⬜⬜⬜, Prior Entropy: 9.997, Expected Info Gain: 5.938 bits, Actual Info Gain: 3.443 bits, Posterior Entropy: 6.555, 
Guess: 𝘽𝙐𝙈𝙋𝙔, Feedback: ⬜🟨⬜⬜⬜, Prior Entropy: 6.555, Expected Info Gain: 5.938 bits, Actual Info Gain: 3.970 bits, Posterior Entropy: 2.585, 
Guess: 𝘿𝙊𝙐𝙂𝙃, Feedback: ⬜🟩🟩🟩🟩, Prior Entropy: 2.585, Expected Info Gain: 2.585 bits, Actual Info Gain: 2.585 bits, Posterior Entropy: 0.000, 
Guess: 𝙃𝙊𝙐𝙂𝙃, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'HOUGH': 34.0442 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝘽𝙐𝙈𝙋𝙔 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜⬜⬜
Actual Info Gain: 3.8615 bits
Posterior entropy: 9.997179480937621
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 9.997179480937621, 'actual_info_gain': 3.8614815037851553, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
1022 candidate words remaining.
Top 10 guesses: [('COLIN', '5.938'), ('NOILY', '5.864'), ('DINLO', '5.841'), ('PIONY', '5.791'), ('LINGO', '5.749'), ('PILON', '5.742'), ('PINOL', '5.704'), ('LOGIN', '5.699'), ('DOILY', '5.681'), ('NICOL', '5.680')]
Top entropy choice: 𝘾𝙊𝙇𝙄𝙉 with entropy: 5.9379
Guess count low, choosing the word with highest entropy: 𝘾𝙊𝙇𝙄𝙉 with entropy: 5.9379

Guess: 𝘾𝙊𝙇𝙄𝙉, Feedback: ⬜⬜⬜⬜⬜
Actual Info Gain: 4.1145 bits
Posterior entropy: 5.882643049361842
entropy_info: {'prior_entropy': 9.997179480937621, 'posterior_entropy': 5.882643049361842, 'actual_info_gain': 4.11453643157578, 'expected_info_gain': 5.937896650379372}
----------
The bot is making a guess...
59 candidate words remaining.
Top 10 guesses: [('PYGMY', '4.052'), ('BUMPH', '4.031'), ('DUMPY', '3.908'), ('BUMPY', '3.895'), ('PUDGY', '3.867'), ('DAMPY', '3.792'), ('PAMBY', '3.787'), ('PIGMY', '3.779'), ('PODGY', '3.712'), ('GYMPS', '3.637')]
Top entropy choice: 𝙋𝙔𝙂𝙈𝙔 with entropy: 4.0516
Using common exploratory word: 𝙋𝙔𝙂𝙈𝙔 with entropy: 4.0516

Guess: 𝙋𝙔𝙂𝙈𝙔, Feedback: 🟨⬜⬜🟨🟩
Actual Info Gain: 3.8826 bits
Posterior entropy: 2.0
entropy_info: {'prior_entropy': 5.882643049361842, 'posterior_entropy': 2.0, 'actual_info_gain': 3.8826430493618416, 'expected_info_gain': 5.937896650379372}
----------
The bot is making a guess...
4 candidate words remaining.
Top 10 guesses: [('ABJAD', '2.000'), ('ABJUD', '2.000'), ('BANDH', '2.000'), ('BHAJI', '2.000'), ('BODHI', '2.000'), ('BODOH', '2.000'), ('BUNDH', '2.000'), ('DHABA', '2.000'), ('DHOBI', '2.000'), ('HADJI', '2.000')]
Top entropy choice: 𝘼𝘽𝙅𝘼𝘿 with entropy: 2.0000
Guess count high, choosing a candidate with the highest entropy: 𝘽𝙐𝙈𝙋𝙔 with entropy: 0.8113
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜⬜⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 3.861 bits, Posterior Entropy: 9.997, 
Guess: 𝘾𝙊𝙇𝙄𝙉, Feedback: ⬜⬜⬜⬜⬜, Prior Entropy: 9.997, Expected Info Gain: 5.938 bits, Actual Info Gain: 4.115 bits, Posterior Entropy: 5.883, 
Guess: 𝙋𝙔𝙂𝙈𝙔, Feedback: 🟨⬜⬜🟨🟩, Prior Entropy: 5.883, Expected Info Gain: 5.938 bits, Actual Info Gain: 3.883 bits, Posterior Entropy: 2.000, 
Guess: 𝘽𝙐𝙈𝙋𝙔, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'BUMPY': 32.6974 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙋𝙀𝙏𝙀𝙍 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟨⬜🟨🟩⬜
Actual Info Gain: 8.3992 bits
Posterior entropy: 5.459431618637297
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 5.459431618637297, 'actual_info_gain': 8.399229366085478, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
44 candidate words remaining.
Top 10 guesses: [('NOTUM', '3.624'), ('MOULT', '3.571'), ('MOUNT', '3.570'), ('MOHUR', '3.492'), ('POULT', '3.455'), ('PITOT', '3.441'), ('OUBIT', '3.433'), ('MUTON', '3.424'), ('POTIN', '3.410'), ('HUMOR', '3.410')]
Top entropy choice: 𝙉𝙊𝙏𝙐𝙈 with entropy: 3.6238
Guess count low, choosing the word with highest entropy: 𝙉𝙊𝙏𝙐𝙈 with entropy: 3.6238

Guess: 𝙉𝙊𝙏𝙐𝙈, Feedback: ⬜⬜🟩⬜⬜
Actual Info Gain: 2.8745 bits
Posterior entropy: 2.584962500721156
entropy_info: {'prior_entropy': 5.459431618637297, 'posterior_entropy': 2.584962500721156, 'actual_info_gain': 2.8744691179161412, 'expected_info_gain': 3.6237917102166546}
----------
The bot is making a guess...
6 candidate words remaining.
Top 10 guesses: [('BECAP', '2.252'), ('BICEP', '2.252'), ('CEBID', '2.252'), ('PUBIC', '2.252'), ('ADLIB', '2.252'), ('ALBID', '2.252'), ('ALCID', '2.252'), ('ALICK', '2.252'), ('BECKE', '2.252'), ('BECKS', '2.252')]
Top entropy choice: 𝘽𝙀𝘾𝘼𝙋 with entropy: 2.2516
Using common exploratory word: 𝘽𝙄𝘾𝙀𝙋 with entropy: 2.2516

Guess: 𝘽𝙄𝘾𝙀𝙋, Feedback: ⬜⬜⬜🟩🟨
Actual Info Gain: 2.5850 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 2.584962500721156, 'posterior_entropy': 0.0, 'actual_info_gain': 2.584962500721156, 'expected_info_gain': 3.6237917102166546}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('PETER', '0.000')]
Top entropy choice: 𝙋𝙀𝙏𝙀𝙍 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝙋𝙀𝙏𝙀𝙍 with entropy: 0.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟨⬜🟨🟩⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 8.399 bits, Posterior Entropy: 5.459, 
Guess: 𝙉𝙊𝙏𝙐𝙈, Feedback: ⬜⬜🟩⬜⬜, Prior Entropy: 5.459, Expected Info Gain: 3.624 bits, Actual Info Gain: 2.874 bits, Posterior Entropy: 2.585, 
Guess: 𝘽𝙄𝘾𝙀𝙋, Feedback: ⬜⬜⬜🟩🟨, Prior Entropy: 2.585, Expected Info Gain: 3.624 bits, Actual Info Gain: 2.585 bits, Posterior Entropy: 0.000, 
Guess: 𝙋𝙀𝙏𝙀𝙍, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'PETER': 1.4062 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙇𝙔𝙄𝙉𝙂 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜⬜⬜
Actual Info Gain: 3.8615 bits
Posterior entropy: 9.997179480937621
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 9.997179480937621, 'actual_info_gain': 3.8614815037851553, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
1022 candidate words remaining.
Top 10 guesses: [('COLIN', '5.938'), ('NOILY', '5.864'), ('DINLO', '5.841'), ('PIONY', '5.791'), ('LINGO', '5.749'), ('PILON', '5.742'), ('PINOL', '5.704'), ('LOGIN', '5.699'), ('DOILY', '5.681'), ('NICOL', '5.680')]
Top entropy choice: 𝘾𝙊𝙇𝙄𝙉 with entropy: 5.9379
Guess count low, choosing the word with highest entropy: 𝘾𝙊𝙇𝙄𝙉 with entropy: 5.9379

Guess: 𝘾𝙊𝙇𝙄𝙉, Feedback: ⬜⬜🟨🟨🟨
Actual Info Gain: 5.9097 bits
Posterior entropy: 4.087462841250339
entropy_info: {'prior_entropy': 9.997179480937621, 'posterior_entropy': 4.087462841250339, 'actual_info_gain': 5.909716639687282, 'expected_info_gain': 5.937896650379372}
----------
The bot is making a guess...
17 candidate words remaining.
Top 10 guesses: [('BODGY', '3.455'), ('PUDGY', '3.455'), ('MIDGY', '3.410'), ('GUNKY', '3.337'), ('KINGY', '3.337'), ('DINGY', '3.293'), ('DUNGY', '3.293'), ('DYING', '3.293'), ('GANDY', '3.293'), ('GLADY', '3.293')]
Top entropy choice: 𝘽𝙊𝘿𝙂𝙔 with entropy: 3.4548
Using common exploratory word: 𝘿𝙄𝙉𝙂𝙔 with entropy: 3.2928

Guess: 𝘿𝙄𝙉𝙂𝙔, Feedback: ⬜🟨🟨🟨🟨
Actual Info Gain: 4.0875 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 4.087462841250339, 'posterior_entropy': 0.0, 'actual_info_gain': 4.087462841250339, 'expected_info_gain': 5.937896650379372}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('LYING', '0.000')]
Top entropy choice: 𝙇𝙔𝙄𝙉𝙂 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝙇𝙔𝙄𝙉𝙂 with entropy: 0.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜⬜⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 3.861 bits, Posterior Entropy: 9.997, 
Guess: 𝘾𝙊𝙇𝙄𝙉, Feedback: ⬜⬜🟨🟨🟨, Prior Entropy: 9.997, Expected Info Gain: 5.938 bits, Actual Info Gain: 5.910 bits, Posterior Entropy: 4.087, 
Guess: 𝘿𝙄𝙉𝙂𝙔, Feedback: ⬜🟨🟨🟨🟨, Prior Entropy: 4.087, Expected Info Gain: 5.938 bits, Actual Info Gain: 4.087 bits, Posterior Entropy: 0.000, 
Guess: 𝙇𝙔𝙄𝙉𝙂, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'LYING': 32.0816 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙎𝙀𝙒𝙀𝙍 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜🟨🟩🟨
Actual Info Gain: 8.1862 bits
Posterior entropy: 5.672425341971495
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 5.672425341971495, 'actual_info_gain': 8.186235642751281, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
51 candidate words remaining.
Top 10 guesses: [('SOILY', '3.546'), ('SHIOK', '3.530'), ('SPOIL', '3.490'), ('SLOID', '3.422'), ('SOLID', '3.422'), ('SILKY', '3.388'), ('SOLDI', '3.383'), ('SHILY', '3.379'), ('SLINK', '3.378'), ('SIELD', '3.340')]
Top entropy choice: 𝙎𝙊𝙄𝙇𝙔 with entropy: 3.5461
Guess count low, choosing the word with highest entropy: 𝙎𝙊𝙄𝙇𝙔 with entropy: 3.5461

Guess: 𝙎𝙊𝙄𝙇𝙔, Feedback: 🟩⬜⬜⬜⬜
Actual Info Gain: 1.9720 bits
Posterior entropy: 3.700439718141092
entropy_info: {'prior_entropy': 5.672425341971495, 'posterior_entropy': 3.700439718141092, 'actual_info_gain': 1.971985623830403, 'expected_info_gain': 3.5460643073936224}
----------
The bot is making a guess...
13 candidate words remaining.
Top 10 guesses: [('PEWED', '2.815'), ('WHEEP', '2.777'), ('UPEND', '2.719'), ('HEWED', '2.603'), ('NEWED', '2.603'), ('BEDEW', '2.565'), ('DWEEB', '2.565'), ('PENDU', '2.565'), ('PENED', '2.565'), ('PEWEE', '2.565')]
Top entropy choice: 𝙋𝙀𝙒𝙀𝘿 with entropy: 2.8151

Guess: 𝙋𝙀𝙒𝙀𝘿, Feedback: ⬜🟩🟩🟩⬜
Actual Info Gain: 3.7004 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 3.700439718141092, 'posterior_entropy': 0.0, 'actual_info_gain': 3.700439718141092, 'expected_info_gain': 2.8150724101159437}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('SEWER', '0.000')]
Top entropy choice: 𝙎𝙀𝙒𝙀𝙍 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝙎𝙀𝙒𝙀𝙍 with entropy: 0.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜🟨🟩🟨, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 8.186 bits, Posterior Entropy: 5.672, 
Guess: 𝙎𝙊𝙄𝙇𝙔, Feedback: 🟩⬜⬜⬜⬜, Prior Entropy: 5.672, Expected Info Gain: 3.546 bits, Actual Info Gain: 1.972 bits, Posterior Entropy: 3.700, 
Guess: 𝙋𝙀𝙒𝙀𝘿, Feedback: ⬜🟩🟩🟩⬜, Prior Entropy: 3.700, Expected Info Gain: 2.815 bits, Actual Info Gain: 3.700 bits, Posterior Entropy: 0.000, 
Guess: 𝙎𝙀𝙒𝙀𝙍, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'SEWER': 1.9244 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝘾𝙍𝙄𝙀𝘿 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜🟨🟩⬜
Actual Info Gain: 5.5966 bits
Posterior entropy: 8.262094845370179
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 8.262094845370179, 'actual_info_gain': 5.596566139352598, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
307 candidate words remaining.
Top 10 guesses: [('POIND', '4.424'), ('DINLO', '4.295'), ('NIDOR', '4.259'), ('BIPOD', '4.172'), ('PINOL', '4.170'), ('PILON', '4.152'), ('PRIOR', '4.152'), ('VIOLD', '4.149'), ('INDOL', '4.122'), ('BIDON', '4.115')]
Top entropy choice: 𝙋𝙊𝙄𝙉𝘿 with entropy: 4.4242
Guess count low, choosing the word with highest entropy: 𝙋𝙊𝙄𝙉𝘿 with entropy: 4.4242

Guess: 𝙋𝙊𝙄𝙉𝘿, Feedback: ⬜⬜🟩⬜🟩
Actual Info Gain: 6.2621 bits
Posterior entropy: 2.0
entropy_info: {'prior_entropy': 8.262094845370179, 'posterior_entropy': 2.0, 'actual_info_gain': 6.262094845370179, 'expected_info_gain': 4.424196059405291}
----------
The bot is making a guess...
4 candidate words remaining.
Top 10 guesses: [('DECAF', '2.000'), ('DOWFS', '2.000'), ('DWARF', '2.000'), ('AFANC', '1.500'), ('AFLOW', '1.500'), ('AWFUL', '1.500'), ('CADDY', '1.500'), ('CAFES', '1.500'), ('CAFFE', '1.500'), ('CAFFS', '1.500')]
Top entropy choice: 𝘿𝙀𝘾𝘼𝙁 with entropy: 2.0000
Using common exploratory word: 𝘿𝙀𝘾𝘼𝙁 with entropy: 2.0000

Guess: 𝘿𝙀𝘾𝘼𝙁, Feedback: 🟨🟨🟨⬜⬜
Actual Info Gain: 2.0000 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 2.0, 'posterior_entropy': 0.0, 'actual_info_gain': 2.0, 'expected_info_gain': 4.424196059405291}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('CRIED', '0.000')]
Top entropy choice: 𝘾𝙍𝙄𝙀𝘿 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝘾𝙍𝙄𝙀𝘿 with entropy: 0.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜🟨🟩⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 5.597 bits, Posterior Entropy: 8.262, 
Guess: 𝙋𝙊𝙄𝙉𝘿, Feedback: ⬜⬜🟩⬜🟩, Prior Entropy: 8.262, Expected Info Gain: 4.424 bits, Actual Info Gain: 6.262 bits, Posterior Entropy: 2.000, 
Guess: 𝘿𝙀𝘾𝘼𝙁, Feedback: 🟨🟨🟨⬜⬜, Prior Entropy: 2.000, Expected Info Gain: 4.424 bits, Actual Info Gain: 2.000 bits, Posterior Entropy: 0.000, 
Guess: 𝘾𝙍𝙄𝙀𝘿, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'CRIED': 8.7355 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝘾𝙊𝙉𝙂𝘼 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟨⬜⬜⬜
Actual Info Gain: 4.3449 bits
Posterior entropy: 9.513727595952437
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 9.513727595952437, 'actual_info_gain': 4.34493338877034, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
731 candidate words remaining.
Top 10 guesses: [('COLIN', '5.613'), ('LIANA', '5.503'), ('ALOIN', '5.496'), ('PILON', '5.458'), ('PINOL', '5.439'), ('DINLO', '5.430'), ('LOGIN', '5.388'), ('NICOL', '5.363'), ('NMOLI', '5.336'), ('ULMIN', '5.322')]
Top entropy choice: 𝘾𝙊𝙇𝙄𝙉 with entropy: 5.6132
Guess count low, choosing the word with highest entropy: 𝘾𝙊𝙇𝙄𝙉 with entropy: 5.6132

Guess: 𝘾𝙊𝙇𝙄𝙉, Feedback: 🟩🟩⬜⬜🟨
Actual Info Gain: 9.5137 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 9.513727595952437, 'posterior_entropy': 0.0, 'actual_info_gain': 9.513727595952437, 'expected_info_gain': 5.613155296118865}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('CONGA', '0.000')]
Top entropy choice: 𝘾𝙊𝙉𝙂𝘼 with entropy: 0.0000
Few candidates left, going through them all to pick a common word...
Using common word: 𝘾𝙊𝙉𝙂𝘼 with entropy: 0.0000
You won! Amount of guesses: 3

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟨⬜⬜⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 4.345 bits, Posterior Entropy: 9.514, 
Guess: 𝘾𝙊𝙇𝙄𝙉, Feedback: 🟩🟩⬜⬜🟨, Prior Entropy: 9.514, Expected Info Gain: 5.613 bits, Actual Info Gain: 9.514 bits, Posterior Entropy: 0.000, 
Guess: 𝘾𝙊𝙉𝙂𝘼, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'CONGA': 25.3240 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝘽𝙍𝙐𝙉𝙏 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟨⬜🟨⬜⬜
Actual Info Gain: 7.7089 bits
Posterior entropy: 6.149747119504682
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 6.149747119504682, 'actual_info_gain': 7.708913865218094, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
71 candidate words remaining.
Top 10 guesses: [('PITOT', '4.608'), ('POINT', '4.505'), ('PICOT', '4.479'), ('NOOIT', '4.471'), ('RHINO', '4.428'), ('PINOT', '4.414'), ('BIONT', '4.414'), ('FRUIT', '4.405'), ('GRIOT', '4.404'), ('FOUTH', '4.392')]
Top entropy choice: 𝙋𝙄𝙏𝙊𝙏 with entropy: 4.6083
Guess count low, choosing the word with highest entropy: 𝙋𝙄𝙏𝙊𝙏 with entropy: 4.6083

Guess: 𝙋𝙄𝙏𝙊𝙏, Feedback: ⬜⬜⬜⬜🟩
Actual Info Gain: 4.1497 bits
Posterior entropy: 2.0
entropy_info: {'prior_entropy': 6.149747119504682, 'posterior_entropy': 2.0, 'actual_info_gain': 4.149747119504682, 'expected_info_gain': 4.608311423783217}
----------
The bot is making a guess...
4 candidate words remaining.
Top 10 guesses: [('ABAND', '2.000'), ('ABENG', '2.000'), ('ABNET', '2.000'), ('ABOON', '2.000'), ('ABORN', '2.000'), ('ABRIN', '2.000'), ('ABUNA', '2.000'), ('ABUNE', '2.000'), ('ABURN', '2.000'), ('ACERB', '2.000')]
Top entropy choice: 𝘼𝘽𝘼𝙉𝘿 with entropy: 2.0000

Guess: 𝘼𝘽𝘼𝙉𝘿, Feedback: ⬜🟨⬜🟩⬜
Actual Info Gain: 2.0000 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 2.0, 'posterior_entropy': 0.0, 'actual_info_gain': 2.0, 'expected_info_gain': 2.0}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('BRUNT', '0.000')]
Top entropy choice: 𝘽𝙍𝙐𝙉𝙏 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝘽𝙍𝙐𝙉𝙏 with entropy: 0.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟨⬜🟨⬜⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 7.709 bits, Posterior Entropy: 6.150, 
Guess: 𝙋𝙄𝙏𝙊𝙏, Feedback: ⬜⬜⬜⬜🟩, Prior Entropy: 6.150, Expected Info Gain: 4.608 bits, Actual Info Gain: 4.150 bits, Posterior Entropy: 2.000, 
Guess: 𝘼𝘽𝘼𝙉𝘿, Feedback: ⬜🟨⬜🟩⬜, Prior Entropy: 2.000, Expected Info Gain: 2.000 bits, Actual Info Gain: 2.000 bits, Posterior Entropy: 0.000, 
Guess: 𝘽𝙍𝙐𝙉𝙏, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'BRUNT': 2.1565 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙋𝙇𝘼𝙄𝙉 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟨⬜⬜⬜
Actual Info Gain: 4.3449 bits
Posterior entropy: 9.513727595952437
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 9.513727595952437, 'actual_info_gain': 4.34493338877034, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
731 candidate words remaining.
Top 10 guesses: [('COLIN', '5.613'), ('LIANA', '5.503'), ('ALOIN', '5.496'), ('PILON', '5.458'), ('PINOL', '5.439'), ('DINLO', '5.430'), ('LOGIN', '5.388'), ('NICOL', '5.363'), ('NMOLI', '5.336'), ('ULMIN', '5.322')]
Top entropy choice: 𝘾𝙊𝙇𝙄𝙉 with entropy: 5.6132
Guess count low, choosing the word with highest entropy: 𝘾𝙊𝙇𝙄𝙉 with entropy: 5.6132

Guess: 𝘾𝙊𝙇𝙄𝙉, Feedback: ⬜⬜🟨🟩🟩
Actual Info Gain: 7.5137 bits
Posterior entropy: 2.0
entropy_info: {'prior_entropy': 9.513727595952437, 'posterior_entropy': 2.0, 'actual_info_gain': 7.513727595952437, 'expected_info_gain': 5.613155296118865}
----------
The bot is making a guess...
4 candidate words remaining.
Top 10 guesses: [('ABACK', '2.000'), ('ABAKA', '2.000'), ('ABASK', '2.000'), ('ABENG', '2.000'), ('AGAPE', '2.000'), ('AGASP', '2.000'), ('AGBAS', '2.000'), ('APAGE', '2.000'), ('APEAK', '2.000'), ('APEEK', '2.000')]
Top entropy choice: 𝘼𝘽𝘼𝘾𝙆 with entropy: 2.0000
Using common exploratory word: 𝘼𝘽𝘼𝘾𝙆 with entropy: 2.0000

Guess: 𝘼𝘽𝘼𝘾𝙆, Feedback: ⬜⬜🟩⬜⬜
Actual Info Gain: 2.0000 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 2.0, 'posterior_entropy': 0.0, 'actual_info_gain': 2.0, 'expected_info_gain': 5.613155296118865}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('PLAIN', '0.000')]
Top entropy choice: 𝙋𝙇𝘼𝙄𝙉 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝙋𝙇𝘼𝙄𝙉 with entropy: 0.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟨⬜⬜⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 4.345 bits, Posterior Entropy: 9.514, 
Guess: 𝘾𝙊𝙇𝙄𝙉, Feedback: ⬜⬜🟨🟩🟩, Prior Entropy: 9.514, Expected Info Gain: 5.613 bits, Actual Info Gain: 7.514 bits, Posterior Entropy: 2.000, 
Guess: 𝘼𝘽𝘼𝘾𝙆, Feedback: ⬜⬜🟩⬜⬜, Prior Entropy: 2.000, Expected Info Gain: 5.613 bits, Actual Info Gain: 2.000 bits, Posterior Entropy: 0.000, 
Guess: 𝙋𝙇𝘼𝙄𝙉, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'PLAIN': 24.6173 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙁𝙊𝘾𝙐𝙎 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜⬜🟩
Actual Info Gain: 4.0481 bits
Posterior entropy: 9.810571634741146
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 9.810571634741146, 'actual_info_gain': 4.04808934998163, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
898 candidate words remaining.
Top 10 guesses: [('PILON', '5.176'), ('PINOL', '5.127'), ('LOUND', '5.104'), ('NOULD', '5.095'), ('MOULD', '5.068'), ('PLOUK', '5.067'), ('COLIN', '5.024'), ('POILU', '5.021'), ('NMOLI', '5.014'), ('LOKUM', '5.007')]
Top entropy choice: 𝙋𝙄𝙇𝙊𝙉 with entropy: 5.1762
Guess count low, choosing the word with highest entropy: 𝙋𝙄𝙇𝙊𝙉 with entropy: 5.1762

Guess: 𝙋𝙄𝙇𝙊𝙉, Feedback: ⬜⬜⬜🟨⬜
Actual Info Gain: 2.9526 bits
Posterior entropy: 6.857980995127572
entropy_info: {'prior_entropy': 9.810571634741146, 'posterior_entropy': 6.857980995127572, 'actual_info_gain': 2.9525906396135744, 'expected_info_gain': 5.176180808335691}
----------
The bot is making a guess...
116 candidate words remaining.
Top 10 guesses: [('KHOUM', '4.177'), ('SMOCK', '4.080'), ('CHOWK', '4.059'), ('COOMB', '4.027'), ('MOUCH', '4.012'), ('SHUCK', '4.006'), ('KOMBU', '3.998'), ('CHOKY', '3.998'), ('COUGH', '3.985'), ('MUCKY', '3.975')]
Top entropy choice: 𝙆𝙃𝙊𝙐𝙈 with entropy: 4.1770
Using common exploratory word: 𝙎𝙈𝙊𝘾𝙆 with entropy: 4.0799

Guess: 𝙎𝙈𝙊𝘾𝙆, Feedback: 🟨⬜🟨🟨⬜
Actual Info Gain: 4.2730 bits
Posterior entropy: 2.584962500721156
entropy_info: {'prior_entropy': 6.857980995127572, 'posterior_entropy': 2.584962500721156, 'actual_info_gain': 4.273018494406416, 'expected_info_gain': 5.176180808335691}
----------
The bot is making a guess...
6 candidate words remaining.
Top 10 guesses: [('CHUFA', '2.585'), ('CHUFF', '2.585'), ('CHUBS', '2.252'), ('CRUFT', '2.252'), ('CUFFO', '2.252'), ('CUFFS', '2.252'), ('CUIFS', '2.252'), ('CURFS', '2.252'), ('BIFID', '2.252'), ('BUNDH', '2.252')]
Top entropy choice: 𝘾𝙃𝙐𝙁𝘼 with entropy: 2.5850
Guess count high, choosing a candidate with the highest entropy: 𝙁𝙊𝘾𝙐𝙎 with entropy: 1.7925
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜⬜🟩, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 4.048 bits, Posterior Entropy: 9.811, 
Guess: 𝙋𝙄𝙇𝙊𝙉, Feedback: ⬜⬜⬜🟨⬜, Prior Entropy: 9.811, Expected Info Gain: 5.176 bits, Actual Info Gain: 2.953 bits, Posterior Entropy: 6.858, 
Guess: 𝙎𝙈𝙊𝘾𝙆, Feedback: 🟨⬜🟨🟨⬜, Prior Entropy: 6.858, Expected Info Gain: 5.176 bits, Actual Info Gain: 4.273 bits, Posterior Entropy: 2.585, 
Guess: 𝙁𝙊𝘾𝙐𝙎, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'FOCUS': 39.1159 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝘿𝙊𝘽𝘽𝙔 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜⬜⬜
Actual Info Gain: 3.8615 bits
Posterior entropy: 9.997179480937621
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 9.997179480937621, 'actual_info_gain': 3.8614815037851553, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
1022 candidate words remaining.
Top 10 guesses: [('COLIN', '5.938'), ('NOILY', '5.864'), ('DINLO', '5.841'), ('PIONY', '5.791'), ('LINGO', '5.749'), ('PILON', '5.742'), ('PINOL', '5.704'), ('LOGIN', '5.699'), ('DOILY', '5.681'), ('NICOL', '5.680')]
Top entropy choice: 𝘾𝙊𝙇𝙄𝙉 with entropy: 5.9379
Guess count low, choosing the word with highest entropy: 𝘾𝙊𝙇𝙄𝙉 with entropy: 5.9379

Guess: 𝘾𝙊𝙇𝙄𝙉, Feedback: ⬜🟩⬜⬜⬜
Actual Info Gain: 3.4426 bits
Posterior entropy: 6.554588851677638
entropy_info: {'prior_entropy': 9.997179480937621, 'posterior_entropy': 6.554588851677638, 'actual_info_gain': 3.4425906292599837, 'expected_info_gain': 5.937896650379372}
----------
The bot is making a guess...
94 candidate words remaining.
Top 10 guesses: [('PUDGY', '4.344'), ('BOODY', '4.273'), ('BODGY', '4.267'), ('DUMPY', '4.225'), ('BUMPY', '4.212'), ('GOMBO', '4.207'), ('BUMPH', '4.177'), ('BODOH', '4.121'), ('GOODY', '4.093'), ('PODGY', '4.085')]
Top entropy choice: 𝙋𝙐𝘿𝙂𝙔 with entropy: 4.3443
Using common exploratory word: 𝘽𝙐𝙈𝙋𝙔 with entropy: 4.2124

Guess: 𝘽𝙐𝙈𝙋𝙔, Feedback: 🟨⬜⬜⬜🟩
Actual Info Gain: 4.2327 bits
Posterior entropy: 2.321928094887362
entropy_info: {'prior_entropy': 6.554588851677638, 'posterior_entropy': 2.321928094887362, 'actual_info_gain': 4.232660756790276, 'expected_info_gain': 5.937896650379372}
----------
The bot is making a guess...
5 candidate words remaining.
Top 10 guesses: [('AGOOD', '1.922'), ('AHOLD', '1.922'), ('BODOH', '1.922'), ('BROGH', '1.922'), ('CHODE', '1.922'), ('CHOGS', '1.922'), ('CHORD', '1.922'), ('COHOG', '1.922'), ('DEBAG', '1.922'), ('DEBUG', '1.922')]
Top entropy choice: 𝘼𝙂𝙊𝙊𝘿 with entropy: 1.9219
Guess count high, choosing a candidate with the highest entropy: 𝘿𝙊𝘽𝘽𝙔 with entropy: 1.3710
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜⬜⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 3.861 bits, Posterior Entropy: 9.997, 
Guess: 𝘾𝙊𝙇𝙄𝙉, Feedback: ⬜🟩⬜⬜⬜, Prior Entropy: 9.997, Expected Info Gain: 5.938 bits, Actual Info Gain: 3.443 bits, Posterior Entropy: 6.555, 
Guess: 𝘽𝙐𝙈𝙋𝙔, Feedback: 🟨⬜⬜⬜🟩, Prior Entropy: 6.555, Expected Info Gain: 5.938 bits, Actual Info Gain: 4.233 bits, Posterior Entropy: 2.322, 
Guess: 𝘿𝙊𝘽𝘽𝙔, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'DOBBY': 34.5436 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙋𝘼𝙍𝙈𝘼 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟩🟩⬜⬜
Actual Info Gain: 6.9760 bits
Posterior entropy: 6.882643049361842
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 6.882643049361842, 'actual_info_gain': 6.976017935360935, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
118 candidate words remaining.
Top 10 guesses: [('MINCY', '3.994'), ('MOLDY', '3.993'), ('COMBY', '3.944'), ('MYOID', '3.913'), ('CYMOL', '3.890'), ('COMBI', '3.882'), ('MOCKY', '3.855'), ('MICKY', '3.849'), ('DIMBO', '3.844'), ('MIDGY', '3.812')]
Top entropy choice: 𝙈𝙄𝙉𝘾𝙔 with entropy: 3.9942
Guess count low, choosing the word with highest entropy: 𝙈𝙄𝙉𝘾𝙔 with entropy: 3.9942

Guess: 𝙈𝙄𝙉𝘾𝙔, Feedback: 🟨⬜⬜⬜⬜
Actual Info Gain: 4.0753 bits
Posterior entropy: 2.807354922057604
entropy_info: {'prior_entropy': 6.882643049361842, 'posterior_entropy': 2.807354922057604, 'actual_info_gain': 4.075288127304237, 'expected_info_gain': 3.994197821504514}
----------
The bot is making a guess...
7 candidate words remaining.
Top 10 guesses: [('ALPHA', '2.807'), ('APGAR', '2.807'), ('GOPAK', '2.807'), ('KOGAL', '2.807'), ('PAGAN', '2.807'), ('PANGA', '2.807'), ('PASAG', '2.807'), ('PLAGA', '2.807'), ('PUNGA', '2.807'), ('PURGA', '2.807')]
Top entropy choice: 𝘼𝙇𝙋𝙃𝘼 with entropy: 2.8074
Using common exploratory word: 𝘼𝙇𝙋𝙃𝘼 with entropy: 2.8074

Guess: 𝘼𝙇𝙋𝙃𝘼, Feedback: 🟨⬜🟨⬜🟩
Actual Info Gain: 2.8074 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 2.807354922057604, 'posterior_entropy': 0.0, 'actual_info_gain': 2.807354922057604, 'expected_info_gain': 3.994197821504514}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('PARMA', '0.000')]
Top entropy choice: 𝙋𝘼𝙍𝙈𝘼 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝙋𝘼𝙍𝙈𝘼 with entropy: 0.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟩🟩⬜⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 6.976 bits, Posterior Entropy: 6.883, 
Guess: 𝙈𝙄𝙉𝘾𝙔, Feedback: 🟨⬜⬜⬜⬜, Prior Entropy: 6.883, Expected Info Gain: 3.994 bits, Actual Info Gain: 4.075 bits, Posterior Entropy: 2.807, 
Guess: 𝘼𝙇𝙋𝙃𝘼, Feedback: 🟨⬜🟨⬜🟩, Prior Entropy: 2.807, Expected Info Gain: 3.994 bits, Actual Info Gain: 2.807 bits, Posterior Entropy: 0.000, 
Guess: 𝙋𝘼𝙍𝙈𝘼, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'PARMA': 3.8144 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙈𝘼𝙏𝙀𝘿 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟨🟩⬜🟩⬜
Actual Info Gain: 9.2737 bits
Posterior entropy: 4.584962500721156
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 4.584962500721156, 'actual_info_gain': 9.27369848400162, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
24 candidate words remaining.
Top 10 guesses: [('DEMPT', '3.153'), ('DELFT', '3.138'), ('PLANT', '3.099'), ('DOLCE', '3.090'), ('POLED', '3.011'), ('DELPH', '3.001'), ('DOTAL', '2.991'), ('LYTED', '2.933'), ('LEMED', '2.928'), ('LOPED', '2.928')]
Top entropy choice: 𝘿𝙀𝙈𝙋𝙏 with entropy: 3.1531
Guess count low, choosing the word with highest entropy: 𝘿𝙀𝙈𝙋𝙏 with entropy: 3.1531

Guess: 𝘿𝙀𝙈𝙋𝙏, Feedback: 🟨🟨🟨⬜🟨
Actual Info Gain: 4.5850 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 4.584962500721156, 'posterior_entropy': 0.0, 'actual_info_gain': 4.584962500721156, 'expected_info_gain': 3.153107168362811}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('MATED', '0.000')]
Top entropy choice: 𝙈𝘼𝙏𝙀𝘿 with entropy: 0.0000
Few candidates left, going through them all to pick a common word...
Using common word: 𝙈𝘼𝙏𝙀𝘿 with entropy: 0.0000
You won! Amount of guesses: 3

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟨🟩⬜🟩⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 9.274 bits, Posterior Entropy: 4.585, 
Guess: 𝘿𝙀𝙈𝙋𝙏, Feedback: 🟨🟨🟨⬜🟨, Prior Entropy: 4.585, Expected Info Gain: 3.153 bits, Actual Info Gain: 4.585 bits, Posterior Entropy: 0.000, 
Guess: 𝙈𝘼𝙏𝙀𝘿, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'MATED': 0.6403 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙎𝘾𝙀𝙉𝙏 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟨⬜⬜🟨🟨
Actual Info Gain: 7.2004 bits
Posterior entropy: 6.658211482751795
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 6.658211482751795, 'actual_info_gain': 7.200449501970982, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
101 candidate words remaining.
Top 10 guesses: [('SPITE', '4.201'), ('PEINE', '4.193'), ('SEINE', '4.162'), ('SUITE', '4.139'), ('POETE', '4.127'), ('PENIE', '4.086'), ('POINT', '4.076'), ('SPINE', '4.047'), ('STIPE', '4.044'), ('LOIPE', '4.043')]
Top entropy choice: 𝙎𝙋𝙄𝙏𝙀 with entropy: 4.2005
Guess count low, choosing the word with highest entropy: 𝙎𝙋𝙄𝙏𝙀 with entropy: 4.2005

Guess: 𝙎𝙋𝙄𝙏𝙀, Feedback: 🟩⬜⬜🟨🟨
Actual Info Gain: 3.0732 bits
Posterior entropy: 3.584962500721156
entropy_info: {'prior_entropy': 6.658211482751795, 'posterior_entropy': 3.584962500721156, 'actual_info_gain': 3.0732489820306386, 'expected_info_gain': 4.200540807955409}
----------
The bot is making a guess...
12 candidate words remaining.
Top 10 guesses: [('COUNT', '2.855'), ('DAUNT', '2.855'), ('OTHYL', '2.855'), ('STOND', '2.855'), ('STOWN', '2.855'), ('WOUND', '2.855'), ('CHYND', '2.792'), ('LOUND', '2.792'), ('MOUND', '2.792'), ('MOUNT', '2.792')]
Top entropy choice: 𝘾𝙊𝙐𝙉𝙏 with entropy: 2.8554
Using common exploratory word: 𝘾𝙊𝙐𝙉𝙏 with entropy: 2.8554

Guess: 𝘾𝙊𝙐𝙉𝙏, Feedback: 🟨⬜⬜🟩🟩
Actual Info Gain: 3.5850 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 3.584962500721156, 'posterior_entropy': 0.0, 'actual_info_gain': 3.584962500721156, 'expected_info_gain': 4.200540807955409}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('SCENT', '0.000')]
Top entropy choice: 𝙎𝘾𝙀𝙉𝙏 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝙎𝘾𝙀𝙉𝙏 with entropy: 0.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟨⬜⬜🟨🟨, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 7.200 bits, Posterior Entropy: 6.658, 
Guess: 𝙎𝙋𝙄𝙏𝙀, Feedback: 🟩⬜⬜🟨🟨, Prior Entropy: 6.658, Expected Info Gain: 4.201 bits, Actual Info Gain: 3.073 bits, Posterior Entropy: 3.585, 
Guess: 𝘾𝙊𝙐𝙉𝙏, Feedback: 🟨⬜⬜🟩🟩, Prior Entropy: 3.585, Expected Info Gain: 4.201 bits, Actual Info Gain: 3.585 bits, Posterior Entropy: 0.000, 
Guess: 𝙎𝘾𝙀𝙉𝙏, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'SCENT': 3.4934 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙇𝘼𝙎𝙎𝙊 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟩⬜⬜🟨
Actual Info Gain: 6.8034 bits
Posterior entropy: 7.05528243550119
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 7.05528243550119, 'actual_info_gain': 6.803378549221587, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
133 candidate words remaining.
Top 10 guesses: [('LYSIN', '4.282'), ('SYBIL', '4.252'), ('LOSSY', '4.134'), ('SHILY', '4.113'), ('SYLIS', '4.112'), ('SONSY', '4.104'), ('SIBYL', '4.099'), ('SPOIL', '4.098'), ('SHINY', '4.092'), ('BYSSI', '4.090')]
Top entropy choice: 𝙇𝙔𝙎𝙄𝙉 with entropy: 4.2816
Guess count low, choosing the word with highest entropy: 𝙇𝙔𝙎𝙄𝙉 with entropy: 4.2816

Guess: 𝙇𝙔𝙎𝙄𝙉, Feedback: 🟩⬜🟩⬜⬜
Actual Info Gain: 6.0553 bits
Posterior entropy: 1.0
entropy_info: {'prior_entropy': 7.05528243550119, 'posterior_entropy': 1.0, 'actual_info_gain': 6.05528243550119, 'expected_info_gain': 4.281552287338487}
----------
The bot is making a guess...
2 candidate words remaining.
Top 10 guesses: [('LASSO', '1.000'), ('LASSU', '1.000')]
Top entropy choice: 𝙇𝘼𝙎𝙎𝙊 with entropy: 1.0000
Few candidates left, going through them all to pick a common word...
Using common word: 𝙇𝘼𝙎𝙎𝙊 with entropy: 1.0000
You won! Amount of guesses: 3

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟩⬜⬜🟨, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 6.803 bits, Posterior Entropy: 7.055, 
Guess: 𝙇𝙔𝙎𝙄𝙉, Feedback: 🟩⬜🟩⬜⬜, Prior Entropy: 7.055, Expected Info Gain: 4.282 bits, Actual Info Gain: 6.055 bits, Posterior Entropy: 1.000, 
Guess: 𝙇𝘼𝙎𝙎𝙊, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'LASSO': 4.0878 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙏𝙒𝙄𝙉𝙆 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟩⬜⬜⬜⬜
Actual Info Gain: 7.3509 bits
Posterior entropy: 6.507794640198696
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 6.507794640198696, 'actual_info_gain': 7.35086634452408, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
91 candidate words remaining.
Top 10 guesses: [('PIONY', '4.966'), ('CHINO', '4.839'), ('NOILY', '4.783'), ('YONIC', '4.754'), ('YOGIN', '4.719'), ('HINKY', '4.708'), ('MINCY', '4.642'), ('PHONY', '4.639'), ('MINTY', '4.630'), ('PINOL', '4.626')]
Top entropy choice: 𝙋𝙄𝙊𝙉𝙔 with entropy: 4.9663
Guess count low, choosing the word with highest entropy: 𝙋𝙄𝙊𝙉𝙔 with entropy: 4.9663

Guess: 𝙋𝙄𝙊𝙉𝙔, Feedback: ⬜🟨⬜🟩⬜
Actual Info Gain: 4.9228 bits
Posterior entropy: 1.584962500721156
entropy_info: {'prior_entropy': 6.507794640198696, 'posterior_entropy': 1.584962500721156, 'actual_info_gain': 4.92283213947754, 'expected_info_gain': 4.966278915200509}
----------
The bot is making a guess...
3 candidate words remaining.
Top 10 guesses: [('THING', '1.585'), ('THINK', '1.585'), ('TWINK', '1.585')]
Top entropy choice: 𝙏𝙃𝙄𝙉𝙂 with entropy: 1.5850
Few candidates left, going through them all to pick a common word...
Using common word: 𝙏𝙃𝙄𝙉𝙂 with entropy: 1.5850

Guess: 𝙏𝙃𝙄𝙉𝙂, Feedback: 🟩⬜🟩🟩⬜
Actual Info Gain: 1.5850 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 1.584962500721156, 'posterior_entropy': 0.0, 'actual_info_gain': 1.584962500721156, 'expected_info_gain': 4.966278915200509}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('TWINK', '0.000')]
Top entropy choice: 𝙏𝙒𝙄𝙉𝙆 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝙏𝙒𝙄𝙉𝙆 with entropy: 0.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟩⬜⬜⬜⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 7.351 bits, Posterior Entropy: 6.508, 
Guess: 𝙋𝙄𝙊𝙉𝙔, Feedback: ⬜🟨⬜🟩⬜, Prior Entropy: 6.508, Expected Info Gain: 4.966 bits, Actual Info Gain: 4.923 bits, Posterior Entropy: 1.585, 
Guess: 𝙏𝙃𝙄𝙉𝙂, Feedback: 🟩⬜🟩🟩⬜, Prior Entropy: 1.585, Expected Info Gain: 4.966 bits, Actual Info Gain: 1.585 bits, Posterior Entropy: 0.000, 
Guess: 𝙏𝙒𝙄𝙉𝙆, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'TWINK': 2.6217 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝘿𝘼𝙇𝙇𝙔 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟩⬜⬜⬜
Actual Info Gain: 4.8308 bits
Posterior entropy: 9.027905996569885
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 9.027905996569885, 'actual_info_gain': 4.830754988152892, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
522 candidate words remaining.
Top 10 guesses: [('MINCY', '4.781'), ('LINDY', '4.711'), ('MILKY', '4.698'), ('KYLIN', '4.687'), ('LINKY', '4.677'), ('MINGY', '4.632'), ('PYLON', '4.632'), ('MOLDY', '4.626'), ('DIMLY', '4.626'), ('LINGY', '4.613')]
Top entropy choice: 𝙈𝙄𝙉𝘾𝙔 with entropy: 4.7811
Guess count low, choosing the word with highest entropy: 𝙈𝙄𝙉𝘾𝙔 with entropy: 4.7811

Guess: 𝙈𝙄𝙉𝘾𝙔, Feedback: ⬜⬜⬜⬜🟩
Actual Info Gain: 3.3555 bits
Posterior entropy: 5.672425341971495
entropy_info: {'prior_entropy': 9.027905996569885, 'posterior_entropy': 5.672425341971495, 'actual_info_gain': 3.3554806545983897, 'expected_info_gain': 4.781133838721642}
----------
The bot is making a guess...
51 candidate words remaining.
Top 10 guesses: [('DUPLE', '3.709'), ('DUPLY', '3.709'), ('GULPH', '3.655'), ('DELPH', '3.616'), ('LABDA', '3.607'), ('GULAB', '3.541'), ('PADLE', '3.536'), ('BADLY', '3.491'), ('BODLE', '3.491'), ('GELDS', '3.485')]
Top entropy choice: 𝘿𝙐𝙋𝙇𝙀 with entropy: 3.7094
Using common exploratory word: 𝘽𝘼𝘿𝙇𝙔 with entropy: 3.4914

Guess: 𝘽𝘼𝘿𝙇𝙔, Feedback: ⬜🟩🟨🟩🟩
Actual Info Gain: 5.6724 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 5.672425341971495, 'posterior_entropy': 0.0, 'actual_info_gain': 5.672425341971495, 'expected_info_gain': 4.781133838721642}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('DALLY', '0.000')]
Top entropy choice: 𝘿𝘼𝙇𝙇𝙔 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝘿𝘼𝙇𝙇𝙔 with entropy: 0.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟩⬜⬜⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 4.831 bits, Posterior Entropy: 9.028, 
Guess: 𝙈𝙄𝙉𝘾𝙔, Feedback: ⬜⬜⬜⬜🟩, Prior Entropy: 9.028, Expected Info Gain: 4.781 bits, Actual Info Gain: 3.355 bits, Posterior Entropy: 5.672, 
Guess: 𝘽𝘼𝘿𝙇𝙔, Feedback: ⬜🟩🟨🟩🟩, Prior Entropy: 5.672, Expected Info Gain: 4.781 bits, Actual Info Gain: 5.672 bits, Posterior Entropy: 0.000, 
Guess: 𝘿𝘼𝙇𝙇𝙔, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'DALLY': 18.0367 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝘾𝙊𝘾𝙆𝙔 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜⬜⬜
Actual Info Gain: 3.8615 bits
Posterior entropy: 9.997179480937621
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 9.997179480937621, 'actual_info_gain': 3.8614815037851553, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
1022 candidate words remaining.
Top 10 guesses: [('COLIN', '5.938'), ('NOILY', '5.864'), ('DINLO', '5.841'), ('PIONY', '5.791'), ('LINGO', '5.749'), ('PILON', '5.742'), ('PINOL', '5.704'), ('LOGIN', '5.699'), ('DOILY', '5.681'), ('NICOL', '5.680')]
Top entropy choice: 𝘾𝙊𝙇𝙄𝙉 with entropy: 5.9379
Guess count low, choosing the word with highest entropy: 𝘾𝙊𝙇𝙄𝙉 with entropy: 5.9379

Guess: 𝘾𝙊𝙇𝙄𝙉, Feedback: 🟩🟩⬜⬜⬜
Actual Info Gain: 5.8273 bits
Posterior entropy: 4.169925001442312
entropy_info: {'prior_entropy': 9.997179480937621, 'posterior_entropy': 4.169925001442312, 'actual_info_gain': 5.827254479495309, 'expected_info_gain': 5.937896650379372}
----------
The bot is making a guess...
18 candidate words remaining.
Top 10 guesses: [('BOOMY', '3.837'), ('GOOMY', '3.684'), ('BUMPY', '3.572'), ('BOMOH', '3.461'), ('COOMY', '3.461'), ('DOOMY', '3.461'), ('ROOMY', '3.461'), ('ZOOMY', '3.461'), ('BUMPH', '3.419'), ('BROMO', '3.414')]
Top entropy choice: 𝘽𝙊𝙊𝙈𝙔 with entropy: 3.8366
Using common exploratory word: 𝘽𝙐𝙈𝙋𝙔 with entropy: 3.5724

Guess: 𝘽𝙐𝙈𝙋𝙔, Feedback: ⬜⬜⬜⬜🟩
Actual Info Gain: 3.1699 bits
Posterior entropy: 1.0
entropy_info: {'prior_entropy': 4.169925001442312, 'posterior_entropy': 1.0, 'actual_info_gain': 3.169925001442312, 'expected_info_gain': 5.937896650379372}
----------
The bot is making a guess...
2 candidate words remaining.
Top 10 guesses: [('COCKY', '1.000'), ('COOKY', '1.000')]
Top entropy choice: 𝘾𝙊𝘾𝙆𝙔 with entropy: 1.0000
Guess count high, choosing a candidate with the highest entropy: 𝘾𝙊𝘾𝙆𝙔 with entropy: 1.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜⬜⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 3.861 bits, Posterior Entropy: 9.997, 
Guess: 𝘾𝙊𝙇𝙄𝙉, Feedback: 🟩🟩⬜⬜⬜, Prior Entropy: 9.997, Expected Info Gain: 5.938 bits, Actual Info Gain: 5.827 bits, Posterior Entropy: 4.170, 
Guess: 𝘽𝙐𝙈𝙋𝙔, Feedback: ⬜⬜⬜⬜🟩, Prior Entropy: 4.170, Expected Info Gain: 5.938 bits, Actual Info Gain: 3.170 bits, Posterior Entropy: 1.000, 
Guess: 𝘾𝙊𝘾𝙆𝙔, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'COCKY': 31.4032 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙋𝘼𝙐𝙎𝙀 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟩⬜🟨🟨
Actual Info Gain: 8.8587 bits
Posterior entropy: 5.0
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 5.0, 'actual_info_gain': 8.858660984722777, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
32 candidate words remaining.
Top 10 guesses: [('SLUSH', '3.793'), ('SULPH', '3.737'), ('SHULS', '3.690'), ('SHULN', '3.664'), ('MULSH', '3.656'), ('SLISH', '3.605'), ('SNUSH', '3.593'), ('SCULP', '3.578'), ('SCULS', '3.578'), ('PLUSH', '3.550')]
Top entropy choice: 𝙎𝙇𝙐𝙎𝙃 with entropy: 3.7928
Guess count low, choosing the word with highest entropy: 𝙎𝙇𝙐𝙎𝙃 with entropy: 3.7928

Guess: 𝙎𝙇𝙐𝙎𝙃, Feedback: ⬜⬜🟩🟩⬜
Actual Info Gain: 4.0000 bits
Posterior entropy: 1.0
entropy_info: {'prior_entropy': 5.0, 'posterior_entropy': 1.0, 'actual_info_gain': 4.0, 'expected_info_gain': 3.7928377974034158}
----------
The bot is making a guess...
2 candidate words remaining.
Top 10 guesses: [('CAUSE', '1.000'), ('PAUSE', '1.000')]
Top entropy choice: 𝘾𝘼𝙐𝙎𝙀 with entropy: 1.0000
Few candidates left, going through them all to pick a common word...
Using common word: 𝘾𝘼𝙐𝙎𝙀 with entropy: 1.0000

Guess: 𝘾𝘼𝙐𝙎𝙀, Feedback: ⬜🟩🟩🟩🟩
Actual Info Gain: 1.0000 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 1.0, 'posterior_entropy': 0.0, 'actual_info_gain': 1.0, 'expected_info_gain': 3.7928377974034158}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('PAUSE', '0.000')]
Top entropy choice: 𝙋𝘼𝙐𝙎𝙀 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝙋𝘼𝙐𝙎𝙀 with entropy: 0.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟩⬜🟨🟨, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 8.859 bits, Posterior Entropy: 5.000, 
Guess: 𝙎𝙇𝙐𝙎𝙃, Feedback: ⬜⬜🟩🟩⬜, Prior Entropy: 5.000, Expected Info Gain: 3.793 bits, Actual Info Gain: 4.000 bits, Posterior Entropy: 1.000, 
Guess: 𝘾𝘼𝙐𝙎𝙀, Feedback: ⬜🟩🟩🟩🟩, Prior Entropy: 1.000, Expected Info Gain: 3.793 bits, Actual Info Gain: 1.000 bits, Posterior Entropy: 0.000, 
Guess: 𝙋𝘼𝙐𝙎𝙀, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'PAUSE': 1.0639 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙁𝙀𝘾𝘼𝙇 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟨⬜🟨⬜
Actual Info Gain: 5.5733 bits
Posterior entropy: 8.285402218862249
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 8.285402218862249, 'actual_info_gain': 5.573258765860528, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
312 candidate words remaining.
Top 10 guesses: [('NEELD', '4.914'), ('PLENE', '4.903'), ('MEDLE', '4.820'), ('ALAND', '4.805'), ('PLANE', '4.795'), ('LEONE', '4.714'), ('BLAND', '4.679'), ('CLINE', '4.662'), ('CLONE', '4.647'), ('ALANE', '4.644')]
Top entropy choice: 𝙉𝙀𝙀𝙇𝘿 with entropy: 4.9138
Guess count low, choosing the word with highest entropy: 𝙉𝙀𝙀𝙇𝘿 with entropy: 4.9138

Guess: 𝙉𝙀𝙀𝙇𝘿, Feedback: ⬜🟩⬜🟨⬜
Actual Info Gain: 3.8931 bits
Posterior entropy: 4.392317422778761
entropy_info: {'prior_entropy': 8.285402218862249, 'posterior_entropy': 4.392317422778761, 'actual_info_gain': 3.893084796083488, 'expected_info_gain': 4.913778257977301}
----------
The bot is making a guess...
21 candidate words remaining.
Top 10 guesses: [('LYMPH', '3.594'), ('BIMAH', '3.459'), ('PHYMA', '3.345'), ('OMLAH', '3.309'), ('BUMPY', '3.291'), ('BUMPH', '3.266'), ('MYNAH', '3.237'), ('MYLAR', '3.237'), ('HALMA', '3.230'), ('CHAMP', '3.196')]
Top entropy choice: 𝙇𝙔𝙈𝙋𝙃 with entropy: 3.5945
Using common exploratory word: 𝙇𝙔𝙈𝙋𝙃 with entropy: 3.5945

Guess: 𝙇𝙔𝙈𝙋𝙃, Feedback: 🟨⬜⬜⬜⬜
Actual Info Gain: 2.3923 bits
Posterior entropy: 2.0
entropy_info: {'prior_entropy': 4.392317422778761, 'posterior_entropy': 2.0, 'actual_info_gain': 2.3923174227787607, 'expected_info_gain': 4.913778257977301}
----------
The bot is making a guess...
4 candidate words remaining.
Top 10 guesses: [('ACCAS', '2.000'), ('ACCHA', '2.000'), ('ACCRA', '2.000'), ('AFLAJ', '2.000'), ('AFLAP', '2.000'), ('BACCA', '2.000'), ('BACCO', '2.000'), ('BACCY', '2.000'), ('BAFFS', '2.000'), ('BAFFY', '2.000')]
Top entropy choice: 𝘼𝘾𝘾𝘼𝙎 with entropy: 2.0000
Guess count high, choosing a candidate with the highest entropy: 𝙁𝙀𝘾𝘼𝙇 with entropy: 2.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟨⬜🟨⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 5.573 bits, Posterior Entropy: 8.285, 
Guess: 𝙉𝙀𝙀𝙇𝘿, Feedback: ⬜🟩⬜🟨⬜, Prior Entropy: 8.285, Expected Info Gain: 4.914 bits, Actual Info Gain: 3.893 bits, Posterior Entropy: 4.392, 
Guess: 𝙇𝙔𝙈𝙋𝙃, Feedback: 🟨⬜⬜⬜⬜, Prior Entropy: 4.392, Expected Info Gain: 4.914 bits, Actual Info Gain: 2.392 bits, Posterior Entropy: 2.000, 
Guess: 𝙁𝙀𝘾𝘼𝙇, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'FECAL': 10.1769 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙂𝙐𝙏𝙎𝙔 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟨⬜⬜⬜🟨
Actual Info Gain: 6.5825 bits
Posterior entropy: 7.2761244052742375
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 7.2761244052742375, 'actual_info_gain': 6.582536579448539, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
155 candidate words remaining.
Top 10 guesses: [('SHOUT', '4.721'), ('HOIST', '4.705'), ('MOUNT', '4.690'), ('MOUST', '4.670'), ('SUINT', '4.665'), ('POINT', '4.665'), ('MOULT', '4.655'), ('MOIST', '4.647'), ('POULT', '4.637'), ('SLUIT', '4.617')]
Top entropy choice: 𝙎𝙃𝙊𝙐𝙏 with entropy: 4.7209
Guess count low, choosing the word with highest entropy: 𝙎𝙃𝙊𝙐𝙏 with entropy: 4.7209

Guess: 𝙎𝙃𝙊𝙐𝙏, Feedback: 🟨⬜⬜🟨🟨
Actual Info Gain: 3.8167 bits
Posterior entropy: 3.4594316186372973
entropy_info: {'prior_entropy': 7.2761244052742375, 'posterior_entropy': 3.4594316186372973, 'actual_info_gain': 3.8166927866369402, 'expected_info_gain': 4.720949920480093}
----------
The bot is making a guess...
11 candidate words remaining.
Top 10 guesses: [('BAGSY', '2.732'), ('GLISK', '2.732'), ('BLING', '2.664'), ('ABYSM', '2.550'), ('AGISM', '2.550'), ('BALTI', '2.550'), ('BASIL', '2.550'), ('BETID', '2.550'), ('BIGLY', '2.550'), ('BILGE', '2.550')]
Top entropy choice: 𝘽𝘼𝙂𝙎𝙔 with entropy: 2.7322
Using common exploratory word: 𝘽𝙇𝙄𝙉𝙂 with entropy: 2.6635

Guess: 𝘽𝙇𝙄𝙉𝙂, Feedback: ⬜⬜⬜⬜🟨
Actual Info Gain: 2.4594 bits
Posterior entropy: 1.0
entropy_info: {'prior_entropy': 3.4594316186372973, 'posterior_entropy': 1.0, 'actual_info_gain': 2.4594316186372973, 'expected_info_gain': 4.720949920480093}
----------
The bot is making a guess...
2 candidate words remaining.
Top 10 guesses: [('GUSTY', '1.000'), ('GUTSY', '1.000')]
Top entropy choice: 𝙂𝙐𝙎𝙏𝙔 with entropy: 1.0000
Guess count high, choosing a candidate with the highest entropy: 𝙂𝙐𝙎𝙏𝙔 with entropy: 1.0000

Guess: 𝙂𝙐𝙎𝙏𝙔, Feedback: 🟩🟩🟨🟨🟩
Actual Info Gain: 1.0000 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 1.0, 'posterior_entropy': 0.0, 'actual_info_gain': 1.0, 'expected_info_gain': 1.0}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('GUTSY', '0.000')]
Top entropy choice: 𝙂𝙐𝙏𝙎𝙔 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝙂𝙐𝙏𝙎𝙔 with entropy: 0.0000
You won! Amount of guesses: 5

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟨⬜⬜⬜🟨, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 6.583 bits, Posterior Entropy: 7.276, 
Guess: 𝙎𝙃𝙊𝙐𝙏, Feedback: 🟨⬜⬜🟨🟨, Prior Entropy: 7.276, Expected Info Gain: 4.721 bits, Actual Info Gain: 3.817 bits, Posterior Entropy: 3.459, 
Guess: 𝘽𝙇𝙄𝙉𝙂, Feedback: ⬜⬜⬜⬜🟨, Prior Entropy: 3.459, Expected Info Gain: 4.721 bits, Actual Info Gain: 2.459 bits, Posterior Entropy: 1.000, 
Guess: 𝙂𝙐𝙎𝙏𝙔, Feedback: 🟩🟩🟨🟨🟩, Prior Entropy: 1.000, Expected Info Gain: 1.000 bits, Actual Info Gain: 1.000 bits, Posterior Entropy: 0.000, 
Guess: 𝙂𝙐𝙏𝙎𝙔, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'GUTSY': 4.8289 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙏𝙍𝙐𝙎𝙏 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟩⬜🟨⬜🟨
Actual Info Gain: 12.2737 bits
Posterior entropy: 1.584962500721156
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 1.584962500721156, 'actual_info_gain': 12.27369848400162, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
3 candidate words remaining.
Top 10 guesses: [('TRIST', '0.918'), ('TRUST', '0.918'), ('TRYST', '0.918')]
Top entropy choice: 𝙏𝙍𝙄𝙎𝙏 with entropy: 0.9183
Guess count low, choosing the word with highest entropy: 𝙏𝙍𝙄𝙎𝙏 with entropy: 0.9183

Guess: 𝙏𝙍𝙄𝙎𝙏, Feedback: 🟩🟩⬜🟩🟩
Actual Info Gain: 0.5850 bits
Posterior entropy: 1.0
entropy_info: {'prior_entropy': 1.584962500721156, 'posterior_entropy': 1.0, 'actual_info_gain': 0.5849625007211561, 'expected_info_gain': 0.9182958340544896}
----------
The bot is making a guess...
2 candidate words remaining.
Top 10 guesses: [('TRUST', '1.000'), ('TRYST', '1.000')]
Top entropy choice: 𝙏𝙍𝙐𝙎𝙏 with entropy: 1.0000
Few candidates left, going through them all to pick a common word...
Using common word: 𝙏𝙍𝙐𝙎𝙏 with entropy: 1.0000
You won! Amount of guesses: 3

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟩⬜🟨⬜🟨, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 12.274 bits, Posterior Entropy: 1.585, 
Guess: 𝙏𝙍𝙄𝙎𝙏, Feedback: 🟩🟩⬜🟩🟩, Prior Entropy: 1.585, Expected Info Gain: 0.918 bits, Actual Info Gain: 0.585 bits, Posterior Entropy: 1.000, 
Guess: 𝙏𝙍𝙐𝙎𝙏, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'TRUST': 0.0056 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝘽𝙇𝙀𝙀𝙋 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜🟩⬜
Actual Info Gain: 4.9914 bits
Posterior entropy: 8.867278739709661
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 8.867278739709661, 'actual_info_gain': 4.991382245013115, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
467 candidate words remaining.
Top 10 guesses: [('INDOL', '4.933'), ('DINLO', '4.859'), ('LOWND', '4.836'), ('BLOND', '4.832'), ('LOUND', '4.773'), ('LINDY', '4.749'), ('DOLCI', '4.730'), ('DOILY', '4.707'), ('NOULD', '4.703'), ('MODIN', '4.688')]
Top entropy choice: 𝙄𝙉𝘿𝙊𝙇 with entropy: 4.9332
Guess count low, choosing the word with highest entropy: 𝙄𝙉𝘿𝙊𝙇 with entropy: 4.9332

Guess: 𝙄𝙉𝘿𝙊𝙇, Feedback: ⬜⬜⬜⬜🟨
Actual Info Gain: 4.3437 bits
Posterior entropy: 4.523561956057013
entropy_info: {'prior_entropy': 8.867278739709661, 'posterior_entropy': 4.523561956057013, 'actual_info_gain': 4.343716783652648, 'expected_info_gain': 4.933171607708794}
----------
The bot is making a guess...
23 candidate words remaining.
Top 10 guesses: [('CLUMP', '3.708'), ('GUCKY', '3.588'), ('BULGE', '3.588'), ('BULGY', '3.588'), ('CLEEP', '3.588'), ('CLEPE', '3.588'), ('PLUMB', '3.588'), ('GULPY', '3.556'), ('GLEBE', '3.534'), ('BLYPE', '3.523')]
Top entropy choice: 𝘾𝙇𝙐𝙈𝙋 with entropy: 3.7081
Using common exploratory word: 𝘾𝙇𝙐𝙈𝙋 with entropy: 3.7081

Guess: 𝘾𝙇𝙐𝙈𝙋, Feedback: ⬜🟩⬜⬜🟩
Actual Info Gain: 3.5236 bits
Posterior entropy: 1.0
entropy_info: {'prior_entropy': 4.523561956057013, 'posterior_entropy': 1.0, 'actual_info_gain': 3.523561956057013, 'expected_info_gain': 4.933171607708794}
----------
The bot is making a guess...
2 candidate words remaining.
Top 10 guesses: [('BLEEP', '1.000'), ('PLEEP', '1.000')]
Top entropy choice: 𝘽𝙇𝙀𝙀𝙋 with entropy: 1.0000
Guess count high, choosing a candidate with the highest entropy: 𝘽𝙇𝙀𝙀𝙋 with entropy: 1.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜🟩⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 4.991 bits, Posterior Entropy: 8.867, 
Guess: 𝙄𝙉𝘿𝙊𝙇, Feedback: ⬜⬜⬜⬜🟨, Prior Entropy: 8.867, Expected Info Gain: 4.933 bits, Actual Info Gain: 4.344 bits, Posterior Entropy: 4.524, 
Guess: 𝘾𝙇𝙐𝙈𝙋, Feedback: ⬜🟩⬜⬜🟩, Prior Entropy: 4.524, Expected Info Gain: 4.933 bits, Actual Info Gain: 3.524 bits, Posterior Entropy: 1.000, 
Guess: 𝘽𝙇𝙀𝙀𝙋, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'BLEEP': 15.5848 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝘿𝙊𝙐𝘽𝙏 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟨⬜⬜⬜⬜
Actual Info Gain: 5.5779 bits
Posterior entropy: 8.280770770130603
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 8.280770770130603, 'actual_info_gain': 5.577890214592173, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
311 candidate words remaining.
Top 10 guesses: [('COLIN', '5.317'), ('CUNIT', '5.247'), ('PIONY', '5.209'), ('COUNT', '5.200'), ('NICOL', '5.186'), ('NITTO', '5.181'), ('PITOT', '5.173'), ('LITHO', '5.162'), ('PILOT', '5.155'), ('POTIN', '5.150')]
Top entropy choice: 𝘾𝙊𝙇𝙄𝙉 with entropy: 5.3169
Guess count low, choosing the word with highest entropy: 𝘾𝙊𝙇𝙄𝙉 with entropy: 5.3169

Guess: 𝘾𝙊𝙇𝙄𝙉, Feedback: ⬜🟩⬜⬜⬜
Actual Info Gain: 3.2808 bits
Posterior entropy: 5.0
entropy_info: {'prior_entropy': 8.280770770130603, 'posterior_entropy': 5.0, 'actual_info_gain': 3.2807707701306033, 'expected_info_gain': 5.316896712755155}
----------
The bot is making a guess...
32 candidate words remaining.
Top 10 guesses: [('HUMPY', '3.948'), ('BUTTY', '3.762'), ('POOHY', '3.757'), ('BUMPH', '3.742'), ('BOOTH', '3.679'), ('BOMOH', '3.668'), ('PHOTY', '3.656'), ('BUMPY', '3.636'), ('PUTTY', '3.614'), ('UMPTY', '3.608')]
Top entropy choice: 𝙃𝙐𝙈𝙋𝙔 with entropy: 3.9484
Using common exploratory word: 𝘽𝙊𝙊𝙏𝙃 with entropy: 3.6792

Guess: 𝘽𝙊𝙊𝙏𝙃, Feedback: 🟨🟩⬜🟨⬜
Actual Info Gain: 5.0000 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 5.0, 'posterior_entropy': 0.0, 'actual_info_gain': 5.0, 'expected_info_gain': 5.316896712755155}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('DOUBT', '0.000')]
Top entropy choice: 𝘿𝙊𝙐𝘽𝙏 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝘿𝙊𝙐𝘽𝙏 with entropy: 0.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟨⬜⬜⬜⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 5.578 bits, Posterior Entropy: 8.281, 
Guess: 𝘾𝙊𝙇𝙄𝙉, Feedback: ⬜🟩⬜⬜⬜, Prior Entropy: 8.281, Expected Info Gain: 5.317 bits, Actual Info Gain: 3.281 bits, Posterior Entropy: 5.000, 
Guess: 𝘽𝙊𝙊𝙏𝙃, Feedback: 🟨🟩⬜🟨⬜, Prior Entropy: 5.000, Expected Info Gain: 5.317 bits, Actual Info Gain: 5.000 bits, Posterior Entropy: 0.000, 
Guess: 𝘿𝙊𝙐𝘽𝙏, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'DOUBT': 10.2623 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙒𝘼𝙇𝙇𝘼 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟩⬜⬜⬜
Actual Info Gain: 4.8308 bits
Posterior entropy: 9.027905996569885
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 9.027905996569885, 'actual_info_gain': 4.830754988152892, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
522 candidate words remaining.
Top 10 guesses: [('MINCY', '4.781'), ('LINDY', '4.711'), ('MILKY', '4.698'), ('KYLIN', '4.687'), ('LINKY', '4.677'), ('MINGY', '4.632'), ('PYLON', '4.632'), ('MOLDY', '4.626'), ('DIMLY', '4.626'), ('LINGY', '4.613')]
Top entropy choice: 𝙈𝙄𝙉𝘾𝙔 with entropy: 4.7811
Guess count low, choosing the word with highest entropy: 𝙈𝙄𝙉𝘾𝙔 with entropy: 4.7811

Guess: 𝙈𝙄𝙉𝘾𝙔, Feedback: ⬜⬜⬜⬜⬜
Actual Info Gain: 2.6016 bits
Posterior entropy: 6.426264754702098
entropy_info: {'prior_entropy': 9.027905996569885, 'posterior_entropy': 6.426264754702098, 'actual_info_gain': 2.601641241867787, 'expected_info_gain': 4.781133838721642}
----------
The bot is making a guess...
86 candidate words remaining.
Top 10 guesses: [('PULAO', '4.670'), ('PALAK', '4.645'), ('GULPH', '4.630'), ('PODAL', '4.585'), ('HULLO', '4.572'), ('KALPA', '4.533'), ('POLKA', '4.530'), ('PULKA', '4.496'), ('PLOUK', '4.471'), ('GHOUL', '4.462')]
Top entropy choice: 𝙋𝙐𝙇𝘼𝙊 with entropy: 4.6700
Using common exploratory word: 𝙋𝙊𝙇𝙆𝘼 with entropy: 4.5296

Guess: 𝙋𝙊𝙇𝙆𝘼, Feedback: ⬜⬜🟩⬜🟩
Actual Info Gain: 4.4263 bits
Posterior entropy: 2.0
entropy_info: {'prior_entropy': 6.426264754702098, 'posterior_entropy': 2.0, 'actual_info_gain': 4.426264754702098, 'expected_info_gain': 4.781133838721642}
----------
The bot is making a guess...
4 candidate words remaining.
Top 10 guesses: [('AVOWS', '2.000'), ('FATWA', '2.000'), ('FETWA', '2.000'), ('FLAWN', '2.000'), ('FLAWS', '2.000'), ('FLAWY', '2.000'), ('FLEWS', '2.000'), ('FLOWN', '2.000'), ('FLOWS', '2.000'), ('FLOWY', '2.000')]
Top entropy choice: 𝘼𝙑𝙊𝙒𝙎 with entropy: 2.0000
Guess count high, choosing a candidate with the highest entropy: 𝙒𝘼𝙇𝙇𝘼 with entropy: 1.5000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟩⬜⬜⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 4.831 bits, Posterior Entropy: 9.028, 
Guess: 𝙈𝙄𝙉𝘾𝙔, Feedback: ⬜⬜⬜⬜⬜, Prior Entropy: 9.028, Expected Info Gain: 4.781 bits, Actual Info Gain: 2.602 bits, Posterior Entropy: 6.426, 
Guess: 𝙋𝙊𝙇𝙆𝘼, Feedback: ⬜⬜🟩⬜🟩, Prior Entropy: 6.426, Expected Info Gain: 4.781 bits, Actual Info Gain: 4.426 bits, Posterior Entropy: 2.000, 
Guess: 𝙒𝘼𝙇𝙇𝘼, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'WALLA': 20.4313 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙁𝙍𝙊𝙉𝙏 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟨⬜🟨⬜⬜
Actual Info Gain: 7.7089 bits
Posterior entropy: 6.149747119504682
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 6.149747119504682, 'actual_info_gain': 7.708913865218094, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
71 candidate words remaining.
Top 10 guesses: [('PITOT', '4.608'), ('POINT', '4.505'), ('PICOT', '4.479'), ('NOOIT', '4.471'), ('RHINO', '4.428'), ('PINOT', '4.414'), ('BIONT', '4.414'), ('FRUIT', '4.405'), ('GRIOT', '4.404'), ('FOUTH', '4.392')]
Top entropy choice: 𝙋𝙄𝙏𝙊𝙏 with entropy: 4.6083
Guess count low, choosing the word with highest entropy: 𝙋𝙄𝙏𝙊𝙏 with entropy: 4.6083

Guess: 𝙋𝙄𝙏𝙊𝙏, Feedback: ⬜⬜⬜🟨🟩
Actual Info Gain: 3.3424 bits
Posterior entropy: 2.807354922057604
entropy_info: {'prior_entropy': 6.149747119504682, 'posterior_entropy': 2.807354922057604, 'actual_info_gain': 3.342392197447078, 'expected_info_gain': 4.608311423783217}
----------
The bot is making a guess...
7 candidate words remaining.
Top 10 guesses: [('ADUNC', '2.807'), ('ARCUS', '2.807'), ('BRUCK', '2.807'), ('CAJUN', '2.807'), ('CAURI', '2.807'), ('CHIRU', '2.807'), ('CHOOF', '2.807'), ('CHOUT', '2.807'), ('CHOUX', '2.807'), ('CHUBS', '2.807')]
Top entropy choice: 𝘼𝘿𝙐𝙉𝘾 with entropy: 2.8074
Using common exploratory word: 𝘾𝘼𝙅𝙐𝙉 with entropy: 2.8074

Guess: 𝘾𝘼𝙅𝙐𝙉, Feedback: ⬜⬜⬜⬜🟨
Actual Info Gain: 2.8074 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 2.807354922057604, 'posterior_entropy': 0.0, 'actual_info_gain': 2.807354922057604, 'expected_info_gain': 4.608311423783217}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('FRONT', '0.000')]
Top entropy choice: 𝙁𝙍𝙊𝙉𝙏 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝙁𝙍𝙊𝙉𝙏 with entropy: 0.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟨⬜🟨⬜⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 7.709 bits, Posterior Entropy: 6.150, 
Guess: 𝙋𝙄𝙏𝙊𝙏, Feedback: ⬜⬜⬜🟨🟩, Prior Entropy: 6.150, Expected Info Gain: 4.608 bits, Actual Info Gain: 3.342 bits, Posterior Entropy: 2.807, 
Guess: 𝘾𝘼𝙅𝙐𝙉, Feedback: ⬜⬜⬜⬜🟨, Prior Entropy: 2.807, Expected Info Gain: 4.608 bits, Actual Info Gain: 2.807 bits, Posterior Entropy: 0.000, 
Guess: 𝙁𝙍𝙊𝙉𝙏, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'FRONT': 2.4124 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙍𝘼𝘿𝙊𝙉 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟩🟨⬜⬜
Actual Info Gain: 6.9398 bits
Posterior entropy: 6.918863237274595
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 6.918863237274595, 'actual_info_gain': 6.939797747448182, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
121 candidate words remaining.
Top 10 guesses: [('RINDY', '4.228'), ('RYPIN', '4.212'), ('KYLIN', '4.200'), ('ROBIN', '4.135'), ('MODIN', '4.120'), ('LINDY', '4.119'), ('RUBIN', '4.087'), ('RANID', '4.047'), ('UNLID', '4.029'), ('FLYIN', '4.013')]
Top entropy choice: 𝙍𝙄𝙉𝘿𝙔 with entropy: 4.2285
Guess count low, choosing the word with highest entropy: 𝙍𝙄𝙉𝘿𝙔 with entropy: 4.2285

Guess: 𝙍𝙄𝙉𝘿𝙔, Feedback: 🟩⬜🟨🟨⬜
Actual Info Gain: 6.9189 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 6.918863237274595, 'posterior_entropy': 0.0, 'actual_info_gain': 6.918863237274595, 'expected_info_gain': 4.2284561813662584}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('RADON', '0.000')]
Top entropy choice: 𝙍𝘼𝘿𝙊𝙉 with entropy: 0.0000
Few candidates left, going through them all to pick a common word...
Using common word: 𝙍𝘼𝘿𝙊𝙉 with entropy: 0.0000
You won! Amount of guesses: 3

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟩🟨⬜⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 6.940 bits, Posterior Entropy: 6.919, 
Guess: 𝙍𝙄𝙉𝘿𝙔, Feedback: 🟩⬜🟨🟨⬜, Prior Entropy: 6.919, Expected Info Gain: 4.228 bits, Actual Info Gain: 6.919 bits, Posterior Entropy: 0.000, 
Guess: 𝙍𝘼𝘿𝙊𝙉, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'RADON': 4.1500 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙁𝙊𝙍𝙀𝙓 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜🟩🟩⬜
Actual Info Gain: 8.0513 bits
Posterior entropy: 5.807354922057604
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 5.807354922057604, 'actual_info_gain': 8.051306062665173, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
56 candidate words remaining.
Top 10 guesses: [('MOULD', '3.981'), ('DUOMI', '3.974'), ('DOMIC', '3.884'), ('DOLCI', '3.836'), ('COULD', '3.823'), ('DUMBO', '3.814'), ('DOILY', '3.778'), ('DEMOI', '3.766'), ('LUCID', '3.759'), ('CLOUD', '3.745')]
Top entropy choice: 𝙈𝙊𝙐𝙇𝘿 with entropy: 3.9805
Guess count low, choosing the word with highest entropy: 𝙈𝙊𝙐𝙇𝘿 with entropy: 3.9805

Guess: 𝙈𝙊𝙐𝙇𝘿, Feedback: ⬜🟩⬜⬜⬜
Actual Info Gain: 2.8074 bits
Posterior entropy: 3.0
entropy_info: {'prior_entropy': 5.807354922057604, 'posterior_entropy': 3.0, 'actual_info_gain': 2.8073549220576037, 'expected_info_gain': 3.980502019846924}
----------
The bot is making a guess...
8 candidate words remaining.
Top 10 guesses: [('CYBER', '2.750'), ('BACKY', '2.500'), ('BICKY', '2.500'), ('BLYPE', '2.500'), ('CABER', '2.500'), ('CUBER', '2.500'), ('UPBYE', '2.500'), ('CAPER', '2.406'), ('CLYPE', '2.406'), ('COPER', '2.406')]
Top entropy choice: 𝘾𝙔𝘽𝙀𝙍 with entropy: 2.7500
Using common exploratory word: 𝘾𝙔𝘽𝙀𝙍 with entropy: 2.7500

Guess: 𝘾𝙔𝘽𝙀𝙍, Feedback: ⬜⬜⬜🟩🟨
Actual Info Gain: 3.0000 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 3.0, 'posterior_entropy': 0.0, 'actual_info_gain': 3.0, 'expected_info_gain': 3.980502019846924}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('FOREX', '0.000')]
Top entropy choice: 𝙁𝙊𝙍𝙀𝙓 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝙁𝙊𝙍𝙀𝙓 with entropy: 0.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜🟩🟩⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 8.051 bits, Posterior Entropy: 5.807, 
Guess: 𝙈𝙊𝙐𝙇𝘿, Feedback: ⬜🟩⬜⬜⬜, Prior Entropy: 5.807, Expected Info Gain: 3.981 bits, Actual Info Gain: 2.807 bits, Posterior Entropy: 3.000, 
Guess: 𝘾𝙔𝘽𝙀𝙍, Feedback: ⬜⬜⬜🟩🟨, Prior Entropy: 3.000, Expected Info Gain: 3.981 bits, Actual Info Gain: 3.000 bits, Posterior Entropy: 0.000, 
Guess: 𝙁𝙊𝙍𝙀𝙓, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'FOREX': 2.1510 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝘿𝙀𝘼𝙏𝙃 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟨🟨⬜🟨⬜
Actual Info Gain: 7.5188 bits
Posterior entropy: 6.339850002884624
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 6.339850002884624, 'actual_info_gain': 7.518810981838152, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
81 candidate words remaining.
Top 10 guesses: [('LEANT', '4.217'), ('PLANE', '4.209'), ('LEAPT', '4.159'), ('PLANT', '4.109'), ('ALANT', '4.106'), ('PLANH', '4.101'), ('ALATE', '4.080'), ('PLATE', '4.060'), ('PLANC', '4.035'), ('PENAL', '4.002')]
Top entropy choice: 𝙇𝙀𝘼𝙉𝙏 with entropy: 4.2173
Guess count low, choosing the word with highest entropy: 𝙇𝙀𝘼𝙉𝙏 with entropy: 4.2173

Guess: 𝙇𝙀𝘼𝙉𝙏, Feedback: ⬜🟩🟩⬜🟨
Actual Info Gain: 3.3399 bits
Posterior entropy: 3.0
entropy_info: {'prior_entropy': 6.339850002884624, 'posterior_entropy': 3.0, 'actual_info_gain': 3.3398500028846243, 'expected_info_gain': 4.217349936796832}
----------
The bot is making a guess...
8 candidate words remaining.
Top 10 guesses: [('BIMAH', '2.750'), ('BOMOH', '2.750'), ('BUMPH', '2.750'), ('BUMPY', '2.750'), ('HAMBA', '2.750'), ('HEMPY', '2.750'), ('HIMBO', '2.750'), ('HUMPH', '2.750'), ('HUMPY', '2.750'), ('PAMBY', '2.750')]
Top entropy choice: 𝘽𝙄𝙈𝘼𝙃 with entropy: 2.7500
Using common exploratory word: 𝘽𝙐𝙈𝙋𝙔 with entropy: 2.7500

Guess: 𝘽𝙐𝙈𝙋𝙔, Feedback: ⬜⬜⬜⬜⬜
Actual Info Gain: 2.0000 bits
Posterior entropy: 1.0
entropy_info: {'prior_entropy': 3.0, 'posterior_entropy': 1.0, 'actual_info_gain': 2.0, 'expected_info_gain': 4.217349936796832}
----------
The bot is making a guess...
2 candidate words remaining.
Top 10 guesses: [('DEATH', '1.000'), ('HEATH', '1.000')]
Top entropy choice: 𝘿𝙀𝘼𝙏𝙃 with entropy: 1.0000
Guess count high, choosing a candidate with the highest entropy: 𝘿𝙀𝘼𝙏𝙃 with entropy: 1.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟨🟨⬜🟨⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 7.519 bits, Posterior Entropy: 6.340, 
Guess: 𝙇𝙀𝘼𝙉𝙏, Feedback: ⬜🟩🟩⬜🟨, Prior Entropy: 6.340, Expected Info Gain: 4.217 bits, Actual Info Gain: 3.340 bits, Posterior Entropy: 3.000, 
Guess: 𝘽𝙐𝙈𝙋𝙔, Feedback: ⬜⬜⬜⬜⬜, Prior Entropy: 3.000, Expected Info Gain: 4.217 bits, Actual Info Gain: 2.000 bits, Posterior Entropy: 1.000, 
Guess: 𝘿𝙀𝘼𝙏𝙃, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'DEATH': 2.8467 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙁𝙊𝙐𝙉𝘿 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜⬜⬜
Actual Info Gain: 3.8615 bits
Posterior entropy: 9.997179480937621
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 9.997179480937621, 'actual_info_gain': 3.8614815037851553, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
1022 candidate words remaining.
Top 10 guesses: [('COLIN', '5.938'), ('NOILY', '5.864'), ('DINLO', '5.841'), ('PIONY', '5.791'), ('LINGO', '5.749'), ('PILON', '5.742'), ('PINOL', '5.704'), ('LOGIN', '5.699'), ('DOILY', '5.681'), ('NICOL', '5.680')]
Top entropy choice: 𝘾𝙊𝙇𝙄𝙉 with entropy: 5.9379
Guess count low, choosing the word with highest entropy: 𝘾𝙊𝙇𝙄𝙉 with entropy: 5.9379

Guess: 𝘾𝙊𝙇𝙄𝙉, Feedback: ⬜🟩⬜⬜🟨
Actual Info Gain: 4.5377 bits
Posterior entropy: 5.459431618637297
entropy_info: {'prior_entropy': 9.997179480937621, 'posterior_entropy': 5.459431618637297, 'actual_info_gain': 4.537747862300324, 'expected_info_gain': 5.937896650379372}
----------
The bot is making a guess...
44 candidate words remaining.
Top 10 guesses: [('DUNNY', '4.212'), ('BUNDY', '4.162'), ('DUNGY', '4.152'), ('GUNDY', '4.152'), ('GUNNY', '4.067'), ('DUNNO', '4.036'), ('BUNNY', '4.033'), ('NYUNG', '4.030'), ('MUNGY', '4.013'), ('PUNNY', '3.987')]
Top entropy choice: 𝘿𝙐𝙉𝙉𝙔 with entropy: 4.2118
Using common exploratory word: 𝘽𝙐𝙉𝘿𝙔 with entropy: 4.1619

Guess: 𝘽𝙐𝙉𝘿𝙔, Feedback: ⬜🟨🟨🟨⬜
Actual Info Gain: 2.8745 bits
Posterior entropy: 2.584962500721156
entropy_info: {'prior_entropy': 5.459431618637297, 'posterior_entropy': 2.584962500721156, 'actual_info_gain': 2.8744691179161412, 'expected_info_gain': 5.937896650379372}
----------
The bot is making a guess...
6 candidate words remaining.
Top 10 guesses: [('WHUMP', '2.585'), ('CHUMP', '2.252'), ('FLUMP', '2.252'), ('FRUMP', '2.252'), ('MAHWA', '2.252'), ('MAWPS', '2.252'), ('MORPH', '2.252'), ('MUMPH', '2.252'), ('NYMPH', '2.252'), ('PASHM', '2.252')]
Top entropy choice: 𝙒𝙃𝙐𝙈𝙋 with entropy: 2.5850
Guess count high, choosing a candidate with the highest entropy: 𝙁𝙊𝙐𝙉𝘿 with entropy: 1.2516
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜⬜⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 3.861 bits, Posterior Entropy: 9.997, 
Guess: 𝘾𝙊𝙇𝙄𝙉, Feedback: ⬜🟩⬜⬜🟨, Prior Entropy: 9.997, Expected Info Gain: 5.938 bits, Actual Info Gain: 4.538 bits, Posterior Entropy: 5.459, 
Guess: 𝘽𝙐𝙉𝘿𝙔, Feedback: ⬜🟨🟨🟨⬜, Prior Entropy: 5.459, Expected Info Gain: 5.938 bits, Actual Info Gain: 2.874 bits, Posterior Entropy: 2.585, 
Guess: 𝙁𝙊𝙐𝙉𝘿, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'FOUND': 32.7182 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙊𝘿𝘿𝙇𝙔 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜⬜⬜
Actual Info Gain: 3.8615 bits
Posterior entropy: 9.997179480937621
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 9.997179480937621, 'actual_info_gain': 3.8614815037851553, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
1022 candidate words remaining.
Top 10 guesses: [('COLIN', '5.938'), ('NOILY', '5.864'), ('DINLO', '5.841'), ('PIONY', '5.791'), ('LINGO', '5.749'), ('PILON', '5.742'), ('PINOL', '5.704'), ('LOGIN', '5.699'), ('DOILY', '5.681'), ('NICOL', '5.680')]
Top entropy choice: 𝘾𝙊𝙇𝙄𝙉 with entropy: 5.9379
Guess count low, choosing the word with highest entropy: 𝘾𝙊𝙇𝙄𝙉 with entropy: 5.9379

Guess: 𝘾𝙊𝙇𝙄𝙉, Feedback: ⬜🟨🟨⬜⬜
Actual Info Gain: 5.4736 bits
Posterior entropy: 4.523561956057013
entropy_info: {'prior_entropy': 9.997179480937621, 'posterior_entropy': 4.523561956057013, 'actual_info_gain': 5.473617524880608, 'expected_info_gain': 5.937896650379372}
----------
The bot is making a guess...
23 candidate words remaining.
Top 10 guesses: [('BLOOP', '3.697'), ('PUDGY', '3.621'), ('BLOOK', '3.610'), ('GLOOP', '3.551'), ('BLUDY', '3.518'), ('FUDGY', '3.501'), ('BOOGY', '3.447'), ('BLOOD', '3.447'), ('BODGY', '3.431'), ('BLOOM', '3.414')]
Top entropy choice: 𝘽𝙇𝙊𝙊𝙋 with entropy: 3.6966
Using common exploratory word: 𝘽𝙇𝙊𝙊𝘿 with entropy: 3.4473

Guess: 𝘽𝙇𝙊𝙊𝘿, Feedback: ⬜🟨🟨⬜🟨
Actual Info Gain: 4.5236 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 4.523561956057013, 'posterior_entropy': 0.0, 'actual_info_gain': 4.523561956057013, 'expected_info_gain': 5.937896650379372}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('ODDLY', '0.000')]
Top entropy choice: 𝙊𝘿𝘿𝙇𝙔 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝙊𝘿𝘿𝙇𝙔 with entropy: 0.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜⬜⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 3.861 bits, Posterior Entropy: 9.997, 
Guess: 𝘾𝙊𝙇𝙄𝙉, Feedback: ⬜🟨🟨⬜⬜, Prior Entropy: 9.997, Expected Info Gain: 5.938 bits, Actual Info Gain: 5.474 bits, Posterior Entropy: 4.524, 
Guess: 𝘽𝙇𝙊𝙊𝘿, Feedback: ⬜🟨🟨⬜🟨, Prior Entropy: 4.524, Expected Info Gain: 5.938 bits, Actual Info Gain: 4.524 bits, Posterior Entropy: 0.000, 
Guess: 𝙊𝘿𝘿𝙇𝙔, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'ODDLY': 32.1665 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙏𝙊𝙍𝘼𝙃 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟩🟨🟩⬜⬜
Actual Info Gain: 11.5367 bits
Posterior entropy: 2.321928094887362
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 2.321928094887362, 'actual_info_gain': 11.536732889835415, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
5 candidate words remaining.
Top 10 guesses: [('AIYAH', '2.322'), ('ALOHA', '2.322'), ('ANCHO', '2.322'), ('ANOAS', '2.322'), ('AROHA', '2.322'), ('AZOTH', '2.322'), ('BENTO', '2.322'), ('BIHON', '2.322'), ('BOGAN', '2.322'), ('BOHEA', '2.322')]
Top entropy choice: 𝘼𝙄𝙔𝘼𝙃 with entropy: 2.3219
Guess count low, choosing the word with highest entropy: 𝘼𝙄𝙔𝘼𝙃 with entropy: 2.3219

Guess: 𝘼𝙄𝙔𝘼𝙃, Feedback: ⬜⬜⬜🟩🟩
Actual Info Gain: 2.3219 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 2.321928094887362, 'posterior_entropy': 0.0, 'actual_info_gain': 2.321928094887362, 'expected_info_gain': 2.321928094887362}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('TORAH', '0.000')]
Top entropy choice: 𝙏𝙊𝙍𝘼𝙃 with entropy: 0.0000
Few candidates left, going through them all to pick a common word...
Using common word: 𝙏𝙊𝙍𝘼𝙃 with entropy: 0.0000
You won! Amount of guesses: 3

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟩🟨🟩⬜⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 11.537 bits, Posterior Entropy: 2.322, 
Guess: 𝘼𝙄𝙔𝘼𝙃, Feedback: ⬜⬜⬜🟩🟩, Prior Entropy: 2.322, Expected Info Gain: 2.322 bits, Actual Info Gain: 2.322 bits, Posterior Entropy: 0.000, 
Guess: 𝙏𝙊𝙍𝘼𝙃, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'TORAH': 0.2041 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙋𝙄𝙋𝙀𝙍 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜🟨🟩⬜
Actual Info Gain: 5.5966 bits
Posterior entropy: 8.262094845370179
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 8.262094845370179, 'actual_info_gain': 5.596566139352598, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
307 candidate words remaining.
Top 10 guesses: [('POIND', '4.424'), ('DINLO', '4.295'), ('NIDOR', '4.259'), ('BIPOD', '4.172'), ('PINOL', '4.170'), ('PILON', '4.152'), ('PRIOR', '4.152'), ('VIOLD', '4.149'), ('INDOL', '4.122'), ('BIDON', '4.115')]
Top entropy choice: 𝙋𝙊𝙄𝙉𝘿 with entropy: 4.4242
Guess count low, choosing the word with highest entropy: 𝙋𝙊𝙄𝙉𝘿 with entropy: 4.4242

Guess: 𝙋𝙊𝙄𝙉𝘿, Feedback: 🟩⬜🟨⬜⬜
Actual Info Gain: 6.2621 bits
Posterior entropy: 2.0
entropy_info: {'prior_entropy': 8.262094845370179, 'posterior_entropy': 2.0, 'actual_info_gain': 6.262094845370179, 'expected_info_gain': 4.424196059405291}
----------
The bot is making a guess...
4 candidate words remaining.
Top 10 guesses: [('KLUTZ', '2.000'), ('LEPAK', '2.000'), ('AIZLE', '1.500'), ('AKELA', '1.500'), ('ALACK', '1.500'), ('ALEAK', '1.500'), ('ALECK', '1.500'), ('ALICK', '1.500'), ('ALIKE', '1.500'), ('ALKIE', '1.500')]
Top entropy choice: 𝙆𝙇𝙐𝙏𝙕 with entropy: 2.0000

Guess: 𝙆𝙇𝙐𝙏𝙕, Feedback: ⬜⬜⬜⬜⬜
Actual Info Gain: 2.0000 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 2.0, 'posterior_entropy': 0.0, 'actual_info_gain': 2.0, 'expected_info_gain': 2.0}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('PIPER', '0.000')]
Top entropy choice: 𝙋𝙄𝙋𝙀𝙍 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝙋𝙄𝙋𝙀𝙍 with entropy: 0.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜🟨🟩⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 5.597 bits, Posterior Entropy: 8.262, 
Guess: 𝙋𝙊𝙄𝙉𝘿, Feedback: 🟩⬜🟨⬜⬜, Prior Entropy: 8.262, Expected Info Gain: 4.424 bits, Actual Info Gain: 6.262 bits, Posterior Entropy: 2.000, 
Guess: 𝙆𝙇𝙐𝙏𝙕, Feedback: ⬜⬜⬜⬜⬜, Prior Entropy: 2.000, Expected Info Gain: 2.000 bits, Actual Info Gain: 2.000 bits, Posterior Entropy: 0.000, 
Guess: 𝙋𝙄𝙋𝙀𝙍, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'PIPER': 8.9281 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙎𝙆𝙐𝙇𝙇 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜⬜🟨
Actual Info Gain: 5.4967 bits
Posterior entropy: 8.361943773735241
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 8.361943773735241, 'actual_info_gain': 5.496717210987535, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
329 candidate words remaining.
Top 10 guesses: [('SOILY', '5.323'), ('NOILY', '5.322'), ('PIONY', '5.294'), ('PINOL', '5.237'), ('PILON', '5.236'), ('SHIOK', '5.193'), ('CHOIL', '5.184'), ('SPOIL', '5.183'), ('SHILY', '5.157'), ('SOULY', '5.152')]
Top entropy choice: 𝙎𝙊𝙄𝙇𝙔 with entropy: 5.3226
Guess count low, choosing the word with highest entropy: 𝙎𝙊𝙄𝙇𝙔 with entropy: 5.3226

Guess: 𝙎𝙊𝙄𝙇𝙔, Feedback: 🟩⬜⬜🟩⬜
Actual Info Gain: 5.7770 bits
Posterior entropy: 2.584962500721156
entropy_info: {'prior_entropy': 8.361943773735241, 'posterior_entropy': 2.584962500721156, 'actual_info_gain': 5.776981273014085, 'expected_info_gain': 5.322573042242079}
----------
The bot is making a guess...
6 candidate words remaining.
Top 10 guesses: [('PLACK', '2.585'), ('PLECK', '2.585'), ('PLOCK', '2.585'), ('PLUCK', '2.585'), ('PRICK', '2.585'), ('SPACK', '2.585'), ('SPECK', '2.585'), ('SPICK', '2.585'), ('CAPUL', '2.252'), ('COPAL', '2.252')]
Top entropy choice: 𝙋𝙇𝘼𝘾𝙆 with entropy: 2.5850
Using common exploratory word: 𝙋𝙇𝙐𝘾𝙆 with entropy: 2.5850

Guess: 𝙋𝙇𝙐𝘾𝙆, Feedback: ⬜🟨🟩⬜🟨
Actual Info Gain: 2.5850 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 2.584962500721156, 'posterior_entropy': 0.0, 'actual_info_gain': 2.584962500721156, 'expected_info_gain': 5.322573042242079}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('SKULL', '0.000')]
Top entropy choice: 𝙎𝙆𝙐𝙇𝙇 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝙎𝙆𝙐𝙇𝙇 with entropy: 0.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜⬜🟨, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 5.497 bits, Posterior Entropy: 8.362, 
Guess: 𝙎𝙊𝙄𝙇𝙔, Feedback: 🟩⬜⬜🟩⬜, Prior Entropy: 8.362, Expected Info Gain: 5.323 bits, Actual Info Gain: 5.777 bits, Posterior Entropy: 2.585, 
Guess: 𝙋𝙇𝙐𝘾𝙆, Feedback: ⬜🟨🟩⬜🟨, Prior Entropy: 2.585, Expected Info Gain: 5.323 bits, Actual Info Gain: 2.585 bits, Posterior Entropy: 0.000, 
Guess: 𝙎𝙆𝙐𝙇𝙇, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'SKULL': 10.3229 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙎𝙃𝙊𝙊𝙆 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜⬜🟨
Actual Info Gain: 5.4967 bits
Posterior entropy: 8.361943773735241
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 8.361943773735241, 'actual_info_gain': 5.496717210987535, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
329 candidate words remaining.
Top 10 guesses: [('SOILY', '5.323'), ('NOILY', '5.322'), ('PIONY', '5.294'), ('PINOL', '5.237'), ('PILON', '5.236'), ('SHIOK', '5.193'), ('CHOIL', '5.184'), ('SPOIL', '5.183'), ('SHILY', '5.157'), ('SOULY', '5.152')]
Top entropy choice: 𝙎𝙊𝙄𝙇𝙔 with entropy: 5.3226
Guess count low, choosing the word with highest entropy: 𝙎𝙊𝙄𝙇𝙔 with entropy: 5.3226

Guess: 𝙎𝙊𝙄𝙇𝙔, Feedback: 🟩🟨⬜⬜⬜
Actual Info Gain: 3.3175 bits
Posterior entropy: 5.044394119358453
entropy_info: {'prior_entropy': 8.361943773735241, 'posterior_entropy': 5.044394119358453, 'actual_info_gain': 3.317549654376788, 'expected_info_gain': 5.322573042242079}
----------
The bot is making a guess...
33 candidate words remaining.
Top 10 guesses: [('KNOOP', '4.196'), ('WHOOP', '4.029'), ('CHUNK', '4.014'), ('CHOON', '4.003'), ('PHONO', '3.980'), ('CHOOK', '3.941'), ('CHOKO', '3.881'), ('PUNCH', '3.859'), ('CHUMP', '3.798'), ('WHUMP', '3.795')]
Top entropy choice: 𝙆𝙉𝙊𝙊𝙋 with entropy: 4.1959
Using common exploratory word: 𝙒𝙃𝙊𝙊𝙋 with entropy: 4.0289

Guess: 𝙒𝙃𝙊𝙊𝙋, Feedback: ⬜🟩🟩🟩⬜
Actual Info Gain: 4.0444 bits
Posterior entropy: 1.0
entropy_info: {'prior_entropy': 5.044394119358453, 'posterior_entropy': 1.0, 'actual_info_gain': 4.044394119358453, 'expected_info_gain': 5.322573042242079}
----------
The bot is making a guess...
2 candidate words remaining.
Top 10 guesses: [('SHOOK', '1.000'), ('SHOON', '1.000')]
Top entropy choice: 𝙎𝙃𝙊𝙊𝙆 with entropy: 1.0000
Guess count high, choosing a candidate with the highest entropy: 𝙎𝙃𝙊𝙊𝙆 with entropy: 1.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜⬜🟨, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 5.497 bits, Posterior Entropy: 8.362, 
Guess: 𝙎𝙊𝙄𝙇𝙔, Feedback: 🟩🟨⬜⬜⬜, Prior Entropy: 8.362, Expected Info Gain: 5.323 bits, Actual Info Gain: 3.318 bits, Posterior Entropy: 5.044, 
Guess: 𝙒𝙃𝙊𝙊𝙋, Feedback: ⬜🟩🟩🟩⬜, Prior Entropy: 5.044, Expected Info Gain: 5.323 bits, Actual Info Gain: 4.044 bits, Posterior Entropy: 1.000, 
Guess: 𝙎𝙃𝙊𝙊𝙆, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'SHOOK': 10.6973 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙋𝙊𝙋𝙋𝙔 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜⬜⬜
Actual Info Gain: 3.8615 bits
Posterior entropy: 9.997179480937621
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 9.997179480937621, 'actual_info_gain': 3.8614815037851553, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
1022 candidate words remaining.
Top 10 guesses: [('COLIN', '5.938'), ('NOILY', '5.864'), ('DINLO', '5.841'), ('PIONY', '5.791'), ('LINGO', '5.749'), ('PILON', '5.742'), ('PINOL', '5.704'), ('LOGIN', '5.699'), ('DOILY', '5.681'), ('NICOL', '5.680')]
Top entropy choice: 𝘾𝙊𝙇𝙄𝙉 with entropy: 5.9379
Guess count low, choosing the word with highest entropy: 𝘾𝙊𝙇𝙄𝙉 with entropy: 5.9379

Guess: 𝘾𝙊𝙇𝙄𝙉, Feedback: ⬜🟩⬜⬜⬜
Actual Info Gain: 3.4426 bits
Posterior entropy: 6.554588851677638
entropy_info: {'prior_entropy': 9.997179480937621, 'posterior_entropy': 6.554588851677638, 'actual_info_gain': 3.4425906292599837, 'expected_info_gain': 5.937896650379372}
----------
The bot is making a guess...
94 candidate words remaining.
Top 10 guesses: [('PUDGY', '4.344'), ('BOODY', '4.273'), ('BODGY', '4.267'), ('DUMPY', '4.225'), ('BUMPY', '4.212'), ('GOMBO', '4.207'), ('BUMPH', '4.177'), ('BODOH', '4.121'), ('GOODY', '4.093'), ('PODGY', '4.085')]
Top entropy choice: 𝙋𝙐𝘿𝙂𝙔 with entropy: 4.3443
Using common exploratory word: 𝘽𝙐𝙈𝙋𝙔 with entropy: 4.2124

Guess: 𝘽𝙐𝙈𝙋𝙔, Feedback: ⬜⬜⬜🟩🟩
Actual Info Gain: 4.2327 bits
Posterior entropy: 2.321928094887362
entropy_info: {'prior_entropy': 6.554588851677638, 'posterior_entropy': 2.321928094887362, 'actual_info_gain': 4.232660756790276, 'expected_info_gain': 5.937896650379372}
----------
The bot is making a guess...
5 candidate words remaining.
Top 10 guesses: [('PAPAW', '2.322'), ('PLONG', '2.322'), ('PLOWS', '2.322'), ('PLOWT', '2.322'), ('POGOS', '2.322'), ('PONGO', '2.322'), ('PROGS', '2.322'), ('PRONG', '2.322'), ('PROWK', '2.322'), ('PROWL', '2.322')]
Top entropy choice: 𝙋𝘼𝙋𝘼𝙒 with entropy: 2.3219
Guess count high, choosing a candidate with the highest entropy: 𝙃𝙊𝙋𝙋𝙔 with entropy: 1.9219

Guess: 𝙃𝙊𝙋𝙋𝙔, Feedback: ⬜🟩🟩🟩🟩
Actual Info Gain: 2.3219 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 2.321928094887362, 'posterior_entropy': 0.0, 'actual_info_gain': 2.321928094887362, 'expected_info_gain': 1.9219280948873623}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('POPPY', '0.000')]
Top entropy choice: 𝙋𝙊𝙋𝙋𝙔 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝙋𝙊𝙋𝙋𝙔 with entropy: 0.0000
You won! Amount of guesses: 5

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜⬜⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 3.861 bits, Posterior Entropy: 9.997, 
Guess: 𝘾𝙊𝙇𝙄𝙉, Feedback: ⬜🟩⬜⬜⬜, Prior Entropy: 9.997, Expected Info Gain: 5.938 bits, Actual Info Gain: 3.443 bits, Posterior Entropy: 6.555, 
Guess: 𝘽𝙐𝙈𝙋𝙔, Feedback: ⬜⬜⬜🟩🟩, Prior Entropy: 6.555, Expected Info Gain: 5.938 bits, Actual Info Gain: 4.233 bits, Posterior Entropy: 2.322, 
Guess: 𝙃𝙊𝙋𝙋𝙔, Feedback: ⬜🟩🟩🟩🟩, Prior Entropy: 2.322, Expected Info Gain: 1.922 bits, Actual Info Gain: 2.322 bits, Posterior Entropy: 0.000, 
Guess: 𝙋𝙊𝙋𝙋𝙔, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'POPPY': 34.0728 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝘼𝙇𝙒𝘼𝙔 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟨⬜⬜⬜
Actual Info Gain: 4.3449 bits
Posterior entropy: 9.513727595952437
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 9.513727595952437, 'actual_info_gain': 4.34493338877034, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
731 candidate words remaining.
Top 10 guesses: [('COLIN', '5.613'), ('LIANA', '5.503'), ('ALOIN', '5.496'), ('PILON', '5.458'), ('PINOL', '5.439'), ('DINLO', '5.430'), ('LOGIN', '5.388'), ('NICOL', '5.363'), ('NMOLI', '5.336'), ('ULMIN', '5.322')]
Top entropy choice: 𝘾𝙊𝙇𝙄𝙉 with entropy: 5.6132
Guess count low, choosing the word with highest entropy: 𝘾𝙊𝙇𝙄𝙉 with entropy: 5.6132

Guess: 𝘾𝙊𝙇𝙄𝙉, Feedback: ⬜⬜🟨⬜⬜
Actual Info Gain: 3.5595 bits
Posterior entropy: 5.954196310386875
entropy_info: {'prior_entropy': 9.513727595952437, 'posterior_entropy': 5.954196310386875, 'actual_info_gain': 3.559531285565562, 'expected_info_gain': 5.613155296118865}
----------
The bot is making a guess...
62 candidate words remaining.
Top 10 guesses: [('AGAMY', '4.658'), ('PLUMY', '4.632'), ('ALUMY', '4.570'), ('PLAYA', '4.529'), ('APAYD', '4.484'), ('ULAMA', '4.480'), ('DUMPY', '4.478'), ('AYMAG', '4.472'), ('FLAMY', '4.459'), ('ALULA', '4.441')]
Top entropy choice: 𝘼𝙂𝘼𝙈𝙔 with entropy: 4.6584
Using common exploratory word: 𝙋𝙇𝘼𝙔𝘼 with entropy: 4.5287

Guess: 𝙋𝙇𝘼𝙔𝘼, Feedback: ⬜🟩🟨🟨🟨
Actual Info Gain: 5.9542 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 5.954196310386875, 'posterior_entropy': 0.0, 'actual_info_gain': 5.954196310386875, 'expected_info_gain': 5.613155296118865}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('ALWAY', '0.000')]
Top entropy choice: 𝘼𝙇𝙒𝘼𝙔 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝘼𝙇𝙒𝘼𝙔 with entropy: 0.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟨⬜⬜⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 4.345 bits, Posterior Entropy: 9.514, 
Guess: 𝘾𝙊𝙇𝙄𝙉, Feedback: ⬜⬜🟨⬜⬜, Prior Entropy: 9.514, Expected Info Gain: 5.613 bits, Actual Info Gain: 3.560 bits, Posterior Entropy: 5.954, 
Guess: 𝙋𝙇𝘼𝙔𝘼, Feedback: ⬜🟩🟨🟨🟨, Prior Entropy: 5.954, Expected Info Gain: 5.613 bits, Actual Info Gain: 5.954 bits, Posterior Entropy: 0.000, 
Guess: 𝘼𝙇𝙒𝘼𝙔, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'ALWAY': 25.6485 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙎𝙐𝙉𝙉𝙔 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜⬜🟨
Actual Info Gain: 5.4967 bits
Posterior entropy: 8.361943773735241
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 8.361943773735241, 'actual_info_gain': 5.496717210987535, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
329 candidate words remaining.
Top 10 guesses: [('SOILY', '5.323'), ('NOILY', '5.322'), ('PIONY', '5.294'), ('PINOL', '5.237'), ('PILON', '5.236'), ('SHIOK', '5.193'), ('CHOIL', '5.184'), ('SPOIL', '5.183'), ('SHILY', '5.157'), ('SOULY', '5.152')]
Top entropy choice: 𝙎𝙊𝙄𝙇𝙔 with entropy: 5.3226
Guess count low, choosing the word with highest entropy: 𝙎𝙊𝙄𝙇𝙔 with entropy: 5.3226

Guess: 𝙎𝙊𝙄𝙇𝙔, Feedback: 🟩⬜⬜⬜🟩
Actual Info Gain: 6.0400 bits
Posterior entropy: 2.321928094887362
entropy_info: {'prior_entropy': 8.361943773735241, 'posterior_entropy': 2.321928094887362, 'actual_info_gain': 6.04001567884788, 'expected_info_gain': 5.322573042242079}
----------
The bot is making a guess...
5 candidate words remaining.
Top 10 guesses: [('ADUNC', '2.322'), ('BLUNK', '2.322'), ('BOUND', '2.322'), ('BRUSK', '2.322'), ('BUNCE', '2.322'), ('BUNCH', '2.322'), ('BUNCO', '2.322'), ('BUNDE', '2.322'), ('BUNDH', '2.322'), ('BUNDS', '2.322')]
Top entropy choice: 𝘼𝘿𝙐𝙉𝘾 with entropy: 2.3219
Using common exploratory word: 𝘽𝙊𝙐𝙉𝘿 with entropy: 2.3219

Guess: 𝘽𝙊𝙐𝙉𝘿, Feedback: ⬜⬜🟨🟩⬜
Actual Info Gain: 2.3219 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 2.321928094887362, 'posterior_entropy': 0.0, 'actual_info_gain': 2.321928094887362, 'expected_info_gain': 5.322573042242079}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('SUNNY', '0.000')]
Top entropy choice: 𝙎𝙐𝙉𝙉𝙔 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝙎𝙐𝙉𝙉𝙔 with entropy: 0.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜⬜🟨, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 5.497 bits, Posterior Entropy: 8.362, 
Guess: 𝙎𝙊𝙄𝙇𝙔, Feedback: 🟩⬜⬜⬜🟩, Prior Entropy: 8.362, Expected Info Gain: 5.323 bits, Actual Info Gain: 6.040 bits, Posterior Entropy: 2.322, 
Guess: 𝘽𝙊𝙐𝙉𝘿, Feedback: ⬜⬜🟨🟩⬜, Prior Entropy: 2.322, Expected Info Gain: 5.323 bits, Actual Info Gain: 2.322 bits, Posterior Entropy: 0.000, 
Guess: 𝙎𝙐𝙉𝙉𝙔, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'SUNNY': 9.9694 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙎𝙐𝙍𝘼𝙏 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟨🟨🟩⬜🟨
Actual Info Gain: 10.3992 bits
Posterior entropy: 3.4594316186372973
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 3.4594316186372973, 'actual_info_gain': 10.399229366085478, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
11 candidate words remaining.
Top 10 guesses: [('POKIT', '2.550'), ('PIYUT', '2.404'), ('PICOT', '2.404'), ('STUPA', '2.404'), ('PHWAT', '2.369'), ('PUCKA', '2.369'), ('ADOPT', '2.222'), ('ATOPY', '2.222'), ('ATTAP', '2.222'), ('COPAY', '2.222')]
Top entropy choice: 𝙋𝙊𝙆𝙄𝙏 with entropy: 2.5503
Guess count low, choosing the word with highest entropy: 𝙋𝙊𝙆𝙄𝙏 with entropy: 2.5503

Guess: 𝙋𝙊𝙆𝙄𝙏, Feedback: ⬜⬜⬜⬜🟩
Actual Info Gain: 2.4594 bits
Posterior entropy: 1.0
entropy_info: {'prior_entropy': 3.4594316186372973, 'posterior_entropy': 1.0, 'actual_info_gain': 2.4594316186372973, 'expected_info_gain': 2.5503407095463886}
----------
The bot is making a guess...
2 candidate words remaining.
Top 10 guesses: [('SCRAT', '1.000'), ('SURAT', '1.000')]
Top entropy choice: 𝙎𝘾𝙍𝘼𝙏 with entropy: 1.0000
Few candidates left, going through them all to pick a common word...
Using common word: 𝙎𝙐𝙍𝘼𝙏 with entropy: 1.0000
You won! Amount of guesses: 3

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟨🟨🟩⬜🟨, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 10.399 bits, Posterior Entropy: 3.459, 
Guess: 𝙋𝙊𝙆𝙄𝙏, Feedback: ⬜⬜⬜⬜🟩, Prior Entropy: 3.459, Expected Info Gain: 2.550 bits, Actual Info Gain: 2.459 bits, Posterior Entropy: 1.000, 
Guess: 𝙎𝙐𝙍𝘼𝙏, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'SURAT': 0.3230 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙐𝙉𝙄𝙏𝙀 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟨⬜⬜🟨⬜
Actual Info Gain: 6.3509 bits
Posterior entropy: 7.507794640198696
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 7.507794640198696, 'actual_info_gain': 6.35086634452408, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
182 candidate words remaining.
Top 10 guesses: [('ELITE', '4.694'), ('LEONE', '4.690'), ('HELIO', '4.644'), ('LOCIE', '4.643'), ('LITHE', '4.629'), ('LENTI', '4.618'), ('BELIE', '4.614'), ('LINTY', '4.598'), ('FLITT', '4.596'), ('LOIPE', '4.588')]
Top entropy choice: 𝙀𝙇𝙄𝙏𝙀 with entropy: 4.6939
Guess count low, choosing the word with highest entropy: 𝙀𝙇𝙄𝙏𝙀 with entropy: 4.6939

Guess: 𝙀𝙇𝙄𝙏𝙀, Feedback: ⬜⬜🟩🟩🟩
Actual Info Gain: 5.5078 bits
Posterior entropy: 2.0
entropy_info: {'prior_entropy': 7.507794640198696, 'posterior_entropy': 2.0, 'actual_info_gain': 5.507794640198696, 'expected_info_gain': 4.69389435581856}
----------
The bot is making a guess...
4 candidate words remaining.
Top 10 guesses: [('ABUNA', '2.000'), ('ABUNE', '2.000'), ('ABURN', '2.000'), ('ADOWN', '2.000'), ('ANCHO', '2.000'), ('AUCHT', '2.000'), ('AUDIO', '2.000'), ('AUGHT', '2.000'), ('AULOI', '2.000'), ('AULOS', '2.000')]
Top entropy choice: 𝘼𝘽𝙐𝙉𝘼 with entropy: 2.0000
Using common exploratory word: 𝘼𝙐𝘿𝙄𝙊 with entropy: 2.0000

Guess: 𝘼𝙐𝘿𝙄𝙊, Feedback: ⬜🟨⬜🟨⬜
Actual Info Gain: 2.0000 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 2.0, 'posterior_entropy': 0.0, 'actual_info_gain': 2.0, 'expected_info_gain': 4.69389435581856}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('UNITE', '0.000')]
Top entropy choice: 𝙐𝙉𝙄𝙏𝙀 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝙐𝙉𝙄𝙏𝙀 with entropy: 0.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟨⬜⬜🟨⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 6.351 bits, Posterior Entropy: 7.508, 
Guess: 𝙀𝙇𝙄𝙏𝙀, Feedback: ⬜⬜🟩🟩🟩, Prior Entropy: 7.508, Expected Info Gain: 4.694 bits, Actual Info Gain: 5.508 bits, Posterior Entropy: 2.000, 
Guess: 𝘼𝙐𝘿𝙄𝙊, Feedback: ⬜🟨⬜🟨⬜, Prior Entropy: 2.000, Expected Info Gain: 4.694 bits, Actual Info Gain: 2.000 bits, Posterior Entropy: 0.000, 
Guess: 𝙐𝙉𝙄𝙏𝙀, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'UNITE': 5.7680 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙎𝙋𝙀𝙀𝘿 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜🟩🟨
Actual Info Gain: 7.0258 bits
Posterior entropy: 6.832890014164741
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 6.832890014164741, 'actual_info_gain': 7.025770970558035, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
114 candidate words remaining.
Top 10 guesses: [('SOLID', '4.301'), ('UNOLD', '4.270'), ('INDOL', '4.269'), ('SOLDI', '4.218'), ('DINLO', '4.213'), ('NOULD', '4.209'), ('LOUND', '4.209'), ('UNSOD', '4.201'), ('UNLID', '4.200'), ('SOUND', '4.188')]
Top entropy choice: 𝙎𝙊𝙇𝙄𝘿 with entropy: 4.3013
Guess count low, choosing the word with highest entropy: 𝙎𝙊𝙇𝙄𝘿 with entropy: 4.3013

Guess: 𝙎𝙊𝙇𝙄𝘿, Feedback: 🟩⬜⬜⬜🟩
Actual Info Gain: 3.2479 bits
Posterior entropy: 3.584962500721156
entropy_info: {'prior_entropy': 6.832890014164741, 'posterior_entropy': 3.584962500721156, 'actual_info_gain': 3.247927513443585, 'expected_info_gain': 4.301344608838756}
----------
The bot is making a guess...
12 candidate words remaining.
Top 10 guesses: [('PUNKY', '3.189'), ('PEEKY', '3.085'), ('WEEPY', '3.085'), ('SPEWY', '3.085'), ('UPEND', '3.022'), ('APEEK', '2.918'), ('EPENA', '2.918'), ('EPENE', '2.918'), ('NEPER', '2.918'), ('PRUNY', '2.918')]
Top entropy choice: 𝙋𝙐𝙉𝙆𝙔 with entropy: 3.1887

Guess: 𝙋𝙐𝙉𝙆𝙔, Feedback: 🟨⬜⬜⬜⬜
Actual Info Gain: 3.5850 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 3.584962500721156, 'posterior_entropy': 0.0, 'actual_info_gain': 3.584962500721156, 'expected_info_gain': 3.188721875540867}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('SPEED', '0.000')]
Top entropy choice: 𝙎𝙋𝙀𝙀𝘿 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝙎𝙋𝙀𝙀𝘿 with entropy: 0.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜🟩🟨, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 7.026 bits, Posterior Entropy: 6.833, 
Guess: 𝙎𝙊𝙇𝙄𝘿, Feedback: 🟩⬜⬜⬜🟩, Prior Entropy: 6.833, Expected Info Gain: 4.301 bits, Actual Info Gain: 3.248 bits, Posterior Entropy: 3.585, 
Guess: 𝙋𝙐𝙉𝙆𝙔, Feedback: 🟨⬜⬜⬜⬜, Prior Entropy: 3.585, Expected Info Gain: 3.189 bits, Actual Info Gain: 3.585 bits, Posterior Entropy: 0.000, 
Guess: 𝙎𝙋𝙀𝙀𝘿, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'SPEED': 4.2148 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙎𝙐𝙎𝙃𝙄 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜⬜🟨
Actual Info Gain: 5.4967 bits
Posterior entropy: 8.361943773735241
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 8.361943773735241, 'actual_info_gain': 5.496717210987535, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
329 candidate words remaining.
Top 10 guesses: [('SOILY', '5.323'), ('NOILY', '5.322'), ('PIONY', '5.294'), ('PINOL', '5.237'), ('PILON', '5.236'), ('SHIOK', '5.193'), ('CHOIL', '5.184'), ('SPOIL', '5.183'), ('SHILY', '5.157'), ('SOULY', '5.152')]
Top entropy choice: 𝙎𝙊𝙄𝙇𝙔 with entropy: 5.3226
Guess count low, choosing the word with highest entropy: 𝙎𝙊𝙄𝙇𝙔 with entropy: 5.3226

Guess: 𝙎𝙊𝙄𝙇𝙔, Feedback: 🟩⬜🟨⬜⬜
Actual Info Gain: 5.1920 bits
Posterior entropy: 3.169925001442312
entropy_info: {'prior_entropy': 8.361943773735241, 'posterior_entropy': 3.169925001442312, 'actual_info_gain': 5.192018772292929, 'expected_info_gain': 5.322573042242079}
----------
The bot is making a guess...
9 candidate words remaining.
Top 10 guesses: [('UNBID', '2.948'), ('BUCHU', '2.948'), ('BUCKO', '2.948'), ('BUCKS', '2.948'), ('BUCKU', '2.948'), ('BUNDH', '2.948'), ('CHEDI', '2.948'), ('CHUBS', '2.948'), ('CHYND', '2.948'), ('CUBED', '2.948')]
Top entropy choice: 𝙐𝙉𝘽𝙄𝘿 with entropy: 2.9477

Guess: 𝙐𝙉𝘽𝙄𝘿, Feedback: 🟨⬜⬜🟨⬜
Actual Info Gain: 2.1699 bits
Posterior entropy: 1.0
entropy_info: {'prior_entropy': 3.169925001442312, 'posterior_entropy': 1.0, 'actual_info_gain': 2.169925001442312, 'expected_info_gain': 2.9477027792200903}
----------
The bot is making a guess...
2 candidate words remaining.
Top 10 guesses: [('SUCCI', '1.000'), ('SUSHI', '1.000')]
Top entropy choice: 𝙎𝙐𝘾𝘾𝙄 with entropy: 1.0000
Guess count high, choosing a candidate with the highest entropy: 𝙎𝙐𝙎𝙃𝙄 with entropy: 1.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜⬜🟨, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 5.497 bits, Posterior Entropy: 8.362, 
Guess: 𝙎𝙊𝙄𝙇𝙔, Feedback: 🟩⬜🟨⬜⬜, Prior Entropy: 8.362, Expected Info Gain: 5.323 bits, Actual Info Gain: 5.192 bits, Posterior Entropy: 3.170, 
Guess: 𝙐𝙉𝘽𝙄𝘿, Feedback: 🟨⬜⬜🟨⬜, Prior Entropy: 3.170, Expected Info Gain: 2.948 bits, Actual Info Gain: 2.170 bits, Posterior Entropy: 1.000, 
Guess: 𝙎𝙐𝙎𝙃𝙄, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'SUSHI': 10.6101 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙏𝘼𝙐𝙉𝙏 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟩🟩⬜⬜⬜
Actual Info Gain: 7.8814 bits
Posterior entropy: 5.977279923499917
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 5.977279923499917, 'actual_info_gain': 7.88138106122286, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
63 candidate words remaining.
Top 10 guesses: [('KYLIN', '4.106'), ('LINKY', '3.945'), ('LANAI', '3.896'), ('LIANA', '3.856'), ('COLIN', '3.852'), ('YONIC', '3.810'), ('CONIA', '3.793'), ('LYING', '3.787'), ('INULA', '3.776'), ('CLINK', '3.763')]
Top entropy choice: 𝙆𝙔𝙇𝙄𝙉 with entropy: 4.1056
Guess count low, choosing the word with highest entropy: 𝙆𝙔𝙇𝙄𝙉 with entropy: 4.1056

Guess: 𝙆𝙔𝙇𝙄𝙉, Feedback: ⬜⬜⬜⬜🟨
Actual Info Gain: 3.6554 bits
Posterior entropy: 2.321928094887362
entropy_info: {'prior_entropy': 5.977279923499917, 'posterior_entropy': 2.321928094887362, 'actual_info_gain': 3.6553518286125546, 'expected_info_gain': 4.105638606067614}
----------
The bot is making a guess...
5 candidate words remaining.
Top 10 guesses: [('AGAST', '2.322'), ('AGATE', '2.322'), ('AGATY', '2.322'), ('AGITA', '2.322'), ('AGORA', '2.322'), ('AGUNA', '2.322'), ('AINGA', '2.322'), ('AJUGA', '2.322'), ('ALANG', '2.322'), ('ARGOT', '2.322')]
Top entropy choice: 𝘼𝙂𝘼𝙎𝙏 with entropy: 2.3219
Using common exploratory word: 𝘼𝙂𝘼𝙏𝙀 with entropy: 2.3219

Guess: 𝘼𝙂𝘼𝙏𝙀, Feedback: 🟨⬜⬜🟨⬜
Actual Info Gain: 2.3219 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 2.321928094887362, 'posterior_entropy': 0.0, 'actual_info_gain': 2.321928094887362, 'expected_info_gain': 4.105638606067614}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('TAUNT', '0.000')]
Top entropy choice: 𝙏𝘼𝙐𝙉𝙏 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝙏𝘼𝙐𝙉𝙏 with entropy: 0.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟩🟩⬜⬜⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 7.881 bits, Posterior Entropy: 5.977, 
Guess: 𝙆𝙔𝙇𝙄𝙉, Feedback: ⬜⬜⬜⬜🟨, Prior Entropy: 5.977, Expected Info Gain: 4.106 bits, Actual Info Gain: 3.655 bits, Posterior Entropy: 2.322, 
Guess: 𝘼𝙂𝘼𝙏𝙀, Feedback: 🟨⬜⬜🟨⬜, Prior Entropy: 2.322, Expected Info Gain: 4.106 bits, Actual Info Gain: 2.322 bits, Posterior Entropy: 0.000, 
Guess: 𝙏𝘼𝙐𝙉𝙏, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'TAUNT': 2.0637 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙇𝘼𝙍𝙑𝘼 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟩🟩⬜⬜
Actual Info Gain: 6.9760 bits
Posterior entropy: 6.882643049361842
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 6.882643049361842, 'actual_info_gain': 6.976017935360935, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
118 candidate words remaining.
Top 10 guesses: [('MINCY', '3.994'), ('MOLDY', '3.993'), ('COMBY', '3.944'), ('MYOID', '3.913'), ('CYMOL', '3.890'), ('COMBI', '3.882'), ('MOCKY', '3.855'), ('MICKY', '3.849'), ('DIMBO', '3.844'), ('MIDGY', '3.812')]
Top entropy choice: 𝙈𝙄𝙉𝘾𝙔 with entropy: 3.9942
Guess count low, choosing the word with highest entropy: 𝙈𝙄𝙉𝘾𝙔 with entropy: 3.9942

Guess: 𝙈𝙄𝙉𝘾𝙔, Feedback: ⬜⬜⬜⬜⬜
Actual Info Gain: 2.2977 bits
Posterior entropy: 4.584962500721156
entropy_info: {'prior_entropy': 6.882643049361842, 'posterior_entropy': 4.584962500721156, 'actual_info_gain': 2.2976805486406855, 'expected_info_gain': 3.994197821504514}
----------
The bot is making a guess...
24 candidate words remaining.
Top 10 guesses: [('GOOLD', '3.824'), ('DOGAL', '3.720'), ('GLODE', '3.720'), ('GOLDS', '3.720'), ('GOLDY', '3.720'), ('GOURD', '3.689'), ('GOVED', '3.668'), ('PLAGA', '3.653'), ('LODGE', '3.637'), ('DOBLA', '3.637')]
Top entropy choice: 𝙂𝙊𝙊𝙇𝘿 with entropy: 3.8239
Using common exploratory word: 𝙂𝙊𝙐𝙍𝘿 with entropy: 3.6887

Guess: 𝙂𝙊𝙐𝙍𝘿, Feedback: ⬜⬜⬜🟨⬜
Actual Info Gain: 3.5850 bits
Posterior entropy: 1.0
entropy_info: {'prior_entropy': 4.584962500721156, 'posterior_entropy': 1.0, 'actual_info_gain': 3.584962500721156, 'expected_info_gain': 3.994197821504514}
----------
The bot is making a guess...
2 candidate words remaining.
Top 10 guesses: [('LARVA', '1.000'), ('PARKA', '1.000')]
Top entropy choice: 𝙇𝘼𝙍𝙑𝘼 with entropy: 1.0000
Guess count high, choosing a candidate with the highest entropy: 𝙇𝘼𝙍𝙑𝘼 with entropy: 1.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟩🟩⬜⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 6.976 bits, Posterior Entropy: 6.883, 
Guess: 𝙈𝙄𝙉𝘾𝙔, Feedback: ⬜⬜⬜⬜⬜, Prior Entropy: 6.883, Expected Info Gain: 3.994 bits, Actual Info Gain: 2.298 bits, Posterior Entropy: 4.585, 
Guess: 𝙂𝙊𝙐𝙍𝘿, Feedback: ⬜⬜⬜🟨⬜, Prior Entropy: 4.585, Expected Info Gain: 3.994 bits, Actual Info Gain: 3.585 bits, Posterior Entropy: 1.000, 
Guess: 𝙇𝘼𝙍𝙑𝘼, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'LARVA': 4.3891 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝘾𝙍𝙀𝘼𝙈 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟨🟨🟨⬜
Actual Info Gain: 6.4578 bits
Posterior entropy: 7.400879436282184
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 7.400879436282184, 'actual_info_gain': 6.457781548440592, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
169 candidate words remaining.
Top 10 guesses: [('BEARD', '4.178'), ('DRERE', '4.157'), ('DEARE', '4.132'), ('REERD', '4.123'), ('DEARN', '4.081'), ('REARD', '4.063'), ('BRERE', '4.036'), ('DRAMA', '4.027'), ('BEARE', '4.027'), ('BREDE', '4.025')]
Top entropy choice: 𝘽𝙀𝘼𝙍𝘿 with entropy: 4.1781
Guess count low, choosing the word with highest entropy: 𝘽𝙀𝘼𝙍𝘿 with entropy: 4.1781

Guess: 𝘽𝙀𝘼𝙍𝘿, Feedback: ⬜🟨🟨🟨⬜
Actual Info Gain: 2.4940 bits
Posterior entropy: 4.906890595608519
entropy_info: {'prior_entropy': 7.400879436282184, 'posterior_entropy': 4.906890595608519, 'actual_info_gain': 2.4939888406736657, 'expected_info_gain': 4.178126483345813}
----------
The bot is making a guess...
30 candidate words remaining.
Top 10 guesses: [('ARECA', '3.765'), ('AFEAR', '3.711'), ('ANEAR', '3.673'), ('CANAL', '3.643'), ('ANCLE', '3.628'), ('AREAL', '3.578'), ('AGUNA', '3.574'), ('CRAAL', '3.565'), ('ARENA', '3.556'), ('ALGAE', '3.553')]
Top entropy choice: 𝘼𝙍𝙀𝘾𝘼 with entropy: 3.7647
Using common exploratory word: 𝘾𝘼𝙉𝘼𝙇 with entropy: 3.6430

Guess: 𝘾𝘼𝙉𝘼𝙇, Feedback: 🟩⬜⬜🟩⬜
Actual Info Gain: 3.9069 bits
Posterior entropy: 1.0
entropy_info: {'prior_entropy': 4.906890595608519, 'posterior_entropy': 1.0, 'actual_info_gain': 3.9068905956085187, 'expected_info_gain': 4.178126483345813}
----------
The bot is making a guess...
2 candidate words remaining.
Top 10 guesses: [('CREAK', '1.000'), ('CREAM', '1.000')]
Top entropy choice: 𝘾𝙍𝙀𝘼𝙆 with entropy: 1.0000
Guess count high, choosing a candidate with the highest entropy: 𝘾𝙍𝙀𝘼𝙈 with entropy: 1.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟨🟨🟨⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 6.458 bits, Posterior Entropy: 7.401, 
Guess: 𝘽𝙀𝘼𝙍𝘿, Feedback: ⬜🟨🟨🟨⬜, Prior Entropy: 7.401, Expected Info Gain: 4.178 bits, Actual Info Gain: 2.494 bits, Posterior Entropy: 4.907, 
Guess: 𝘾𝘼𝙉𝘼𝙇, Feedback: 🟩⬜⬜🟩⬜, Prior Entropy: 4.907, Expected Info Gain: 4.178 bits, Actual Info Gain: 3.907 bits, Posterior Entropy: 1.000, 
Guess: 𝘾𝙍𝙀𝘼𝙈, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'CREAM': 5.8874 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙇𝙄𝙑𝙀𝙍 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜🟨🟩⬜
Actual Info Gain: 5.5966 bits
Posterior entropy: 8.262094845370179
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 8.262094845370179, 'actual_info_gain': 5.596566139352598, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
307 candidate words remaining.
Top 10 guesses: [('POIND', '4.424'), ('DINLO', '4.295'), ('NIDOR', '4.259'), ('BIPOD', '4.172'), ('PINOL', '4.170'), ('PILON', '4.152'), ('PRIOR', '4.152'), ('VIOLD', '4.149'), ('INDOL', '4.122'), ('BIDON', '4.115')]
Top entropy choice: 𝙋𝙊𝙄𝙉𝘿 with entropy: 4.4242
Guess count low, choosing the word with highest entropy: 𝙋𝙊𝙄𝙉𝘿 with entropy: 4.4242

Guess: 𝙋𝙊𝙄𝙉𝘿, Feedback: ⬜⬜🟨⬜⬜
Actual Info Gain: 3.2621 bits
Posterior entropy: 5.0
entropy_info: {'prior_entropy': 8.262094845370179, 'posterior_entropy': 5.0, 'actual_info_gain': 3.2620948453701786, 'expected_info_gain': 4.424196059405291}
----------
The bot is making a guess...
32 candidate words remaining.
Top 10 guesses: [('FILMY', '3.049'), ('LIMBY', '2.945'), ('FILCH', '2.924'), ('RIVAL', '2.889'), ('RIVEL', '2.889'), ('FLYBY', '2.874'), ('BILGY', '2.873'), ('RIFLE', '2.865'), ('VILER', '2.858'), ('FOLKY', '2.858')]
Top entropy choice: 𝙁𝙄𝙇𝙈𝙔 with entropy: 3.0494
Using common exploratory word: 𝙍𝙄𝙑𝘼𝙇 with entropy: 2.8891

Guess: 𝙍𝙄𝙑𝘼𝙇, Feedback: 🟨🟩🟩⬜🟨
Actual Info Gain: 5.0000 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 5.0, 'posterior_entropy': 0.0, 'actual_info_gain': 5.0, 'expected_info_gain': 4.424196059405291}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('LIVER', '0.000')]
Top entropy choice: 𝙇𝙄𝙑𝙀𝙍 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝙇𝙄𝙑𝙀𝙍 with entropy: 0.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜🟨🟩⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 5.597 bits, Posterior Entropy: 8.262, 
Guess: 𝙋𝙊𝙄𝙉𝘿, Feedback: ⬜⬜🟨⬜⬜, Prior Entropy: 8.262, Expected Info Gain: 4.424 bits, Actual Info Gain: 3.262 bits, Posterior Entropy: 5.000, 
Guess: 𝙍𝙄𝙑𝘼𝙇, Feedback: 🟨🟩🟩⬜🟨, Prior Entropy: 5.000, Expected Info Gain: 4.424 bits, Actual Info Gain: 5.000 bits, Posterior Entropy: 0.000, 
Guess: 𝙇𝙄𝙑𝙀𝙍, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'LIVER': 9.3756 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙁𝙀𝘿𝙀𝙓 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜🟩⬜
Actual Info Gain: 4.9914 bits
Posterior entropy: 8.867278739709661
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 8.867278739709661, 'actual_info_gain': 4.991382245013115, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
467 candidate words remaining.
Top 10 guesses: [('INDOL', '4.933'), ('DINLO', '4.859'), ('LOWND', '4.836'), ('BLOND', '4.832'), ('LOUND', '4.773'), ('LINDY', '4.749'), ('DOLCI', '4.730'), ('DOILY', '4.707'), ('NOULD', '4.703'), ('MODIN', '4.688')]
Top entropy choice: 𝙄𝙉𝘿𝙊𝙇 with entropy: 4.9332
Guess count low, choosing the word with highest entropy: 𝙄𝙉𝘿𝙊𝙇 with entropy: 4.9332

Guess: 𝙄𝙉𝘿𝙊𝙇, Feedback: ⬜⬜🟩⬜⬜
Actual Info Gain: 6.2823 bits
Posterior entropy: 2.584962500721156
entropy_info: {'prior_entropy': 8.867278739709661, 'posterior_entropy': 2.584962500721156, 'actual_info_gain': 6.282316238988505, 'expected_info_gain': 4.933171607708794}
----------
The bot is making a guess...
6 candidate words remaining.
Top 10 guesses: [('ABJUD', '2.585'), ('BEAUX', '2.585'), ('BEEFY', '2.585'), ('BEMUD', '2.585'), ('BLAUD', '2.585'), ('BLUED', '2.585'), ('BLUID', '2.585'), ('BOUND', '2.585'), ('BOURD', '2.585'), ('BUDDY', '2.585')]
Top entropy choice: 𝘼𝘽𝙅𝙐𝘿 with entropy: 2.5850
Using common exploratory word: 𝘽𝙀𝙀𝙁𝙔 with entropy: 2.5850

Guess: 𝘽𝙀𝙀𝙁𝙔, Feedback: ⬜🟩🟨🟨⬜
Actual Info Gain: 2.5850 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 2.584962500721156, 'posterior_entropy': 0.0, 'actual_info_gain': 2.584962500721156, 'expected_info_gain': 4.933171607708794}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('FEDEX', '0.000')]
Top entropy choice: 𝙁𝙀𝘿𝙀𝙓 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝙁𝙀𝘿𝙀𝙓 with entropy: 0.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜🟩⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 4.991 bits, Posterior Entropy: 8.867, 
Guess: 𝙄𝙉𝘿𝙊𝙇, Feedback: ⬜⬜🟩⬜⬜, Prior Entropy: 8.867, Expected Info Gain: 4.933 bits, Actual Info Gain: 6.282 bits, Posterior Entropy: 2.585, 
Guess: 𝘽𝙀𝙀𝙁𝙔, Feedback: ⬜🟩🟨🟨⬜, Prior Entropy: 2.585, Expected Info Gain: 4.933 bits, Actual Info Gain: 2.585 bits, Posterior Entropy: 0.000, 
Guess: 𝙁𝙀𝘿𝙀𝙓, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'FEDEX': 14.9332 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙌𝙐𝙀𝙇𝙇 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜🟨⬜
Actual Info Gain: 4.2869 bits
Posterior entropy: 9.571752643503546
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 9.571752643503546, 'actual_info_gain': 4.286908341219231, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
761 candidate words remaining.
Top 10 guesses: [('DINLO', '5.536'), ('COLIN', '5.491'), ('DOLIE', '5.487'), ('NOILY', '5.417'), ('MOILE', '5.403'), ('OLDIE', '5.394'), ('DOILY', '5.388'), ('MOLIE', '5.386'), ('LEONE', '5.379'), ('LOGIN', '5.371')]
Top entropy choice: 𝘿𝙄𝙉𝙇𝙊 with entropy: 5.5362
Guess count low, choosing the word with highest entropy: 𝘿𝙄𝙉𝙇𝙊 with entropy: 5.5362

Guess: 𝘿𝙄𝙉𝙇𝙊, Feedback: ⬜⬜⬜🟩⬜
Actual Info Gain: 4.7138 bits
Posterior entropy: 4.857980995127572
entropy_info: {'prior_entropy': 9.571752643503546, 'posterior_entropy': 4.857980995127572, 'actual_info_gain': 4.713771648375974, 'expected_info_gain': 5.536189971336713}
----------
The bot is making a guess...
29 candidate words remaining.
Top 10 guesses: [('BEECH', '3.762'), ('BECKE', '3.650'), ('MEECH', '3.646'), ('KEECH', '3.621'), ('FEHME', '3.581'), ('PECKE', '3.573'), ('HULKY', '3.486'), ('PHEME', '3.479'), ('PULLY', '3.427'), ('BULKY', '3.401')]
Top entropy choice: 𝘽𝙀𝙀𝘾𝙃 with entropy: 3.7623
Using common exploratory word: 𝘽𝙀𝙀𝘾𝙃 with entropy: 3.7623

Guess: 𝘽𝙀𝙀𝘾𝙃, Feedback: ⬜⬜🟩⬜⬜
Actual Info Gain: 3.8580 bits
Posterior entropy: 1.0
entropy_info: {'prior_entropy': 4.857980995127572, 'posterior_entropy': 1.0, 'actual_info_gain': 3.857980995127572, 'expected_info_gain': 5.536189971336713}
----------
The bot is making a guess...
2 candidate words remaining.
Top 10 guesses: [('KVELL', '1.000'), ('QUELL', '1.000')]
Top entropy choice: 𝙆𝙑𝙀𝙇𝙇 with entropy: 1.0000
Guess count high, choosing a candidate with the highest entropy: 𝙌𝙐𝙀𝙇𝙇 with entropy: 1.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜🟨⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 4.287 bits, Posterior Entropy: 9.572, 
Guess: 𝘿𝙄𝙉𝙇𝙊, Feedback: ⬜⬜⬜🟩⬜, Prior Entropy: 9.572, Expected Info Gain: 5.536 bits, Actual Info Gain: 4.714 bits, Posterior Entropy: 4.858, 
Guess: 𝘽𝙀𝙀𝘾𝙃, Feedback: ⬜⬜🟩⬜⬜, Prior Entropy: 4.858, Expected Info Gain: 5.536 bits, Actual Info Gain: 3.858 bits, Posterior Entropy: 1.000, 
Guess: 𝙌𝙐𝙀𝙇𝙇, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'QUELL': 24.1597 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙏𝙀𝙍𝙎𝙀 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟩⬜🟩🟨🟨
Actual Info Gain: 12.8587 bits
Posterior entropy: 1.0
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 1.0, 'actual_info_gain': 12.858660984722777, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
2 candidate words remaining.
Top 10 guesses: [('TERSE', '1.000'), ('TORSE', '1.000')]
Top entropy choice: 𝙏𝙀𝙍𝙎𝙀 with entropy: 1.0000
Guess count low, choosing the word with highest entropy: 𝙏𝙀𝙍𝙎𝙀 with entropy: 1.0000
You won! Amount of guesses: 2

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟩⬜🟩🟨🟨, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 12.859 bits, Posterior Entropy: 1.000, 
Guess: 𝙏𝙀𝙍𝙎𝙀, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'TERSE': 0.0043 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙀𝙉𝙅𝙊𝙔 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜🟨⬜
Actual Info Gain: 4.2869 bits
Posterior entropy: 9.571752643503546
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 9.571752643503546, 'actual_info_gain': 4.286908341219231, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
761 candidate words remaining.
Top 10 guesses: [('DINLO', '5.536'), ('COLIN', '5.491'), ('DOLIE', '5.487'), ('NOILY', '5.417'), ('MOILE', '5.403'), ('OLDIE', '5.394'), ('DOILY', '5.388'), ('MOLIE', '5.386'), ('LEONE', '5.379'), ('LOGIN', '5.371')]
Top entropy choice: 𝘿𝙄𝙉𝙇𝙊 with entropy: 5.5362
Guess count low, choosing the word with highest entropy: 𝘿𝙄𝙉𝙇𝙊 with entropy: 5.5362

Guess: 𝘿𝙄𝙉𝙇𝙊, Feedback: ⬜⬜🟨⬜🟨
Actual Info Gain: 5.4018 bits
Posterior entropy: 4.169925001442312
entropy_info: {'prior_entropy': 9.571752643503546, 'posterior_entropy': 4.169925001442312, 'actual_info_gain': 5.4018276420612334, 'expected_info_gain': 5.536189971336713}
----------
The bot is making a guess...
18 candidate words remaining.
Top 10 guesses: [('MOONY', '3.725'), ('BOOMY', '3.614'), ('GOONY', '3.614'), ('HOOKY', '3.614'), ('BOMOH', '3.572'), ('POOHY', '3.572'), ('PHONY', '3.530'), ('ZOOMY', '3.503'), ('GOOBY', '3.503'), ('WOOPY', '3.503')]
Top entropy choice: 𝙈𝙊𝙊𝙉𝙔 with entropy: 3.7255
Using common exploratory word: 𝙋𝙃𝙊𝙉𝙔 with entropy: 3.5305

Guess: 𝙋𝙃𝙊𝙉𝙔, Feedback: ⬜⬜🟨🟨🟩
Actual Info Gain: 3.1699 bits
Posterior entropy: 1.0
entropy_info: {'prior_entropy': 4.169925001442312, 'posterior_entropy': 1.0, 'actual_info_gain': 3.169925001442312, 'expected_info_gain': 5.536189971336713}
----------
The bot is making a guess...
2 candidate words remaining.
Top 10 guesses: [('ENJOY', '1.000'), ('ENVOY', '1.000')]
Top entropy choice: 𝙀𝙉𝙅𝙊𝙔 with entropy: 1.0000
Guess count high, choosing a candidate with the highest entropy: 𝙀𝙉𝙅𝙊𝙔 with entropy: 1.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜🟨⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 4.287 bits, Posterior Entropy: 9.572, 
Guess: 𝘿𝙄𝙉𝙇𝙊, Feedback: ⬜⬜🟨⬜🟨, Prior Entropy: 9.572, Expected Info Gain: 5.536 bits, Actual Info Gain: 5.402 bits, Posterior Entropy: 4.170, 
Guess: 𝙋𝙃𝙊𝙉𝙔, Feedback: ⬜⬜🟨🟨🟩, Prior Entropy: 4.170, Expected Info Gain: 5.536 bits, Actual Info Gain: 3.170 bits, Posterior Entropy: 1.000, 
Guess: 𝙀𝙉𝙅𝙊𝙔, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'ENJOY': 24.1125 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙋𝙀𝙏𝘼𝙇 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟨🟨⬜🟨⬜
Actual Info Gain: 7.5188 bits
Posterior entropy: 6.339850002884624
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 6.339850002884624, 'actual_info_gain': 7.518810981838152, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
81 candidate words remaining.
Top 10 guesses: [('LEANT', '4.217'), ('PLANE', '4.209'), ('LEAPT', '4.159'), ('PLANT', '4.109'), ('ALANT', '4.106'), ('PLANH', '4.101'), ('ALATE', '4.080'), ('PLATE', '4.060'), ('PLANC', '4.035'), ('PENAL', '4.002')]
Top entropy choice: 𝙇𝙀𝘼𝙉𝙏 with entropy: 4.2173
Guess count low, choosing the word with highest entropy: 𝙇𝙀𝘼𝙉𝙏 with entropy: 4.2173

Guess: 𝙇𝙀𝘼𝙉𝙏, Feedback: 🟨🟩🟨⬜🟨
Actual Info Gain: 4.0179 bits
Posterior entropy: 2.321928094887362
entropy_info: {'prior_entropy': 6.339850002884624, 'posterior_entropy': 2.321928094887362, 'actual_info_gain': 4.017921907997263, 'expected_info_gain': 4.217349936796832}
----------
The bot is making a guess...
5 candidate words remaining.
Top 10 guesses: [('AFLAP', '2.322'), ('AMPUL', '2.322'), ('APPAM', '2.322'), ('EMPTS', '2.322'), ('EMPTY', '2.322'), ('GOMPA', '2.322'), ('IMPEL', '2.322'), ('LIMPA', '2.322'), ('MAPAU', '2.322'), ('MILPA', '2.322')]
Top entropy choice: 𝘼𝙁𝙇𝘼𝙋 with entropy: 2.3219
Using common exploratory word: 𝙀𝙈𝙋𝙏𝙔 with entropy: 2.3219

Guess: 𝙀𝙈𝙋𝙏𝙔, Feedback: 🟨⬜🟨🟨⬜
Actual Info Gain: 2.3219 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 2.321928094887362, 'posterior_entropy': 0.0, 'actual_info_gain': 2.321928094887362, 'expected_info_gain': 4.217349936796832}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('PETAL', '0.000')]
Top entropy choice: 𝙋𝙀𝙏𝘼𝙇 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝙋𝙀𝙏𝘼𝙇 with entropy: 0.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟨🟨⬜🟨⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 7.519 bits, Posterior Entropy: 6.340, 
Guess: 𝙇𝙀𝘼𝙉𝙏, Feedback: 🟨🟩🟨⬜🟨, Prior Entropy: 6.340, Expected Info Gain: 4.217 bits, Actual Info Gain: 4.018 bits, Posterior Entropy: 2.322, 
Guess: 𝙀𝙈𝙋𝙏𝙔, Feedback: 🟨⬜🟨🟨⬜, Prior Entropy: 2.322, Expected Info Gain: 4.217 bits, Actual Info Gain: 2.322 bits, Posterior Entropy: 0.000, 
Guess: 𝙋𝙀𝙏𝘼𝙇, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'PETAL': 2.4362 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙇𝘼𝘽𝙊𝙍 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟩🟨⬜⬜
Actual Info Gain: 6.9398 bits
Posterior entropy: 6.918863237274595
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 6.918863237274595, 'actual_info_gain': 6.939797747448182, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
121 candidate words remaining.
Top 10 guesses: [('RINDY', '4.228'), ('RYPIN', '4.212'), ('KYLIN', '4.200'), ('ROBIN', '4.135'), ('MODIN', '4.120'), ('LINDY', '4.119'), ('RUBIN', '4.087'), ('RANID', '4.047'), ('UNLID', '4.029'), ('FLYIN', '4.013')]
Top entropy choice: 𝙍𝙄𝙉𝘿𝙔 with entropy: 4.2285
Guess count low, choosing the word with highest entropy: 𝙍𝙄𝙉𝘿𝙔 with entropy: 4.2285

Guess: 𝙍𝙄𝙉𝘿𝙔, Feedback: 🟨⬜⬜⬜⬜
Actual Info Gain: 2.1640 bits
Posterior entropy: 4.754887502163468
entropy_info: {'prior_entropy': 6.918863237274595, 'posterior_entropy': 4.754887502163468, 'actual_info_gain': 2.1639757351111264, 'expected_info_gain': 4.2284561813662584}
----------
The bot is making a guess...
27 candidate words remaining.
Top 10 guesses: [('LABRA', '3.736'), ('BOLAR', '3.726'), ('LOBAR', '3.726'), ('KALAM', '3.690'), ('LUBRA', '3.680'), ('KAVAL', '3.648'), ('KALPA', '3.634'), ('GULAB', '3.630'), ('MALVA', '3.616'), ('LABOR', '3.606')]
Top entropy choice: 𝙇𝘼𝘽𝙍𝘼 with entropy: 3.7360
Using common exploratory word: 𝙇𝘼𝘽𝙊𝙍 with entropy: 3.6060
You won! Amount of guesses: 3

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟩🟨⬜⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 6.940 bits, Posterior Entropy: 6.919, 
Guess: 𝙍𝙄𝙉𝘿𝙔, Feedback: 🟨⬜⬜⬜⬜, Prior Entropy: 6.919, Expected Info Gain: 4.228 bits, Actual Info Gain: 2.164 bits, Posterior Entropy: 4.755, 
Guess: 𝙇𝘼𝘽𝙊𝙍, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'LABOR': 4.3791 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙇𝘼𝘾𝙀𝘿 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟩⬜🟩⬜
Actual Info Gain: 6.6298 bits
Posterior entropy: 7.22881869049588
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 7.22881869049588, 'actual_info_gain': 6.629842294226896, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
150 candidate words remaining.
Top 10 guesses: [('LOWND', '3.832'), ('LYNCH', '3.736'), ('CHYND', '3.702'), ('LYMPH', '3.623'), ('MOLDY', '3.566'), ('DOWLY', '3.547'), ('WYLED', '3.538'), ('POWND', '3.534'), ('DYNEL', '3.508'), ('LINDY', '3.495')]
Top entropy choice: 𝙇𝙊𝙒𝙉𝘿 with entropy: 3.8325
Guess count low, choosing the word with highest entropy: 𝙇𝙊𝙒𝙉𝘿 with entropy: 3.8325

Guess: 𝙇𝙊𝙒𝙉𝘿, Feedback: 🟩⬜⬜⬜🟩
Actual Info Gain: 4.2288 bits
Posterior entropy: 3.0
entropy_info: {'prior_entropy': 7.22881869049588, 'posterior_entropy': 3.0, 'actual_info_gain': 4.22881869049588, 'expected_info_gain': 3.832485151723582}
----------
The bot is making a guess...
8 candidate words remaining.
Top 10 guesses: [('MICKY', '2.000'), ('MOCKY', '2.000'), ('MUCKY', '2.000'), ('ZYMIC', '2.000'), ('AMUCK', '1.549'), ('AVYZE', '1.549'), ('AZYME', '1.549'), ('AZYMS', '1.549'), ('BACKY', '1.549'), ('BICKY', '1.549')]
Top entropy choice: 𝙈𝙄𝘾𝙆𝙔 with entropy: 2.0000
Using common exploratory word: 𝙈𝙄𝘾𝙆𝙔 with entropy: 2.0000

Guess: 𝙈𝙄𝘾𝙆𝙔, Feedback: ⬜⬜🟩⬜⬜
Actual Info Gain: 3.0000 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 3.0, 'posterior_entropy': 0.0, 'actual_info_gain': 3.0, 'expected_info_gain': 3.832485151723582}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('LACED', '0.000')]
Top entropy choice: 𝙇𝘼𝘾𝙀𝘿 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝙇𝘼𝘾𝙀𝘿 with entropy: 0.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟩⬜🟩⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 6.630 bits, Posterior Entropy: 7.229, 
Guess: 𝙇𝙊𝙒𝙉𝘿, Feedback: 🟩⬜⬜⬜🟩, Prior Entropy: 7.229, Expected Info Gain: 3.832 bits, Actual Info Gain: 4.229 bits, Posterior Entropy: 3.000, 
Guess: 𝙈𝙄𝘾𝙆𝙔, Feedback: ⬜⬜🟩⬜⬜, Prior Entropy: 3.000, Expected Info Gain: 3.832 bits, Actual Info Gain: 3.000 bits, Posterior Entropy: 0.000, 
Guess: 𝙇𝘼𝘾𝙀𝘿, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'LACED': 4.8653 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙌𝙐𝘼𝙄𝙇 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟨⬜⬜⬜
Actual Info Gain: 4.3449 bits
Posterior entropy: 9.513727595952437
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 9.513727595952437, 'actual_info_gain': 4.34493338877034, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
731 candidate words remaining.
Top 10 guesses: [('COLIN', '5.613'), ('LIANA', '5.503'), ('ALOIN', '5.496'), ('PILON', '5.458'), ('PINOL', '5.439'), ('DINLO', '5.430'), ('LOGIN', '5.388'), ('NICOL', '5.363'), ('NMOLI', '5.336'), ('ULMIN', '5.322')]
Top entropy choice: 𝘾𝙊𝙇𝙄𝙉 with entropy: 5.6132
Guess count low, choosing the word with highest entropy: 𝘾𝙊𝙇𝙄𝙉 with entropy: 5.6132

Guess: 𝘾𝙊𝙇𝙄𝙉, Feedback: ⬜⬜🟨🟩⬜
Actual Info Gain: 6.3438 bits
Posterior entropy: 3.169925001442312
entropy_info: {'prior_entropy': 9.513727595952437, 'posterior_entropy': 3.169925001442312, 'actual_info_gain': 6.343802594510125, 'expected_info_gain': 5.613155296118865}
----------
The bot is making a guess...
9 candidate words remaining.
Top 10 guesses: [('PLAGA', '3.170'), ('AGUED', '2.948'), ('ALANG', '2.948'), ('ALGAL', '2.948'), ('ALMUG', '2.948'), ('GLADE', '2.948'), ('GLADS', '2.948'), ('GLADY', '2.948'), ('GLAMP', '2.948'), ('GLAND', '2.948')]
Top entropy choice: 𝙋𝙇𝘼𝙂𝘼 with entropy: 3.1699
Using common exploratory word: 𝘼𝙇𝙂𝘼𝙇 with entropy: 2.9477

Guess: 𝘼𝙇𝙂𝘼𝙇, Feedback: 🟨⬜⬜⬜🟩
Actual Info Gain: 3.1699 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 3.169925001442312, 'posterior_entropy': 0.0, 'actual_info_gain': 3.169925001442312, 'expected_info_gain': 5.613155296118865}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('QUAIL', '0.000')]
Top entropy choice: 𝙌𝙐𝘼𝙄𝙇 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝙌𝙐𝘼𝙄𝙇 with entropy: 0.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟨⬜⬜⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 4.345 bits, Posterior Entropy: 9.514, 
Guess: 𝘾𝙊𝙇𝙄𝙉, Feedback: ⬜⬜🟨🟩⬜, Prior Entropy: 9.514, Expected Info Gain: 5.613 bits, Actual Info Gain: 6.344 bits, Posterior Entropy: 3.170, 
Guess: 𝘼𝙇𝙂𝘼𝙇, Feedback: 🟨⬜⬜⬜🟩, Prior Entropy: 3.170, Expected Info Gain: 5.613 bits, Actual Info Gain: 3.170 bits, Posterior Entropy: 0.000, 
Guess: 𝙌𝙐𝘼𝙄𝙇, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'QUAIL': 23.9551 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙋𝙄𝙉𝙆𝙔 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜⬜⬜
Actual Info Gain: 3.8615 bits
Posterior entropy: 9.997179480937621
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 9.997179480937621, 'actual_info_gain': 3.8614815037851553, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
1022 candidate words remaining.
Top 10 guesses: [('COLIN', '5.938'), ('NOILY', '5.864'), ('DINLO', '5.841'), ('PIONY', '5.791'), ('LINGO', '5.749'), ('PILON', '5.742'), ('PINOL', '5.704'), ('LOGIN', '5.699'), ('DOILY', '5.681'), ('NICOL', '5.680')]
Top entropy choice: 𝘾𝙊𝙇𝙄𝙉 with entropy: 5.9379
Guess count low, choosing the word with highest entropy: 𝘾𝙊𝙇𝙄𝙉 with entropy: 5.9379

Guess: 𝘾𝙊𝙇𝙄𝙉, Feedback: ⬜⬜⬜🟨🟨
Actual Info Gain: 4.3825 bits
Posterior entropy: 5.614709844115208
entropy_info: {'prior_entropy': 9.997179480937621, 'posterior_entropy': 5.614709844115208, 'actual_info_gain': 4.382469636822413, 'expected_info_gain': 5.937896650379372}
----------
The bot is making a guess...
49 candidate words remaining.
Top 10 guesses: [('DUMKY', '3.944'), ('MIDGY', '3.868'), ('GUNNY', '3.865'), ('KINGY', '3.862'), ('GUNKY', '3.852'), ('DUNGY', '3.843'), ('GUNDY', '3.843'), ('DINGY', '3.835'), ('GIMPY', '3.826'), ('NYUNG', '3.822')]
Top entropy choice: 𝘿𝙐𝙈𝙆𝙔 with entropy: 3.9437
Using common exploratory word: 𝘿𝙄𝙉𝙂𝙔 with entropy: 3.8346

Guess: 𝘿𝙄𝙉𝙂𝙔, Feedback: ⬜🟩🟩⬜🟩
Actual Info Gain: 1.9143 bits
Posterior entropy: 3.700439718141092
entropy_info: {'prior_entropy': 5.614709844115208, 'posterior_entropy': 3.700439718141092, 'actual_info_gain': 1.9142701259741162, 'expected_info_gain': 5.937896650379372}
----------
The bot is making a guess...
13 candidate words remaining.
Top 10 guesses: [('PHYNX', '2.470'), ('KAPHA', '2.442'), ('KAPHS', '2.442'), ('KHAPH', '2.442'), ('KIPAH', '2.442'), ('KOPHS', '2.442'), ('KEMPS', '2.316'), ('KEMPT', '2.316'), ('KEMPY', '2.316'), ('KHOUM', '2.316')]
Top entropy choice: 𝙋𝙃𝙔𝙉𝙓 with entropy: 2.4697
Guess count high, choosing a candidate with the highest entropy: 𝙋𝙄𝙉𝙆𝙔 with entropy: 1.6143
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜⬜⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 3.861 bits, Posterior Entropy: 9.997, 
Guess: 𝘾𝙊𝙇𝙄𝙉, Feedback: ⬜⬜⬜🟨🟨, Prior Entropy: 9.997, Expected Info Gain: 5.938 bits, Actual Info Gain: 4.382 bits, Posterior Entropy: 5.615, 
Guess: 𝘿𝙄𝙉𝙂𝙔, Feedback: ⬜🟩🟩⬜🟩, Prior Entropy: 5.615, Expected Info Gain: 5.938 bits, Actual Info Gain: 1.914 bits, Posterior Entropy: 3.700, 
Guess: 𝙋𝙄𝙉𝙆𝙔, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'PINKY': 32.9349 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙂𝙍𝘼𝙄𝙇 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟨🟨⬜⬜
Actual Info Gain: 5.6938 bits
Posterior entropy: 8.164906926675688
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 8.164906926675688, 'actual_info_gain': 5.693754058047089, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
287 candidate words remaining.
Top 10 guesses: [('BRAAI', '4.634'), ('BROIL', '4.601'), ('MOANA', '4.596'), ('LOGIN', '4.596'), ('COLIN', '4.581'), ('GROIN', '4.579'), ('GRAAL', '4.570'), ('CRAAL', '4.564'), ('LIANA', '4.563'), ('COALA', '4.560')]
Top entropy choice: 𝘽𝙍𝘼𝘼𝙄 with entropy: 4.6343
Guess count low, choosing the word with highest entropy: 𝘽𝙍𝘼𝘼𝙄 with entropy: 4.6343

Guess: 𝘽𝙍𝘼𝘼𝙄, Feedback: ⬜🟩🟩⬜🟨
Actual Info Gain: 4.8430 bits
Posterior entropy: 3.321928094887362
entropy_info: {'prior_entropy': 8.164906926675688, 'posterior_entropy': 3.321928094887362, 'actual_info_gain': 4.842978831788326, 'expected_info_gain': 4.634280696834034}
----------
The bot is making a guess...
10 candidate words remaining.
Top 10 guesses: [('CLANG', '3.122'), ('CLING', '3.122'), ('CLUNG', '3.122'), ('FLING', '3.122'), ('FLONG', '3.122'), ('FLUNG', '3.122'), ('GLAND', '2.846'), ('CLAGS', '2.722'), ('CLEGS', '2.722'), ('CLOGS', '2.722')]
Top entropy choice: 𝘾𝙇𝘼𝙉𝙂 with entropy: 3.1219
Using common exploratory word: 𝘾𝙇𝘼𝙉𝙂 with entropy: 3.1219

Guess: 𝘾𝙇𝘼𝙉𝙂, Feedback: ⬜🟨🟩⬜🟨
Actual Info Gain: 3.3219 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 3.321928094887362, 'posterior_entropy': 0.0, 'actual_info_gain': 3.321928094887362, 'expected_info_gain': 4.634280696834034}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('GRAIL', '0.000')]
Top entropy choice: 𝙂𝙍𝘼𝙄𝙇 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝙂𝙍𝘼𝙄𝙇 with entropy: 0.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟨🟨⬜⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 5.694 bits, Posterior Entropy: 8.165, 
Guess: 𝘽𝙍𝘼𝘼𝙄, Feedback: ⬜🟩🟩⬜🟨, Prior Entropy: 8.165, Expected Info Gain: 4.634 bits, Actual Info Gain: 4.843 bits, Posterior Entropy: 3.322, 
Guess: 𝘾𝙇𝘼𝙉𝙂, Feedback: ⬜🟨🟩⬜🟨, Prior Entropy: 3.322, Expected Info Gain: 4.634 bits, Actual Info Gain: 3.322 bits, Posterior Entropy: 0.000, 
Guess: 𝙂𝙍𝘼𝙄𝙇, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'GRAIL': 8.7193 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙏𝙍𝘼𝙎𝙃 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟩🟨🟨⬜🟨
Actual Info Gain: 13.8587 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 0.0, 'actual_info_gain': 13.858660984722777, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('TRASH', '0.000')]
Top entropy choice: 𝙏𝙍𝘼𝙎𝙃 with entropy: 0.0000
Guess count low, choosing the word with highest entropy: 𝙏𝙍𝘼𝙎𝙃 with entropy: 0.0000
You won! Amount of guesses: 2

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟩🟨🟨⬜🟨, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 13.859 bits, Posterior Entropy: 0.000, 
Guess: 𝙏𝙍𝘼𝙎𝙃, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'TRASH': 0.0061 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙊𝙇𝘿𝙀𝙉 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜🟩⬜
Actual Info Gain: 4.9914 bits
Posterior entropy: 8.867278739709661
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 8.867278739709661, 'actual_info_gain': 4.991382245013115, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
467 candidate words remaining.
Top 10 guesses: [('INDOL', '4.933'), ('DINLO', '4.859'), ('LOWND', '4.836'), ('BLOND', '4.832'), ('LOUND', '4.773'), ('LINDY', '4.749'), ('DOLCI', '4.730'), ('DOILY', '4.707'), ('NOULD', '4.703'), ('MODIN', '4.688')]
Top entropy choice: 𝙄𝙉𝘿𝙊𝙇 with entropy: 4.9332
Guess count low, choosing the word with highest entropy: 𝙄𝙉𝘿𝙊𝙇 with entropy: 4.9332

Guess: 𝙄𝙉𝘿𝙊𝙇, Feedback: ⬜🟨🟩🟨🟨
Actual Info Gain: 7.8673 bits
Posterior entropy: 1.0
entropy_info: {'prior_entropy': 8.867278739709661, 'posterior_entropy': 1.0, 'actual_info_gain': 7.867278739709661, 'expected_info_gain': 4.933171607708794}
----------
The bot is making a guess...
2 candidate words remaining.
Top 10 guesses: [('LODEN', '1.000'), ('OLDEN', '1.000')]
Top entropy choice: 𝙇𝙊𝘿𝙀𝙉 with entropy: 1.0000
Few candidates left, going through them all to pick a common word...
Using common word: 𝙊𝙇𝘿𝙀𝙉 with entropy: 1.0000
You won! Amount of guesses: 3

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜🟩⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 4.991 bits, Posterior Entropy: 8.867, 
Guess: 𝙄𝙉𝘿𝙊𝙇, Feedback: ⬜🟨🟩🟨🟨, Prior Entropy: 8.867, Expected Info Gain: 4.933 bits, Actual Info Gain: 7.867 bits, Posterior Entropy: 1.000, 
Guess: 𝙊𝙇𝘿𝙀𝙉, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'OLDEN': 14.9448 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝘽𝘼𝙎𝙄𝙉 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟩⬜⬜🟨
Actual Info Gain: 6.8034 bits
Posterior entropy: 7.05528243550119
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 7.05528243550119, 'actual_info_gain': 6.803378549221587, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
133 candidate words remaining.
Top 10 guesses: [('LYSIN', '4.282'), ('SYBIL', '4.252'), ('LOSSY', '4.134'), ('SHILY', '4.113'), ('SYLIS', '4.112'), ('SONSY', '4.104'), ('SIBYL', '4.099'), ('SPOIL', '4.098'), ('SHINY', '4.092'), ('BYSSI', '4.090')]
Top entropy choice: 𝙇𝙔𝙎𝙄𝙉 with entropy: 4.2816
Guess count low, choosing the word with highest entropy: 𝙇𝙔𝙎𝙄𝙉 with entropy: 4.2816

Guess: 𝙇𝙔𝙎𝙄𝙉, Feedback: ⬜⬜🟩🟩🟩
Actual Info Gain: 6.0553 bits
Posterior entropy: 1.0
entropy_info: {'prior_entropy': 7.05528243550119, 'posterior_entropy': 1.0, 'actual_info_gain': 6.05528243550119, 'expected_info_gain': 4.281552287338487}
----------
The bot is making a guess...
2 candidate words remaining.
Top 10 guesses: [('BASIN', '1.000'), ('SASIN', '1.000')]
Top entropy choice: 𝘽𝘼𝙎𝙄𝙉 with entropy: 1.0000
Few candidates left, going through them all to pick a common word...
Using common word: 𝘽𝘼𝙎𝙄𝙉 with entropy: 1.0000
You won! Amount of guesses: 3

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟩⬜⬜🟨, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 6.803 bits, Posterior Entropy: 7.055, 
Guess: 𝙇𝙔𝙎𝙄𝙉, Feedback: ⬜⬜🟩🟩🟩, Prior Entropy: 7.055, Expected Info Gain: 4.282 bits, Actual Info Gain: 6.055 bits, Posterior Entropy: 1.000, 
Guess: 𝘽𝘼𝙎𝙄𝙉, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'BASIN': 4.1346 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝘾𝙍𝘼𝙈𝙋 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟨🟨⬜⬜
Actual Info Gain: 5.6938 bits
Posterior entropy: 8.164906926675688
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 8.164906926675688, 'actual_info_gain': 5.693754058047089, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
287 candidate words remaining.
Top 10 guesses: [('BRAAI', '4.634'), ('BROIL', '4.601'), ('MOANA', '4.596'), ('LOGIN', '4.596'), ('COLIN', '4.581'), ('GROIN', '4.579'), ('GRAAL', '4.570'), ('CRAAL', '4.564'), ('LIANA', '4.563'), ('COALA', '4.560')]
Top entropy choice: 𝘽𝙍𝘼𝘼𝙄 with entropy: 4.6343
Guess count low, choosing the word with highest entropy: 𝘽𝙍𝘼𝘼𝙄 with entropy: 4.6343

Guess: 𝘽𝙍𝘼𝘼𝙄, Feedback: ⬜🟩🟩⬜⬜
Actual Info Gain: 2.9950 bits
Posterior entropy: 5.169925001442312
entropy_info: {'prior_entropy': 8.164906926675688, 'posterior_entropy': 5.169925001442312, 'actual_info_gain': 2.9949819252333754, 'expected_info_gain': 4.634280696834034}
----------
The bot is making a guess...
36 candidate words remaining.
Top 10 guesses: [('CHYND', '3.906'), ('KNOWD', '3.856'), ('POWND', '3.848'), ('YCOND', '3.816'), ('DYKON', '3.704'), ('PONCY', '3.669'), ('PLONG', '3.662'), ('DOWNY', '3.613'), ('PLUCK', '3.593'), ('PLOCK', '3.582')]
Top entropy choice: 𝘾𝙃𝙔𝙉𝘿 with entropy: 3.9058
Using common exploratory word: 𝘿𝙊𝙒𝙉𝙔 with entropy: 3.6128

Guess: 𝘿𝙊𝙒𝙉𝙔, Feedback: ⬜⬜⬜⬜⬜
Actual Info Gain: 2.3626 bits
Posterior entropy: 2.807354922057604
entropy_info: {'prior_entropy': 5.169925001442312, 'posterior_entropy': 2.807354922057604, 'actual_info_gain': 2.362570079384708, 'expected_info_gain': 4.634280696834034}
----------
The bot is making a guess...
7 candidate words remaining.
Top 10 guesses: [('CAMPI', '2.807'), ('CAMPO', '2.807'), ('CAMPS', '2.807'), ('CAMPY', '2.807'), ('CAPHS', '2.807'), ('CAUPS', '2.807'), ('CHAPE', '2.807'), ('CHAPS', '2.807'), ('CHAPT', '2.807'), ('CHIPS', '2.807')]
Top entropy choice: 𝘾𝘼𝙈𝙋𝙄 with entropy: 2.8074
Guess count high, choosing a candidate with the highest entropy: 𝙂𝙍𝘼𝙋𝙃 with entropy: 2.5216

Guess: 𝙂𝙍𝘼𝙋𝙃, Feedback: ⬜🟩🟩🟨⬜
Actual Info Gain: 2.8074 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 2.807354922057604, 'posterior_entropy': 0.0, 'actual_info_gain': 2.807354922057604, 'expected_info_gain': 2.5216406363433186}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('CRAMP', '0.000')]
Top entropy choice: 𝘾𝙍𝘼𝙈𝙋 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝘾𝙍𝘼𝙈𝙋 with entropy: 0.0000
You won! Amount of guesses: 5

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟨🟨⬜⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 5.694 bits, Posterior Entropy: 8.165, 
Guess: 𝘽𝙍𝘼𝘼𝙄, Feedback: ⬜🟩🟩⬜⬜, Prior Entropy: 8.165, Expected Info Gain: 4.634 bits, Actual Info Gain: 2.995 bits, Posterior Entropy: 5.170, 
Guess: 𝘿𝙊𝙒𝙉𝙔, Feedback: ⬜⬜⬜⬜⬜, Prior Entropy: 5.170, Expected Info Gain: 4.634 bits, Actual Info Gain: 2.363 bits, Posterior Entropy: 2.807, 
Guess: 𝙂𝙍𝘼𝙋𝙃, Feedback: ⬜🟩🟩🟨⬜, Prior Entropy: 2.807, Expected Info Gain: 2.522 bits, Actual Info Gain: 2.807 bits, Posterior Entropy: 0.000, 
Guess: 𝘾𝙍𝘼𝙈𝙋, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'CRAMP': 9.4106 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝘽𝙀𝙇𝙇𝙔 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜🟨⬜
Actual Info Gain: 4.2869 bits
Posterior entropy: 9.571752643503546
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 9.571752643503546, 'actual_info_gain': 4.286908341219231, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
761 candidate words remaining.
Top 10 guesses: [('DINLO', '5.536'), ('COLIN', '5.491'), ('DOLIE', '5.487'), ('NOILY', '5.417'), ('MOILE', '5.403'), ('OLDIE', '5.394'), ('DOILY', '5.388'), ('MOLIE', '5.386'), ('LEONE', '5.379'), ('LOGIN', '5.371')]
Top entropy choice: 𝘿𝙄𝙉𝙇𝙊 with entropy: 5.5362
Guess count low, choosing the word with highest entropy: 𝘿𝙄𝙉𝙇𝙊 with entropy: 5.5362

Guess: 𝘿𝙄𝙉𝙇𝙊, Feedback: ⬜⬜⬜🟩⬜
Actual Info Gain: 4.7138 bits
Posterior entropy: 4.857980995127572
entropy_info: {'prior_entropy': 9.571752643503546, 'posterior_entropy': 4.857980995127572, 'actual_info_gain': 4.713771648375974, 'expected_info_gain': 5.536189971336713}
----------
The bot is making a guess...
29 candidate words remaining.
Top 10 guesses: [('BEECH', '3.762'), ('BECKE', '3.650'), ('MEECH', '3.646'), ('KEECH', '3.621'), ('FEHME', '3.581'), ('PECKE', '3.573'), ('HULKY', '3.486'), ('PHEME', '3.479'), ('PULLY', '3.427'), ('BULKY', '3.401')]
Top entropy choice: 𝘽𝙀𝙀𝘾𝙃 with entropy: 3.7623
Using common exploratory word: 𝘽𝙀𝙀𝘾𝙃 with entropy: 3.7623

Guess: 𝘽𝙀𝙀𝘾𝙃, Feedback: 🟩🟩⬜⬜⬜
Actual Info Gain: 4.8580 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 4.857980995127572, 'posterior_entropy': 0.0, 'actual_info_gain': 4.857980995127572, 'expected_info_gain': 5.536189971336713}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('BELLY', '0.000')]
Top entropy choice: 𝘽𝙀𝙇𝙇𝙔 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝘽𝙀𝙇𝙇𝙔 with entropy: 0.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜🟨⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 4.287 bits, Posterior Entropy: 9.572, 
Guess: 𝘿𝙄𝙉𝙇𝙊, Feedback: ⬜⬜⬜🟩⬜, Prior Entropy: 9.572, Expected Info Gain: 5.536 bits, Actual Info Gain: 4.714 bits, Posterior Entropy: 4.858, 
Guess: 𝘽𝙀𝙀𝘾𝙃, Feedback: 🟩🟩⬜⬜⬜, Prior Entropy: 4.858, Expected Info Gain: 5.536 bits, Actual Info Gain: 4.858 bits, Posterior Entropy: 0.000, 
Guess: 𝘽𝙀𝙇𝙇𝙔, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'BELLY': 24.0731 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝘾𝘼𝙍𝙉𝙀 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟩🟩🟨⬜
Actual Info Gain: 9.1038 bits
Posterior entropy: 4.754887502163468
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 4.754887502163468, 'actual_info_gain': 9.103773482559308, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
27 candidate words remaining.
Top 10 guesses: [('PYGAL', '2.918'), ('GILPY', '2.845'), ('GLYPH', '2.845'), ('GULPY', '2.845'), ('GLEBY', '2.827'), ('GLOBY', '2.827'), ('GLAMP', '2.817'), ('PLAGA', '2.799'), ('GULAB', '2.750'), ('BILGY', '2.725')]
Top entropy choice: 𝙋𝙔𝙂𝘼𝙇 with entropy: 2.9185
Guess count low, choosing the word with highest entropy: 𝙋𝙔𝙂𝘼𝙇 with entropy: 2.9185

Guess: 𝙋𝙔𝙂𝘼𝙇, Feedback: ⬜⬜⬜🟨⬜
Actual Info Gain: 1.4330 bits
Posterior entropy: 3.321928094887362
entropy_info: {'prior_entropy': 4.754887502163468, 'posterior_entropy': 3.321928094887362, 'actual_info_gain': 1.432959407276106, 'expected_info_gain': 2.918450134553325}
----------
The bot is making a guess...
10 candidate words remaining.
Top 10 guesses: [('CANDO', '2.522'), ('CANDY', '2.522'), ('CNIDA', '2.522'), ('CONDO', '2.522'), ('CUNDY', '2.522'), ('DANCE', '2.522'), ('DANCY', '2.522'), ('DENCH', '2.522'), ('DUNCE', '2.522'), ('DUNCH', '2.522')]
Top entropy choice: 𝘾𝘼𝙉𝘿𝙊 with entropy: 2.5219
Using common exploratory word: 𝘾𝘼𝙉𝘿𝙔 with entropy: 2.5219

Guess: 𝘾𝘼𝙉𝘿𝙔, Feedback: 🟩🟩🟨⬜⬜
Actual Info Gain: 3.3219 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 3.321928094887362, 'posterior_entropy': 0.0, 'actual_info_gain': 3.321928094887362, 'expected_info_gain': 2.918450134553325}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('CARNE', '0.000')]
Top entropy choice: 𝘾𝘼𝙍𝙉𝙀 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝘾𝘼𝙍𝙉𝙀 with entropy: 0.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟩🟩🟨⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 9.104 bits, Posterior Entropy: 4.755, 
Guess: 𝙋𝙔𝙂𝘼𝙇, Feedback: ⬜⬜⬜🟨⬜, Prior Entropy: 4.755, Expected Info Gain: 2.918 bits, Actual Info Gain: 1.433 bits, Posterior Entropy: 3.322, 
Guess: 𝘾𝘼𝙉𝘿𝙔, Feedback: 🟩🟩🟨⬜⬜, Prior Entropy: 3.322, Expected Info Gain: 2.918 bits, Actual Info Gain: 3.322 bits, Posterior Entropy: 0.000, 
Guess: 𝘾𝘼𝙍𝙉𝙀, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'CARNE': 1.0818 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙏𝙃𝙀𝙍𝙀 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟩⬜🟨🟨⬜
Actual Info Gain: 9.0513 bits
Posterior entropy: 4.807354922057604
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 4.807354922057604, 'actual_info_gain': 9.051306062665173, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
28 candidate words remaining.
Top 10 guesses: [('PRINK', '3.753'), ('DRINK', '3.709'), ('KIORE', '3.682'), ('CRINK', '3.638'), ('CRINE', '3.621'), ('KOINE', '3.601'), ('ORPIN', '3.598'), ('PRION', '3.598'), ('PROIN', '3.598'), ('NOIRE', '3.584')]
Top entropy choice: 𝙋𝙍𝙄𝙉𝙆 with entropy: 3.7534
Guess count low, choosing the word with highest entropy: 𝙋𝙍𝙄𝙉𝙆 with entropy: 3.7534

Guess: 𝙋𝙍𝙄𝙉𝙆, Feedback: ⬜🟨⬜⬜⬜
Actual Info Gain: 3.2224 bits
Posterior entropy: 1.584962500721156
entropy_info: {'prior_entropy': 4.807354922057604, 'posterior_entropy': 1.584962500721156, 'actual_info_gain': 3.2223924213364477, 'expected_info_gain': 3.7534343861887853}
----------
The bot is making a guess...
3 candidate words remaining.
Top 10 guesses: [('THERE', '1.585'), ('THERM', '1.585'), ('THEOR', '0.918')]
Top entropy choice: 𝙏𝙃𝙀𝙍𝙀 with entropy: 1.5850
Few candidates left, going through them all to pick a common word...
Using common word: 𝙏𝙃𝙀𝙍𝙀 with entropy: 1.5850
You won! Amount of guesses: 3

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟩⬜🟨🟨⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 9.051 bits, Posterior Entropy: 4.807, 
Guess: 𝙋𝙍𝙄𝙉𝙆, Feedback: ⬜🟨⬜⬜⬜, Prior Entropy: 4.807, Expected Info Gain: 3.753 bits, Actual Info Gain: 3.222 bits, Posterior Entropy: 1.585, 
Guess: 𝙏𝙃𝙀𝙍𝙀, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'THERE': 0.8211 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝘼𝙇𝙏𝙃𝙊 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟨🟨⬜⬜⬜
Actual Info Gain: 6.6298 bits
Posterior entropy: 7.22881869049588
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 7.22881869049588, 'actual_info_gain': 6.629842294226896, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
150 candidate words remaining.
Top 10 guesses: [('LIANA', '4.828'), ('ALOIN', '4.728'), ('COALA', '4.702'), ('NITTO', '4.661'), ('PLOIT', '4.652'), ('POINT', '4.636'), ('COLIN', '4.634'), ('CLINT', '4.625'), ('PITOT', '4.606'), ('NICOL', '4.578')]
Top entropy choice: 𝙇𝙄𝘼𝙉𝘼 with entropy: 4.8283
Guess count low, choosing the word with highest entropy: 𝙇𝙄𝘼𝙉𝘼 with entropy: 4.8283

Guess: 𝙇𝙄𝘼𝙉𝘼, Feedback: 🟨⬜🟨⬜⬜
Actual Info Gain: 3.6439 bits
Posterior entropy: 3.584962500721156
entropy_info: {'prior_entropy': 7.22881869049588, 'posterior_entropy': 3.584962500721156, 'actual_info_gain': 3.6438561897747244, 'expected_info_gain': 4.8283043129740655}
----------
The bot is making a guess...
12 candidate words remaining.
Top 10 guesses: [('ADOPT', '3.189'), ('FLOOD', '3.189'), ('FLOPS', '3.085'), ('ABORT', '3.022'), ('ABOUT', '3.022'), ('AFOOT', '3.022'), ('AFTOS', '3.022'), ('ALOFT', '3.022'), ('APORT', '3.022'), ('BLOAT', '3.022')]
Top entropy choice: 𝘼𝘿𝙊𝙋𝙏 with entropy: 3.1887
Using common exploratory word: 𝘼𝘿𝙊𝙋𝙏 with entropy: 3.1887

Guess: 𝘼𝘿𝙊𝙋𝙏, Feedback: 🟩⬜🟨⬜🟨
Actual Info Gain: 3.5850 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 3.584962500721156, 'posterior_entropy': 0.0, 'actual_info_gain': 3.584962500721156, 'expected_info_gain': 4.8283043129740655}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('ALTHO', '0.000')]
Top entropy choice: 𝘼𝙇𝙏𝙃𝙊 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝘼𝙇𝙏𝙃𝙊 with entropy: 0.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟨🟨⬜⬜⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 6.630 bits, Posterior Entropy: 7.229, 
Guess: 𝙇𝙄𝘼𝙉𝘼, Feedback: 🟨⬜🟨⬜⬜, Prior Entropy: 7.229, Expected Info Gain: 4.828 bits, Actual Info Gain: 3.644 bits, Posterior Entropy: 3.585, 
Guess: 𝘼𝘿𝙊𝙋𝙏, Feedback: 🟩⬜🟨⬜🟨, Prior Entropy: 3.585, Expected Info Gain: 4.828 bits, Actual Info Gain: 3.585 bits, Posterior Entropy: 0.000, 
Guess: 𝘼𝙇𝙏𝙃𝙊, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'ALTHO': 4.7971 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙈𝘼𝙍𝙄𝘼 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟩🟩⬜⬜
Actual Info Gain: 6.9760 bits
Posterior entropy: 6.882643049361842
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 6.882643049361842, 'actual_info_gain': 6.976017935360935, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
118 candidate words remaining.
Top 10 guesses: [('MINCY', '3.994'), ('MOLDY', '3.993'), ('COMBY', '3.944'), ('MYOID', '3.913'), ('CYMOL', '3.890'), ('COMBI', '3.882'), ('MOCKY', '3.855'), ('MICKY', '3.849'), ('DIMBO', '3.844'), ('MIDGY', '3.812')]
Top entropy choice: 𝙈𝙄𝙉𝘾𝙔 with entropy: 3.9942
Guess count low, choosing the word with highest entropy: 𝙈𝙄𝙉𝘾𝙔 with entropy: 3.9942

Guess: 𝙈𝙄𝙉𝘾𝙔, Feedback: 🟩🟨⬜⬜⬜
Actual Info Gain: 4.8826 bits
Posterior entropy: 2.0
entropy_info: {'prior_entropy': 6.882643049361842, 'posterior_entropy': 2.0, 'actual_info_gain': 4.882643049361842, 'expected_info_gain': 3.994197821504514}
----------
The bot is making a guess...
4 candidate words remaining.
Top 10 guesses: [('AALII', '2.000'), ('ADDRA', '2.000'), ('ADLIB', '2.000'), ('AFALD', '2.000'), ('ALAND', '2.000'), ('ALARM', '2.000'), ('ALARY', '2.000'), ('ALBID', '2.000'), ('ALCID', '2.000'), ('ALDEA', '2.000')]
Top entropy choice: 𝘼𝘼𝙇𝙄𝙄 with entropy: 2.0000
Using common exploratory word: 𝘼𝙇𝘼𝙍𝙈 with entropy: 2.0000

Guess: 𝘼𝙇𝘼𝙍𝙈, Feedback: 🟨⬜🟨🟨🟨
Actual Info Gain: 2.0000 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 2.0, 'posterior_entropy': 0.0, 'actual_info_gain': 2.0, 'expected_info_gain': 3.994197821504514}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('MARIA', '0.000')]
Top entropy choice: 𝙈𝘼𝙍𝙄𝘼 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝙈𝘼𝙍𝙄𝘼 with entropy: 0.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟩🟩⬜⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 6.976 bits, Posterior Entropy: 6.883, 
Guess: 𝙈𝙄𝙉𝘾𝙔, Feedback: 🟩🟨⬜⬜⬜, Prior Entropy: 6.883, Expected Info Gain: 3.994 bits, Actual Info Gain: 4.883 bits, Posterior Entropy: 2.000, 
Guess: 𝘼𝙇𝘼𝙍𝙈, Feedback: 🟨⬜🟨🟨🟨, Prior Entropy: 2.000, Expected Info Gain: 3.994 bits, Actual Info Gain: 2.000 bits, Posterior Entropy: 0.000, 
Guess: 𝙈𝘼𝙍𝙄𝘼, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'MARIA': 3.7842 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙏𝘼𝙏𝙀𝙍 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟩🟩🟨🟩⬜
Actual Info Gain: 10.6887 bits
Posterior entropy: 3.169925001442312
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 3.169925001442312, 'actual_info_gain': 10.688735983280464, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
9 candidate words remaining.
Top 10 guesses: [('BLIMP', '1.880'), ('PLOMB', '1.880'), ('PLUMB', '1.880'), ('BALKS', '1.447'), ('BALKY', '1.447'), ('BAULK', '1.447'), ('BILKS', '1.447'), ('BLACK', '1.447'), ('BLANK', '1.447'), ('BLEAK', '1.447')]
Top entropy choice: 𝘽𝙇𝙄𝙈𝙋 with entropy: 1.8800
Guess count low, choosing the word with highest entropy: 𝘽𝙇𝙄𝙈𝙋 with entropy: 1.8800

Guess: 𝘽𝙇𝙄𝙈𝙋, Feedback: ⬜⬜⬜⬜⬜
Actual Info Gain: 0.8480 bits
Posterior entropy: 2.321928094887362
entropy_info: {'prior_entropy': 3.169925001442312, 'posterior_entropy': 2.321928094887362, 'actual_info_gain': 0.84799690655495, 'expected_info_gain': 1.8799649487271108}
----------
The bot is making a guess...
5 candidate words remaining.
Top 10 guesses: [('KOTOW', '1.922'), ('TWIXT', '1.922'), ('VITEX', '1.922'), ('ADVEW', '1.371'), ('ASKEW', '1.371'), ('AVOWS', '1.371'), ('AWAKE', '1.371'), ('AWAVE', '1.371'), ('AWKIN', '1.371'), ('AWOKE', '1.371')]
Top entropy choice: 𝙆𝙊𝙏𝙊𝙒 with entropy: 1.9219

Guess: 𝙆𝙊𝙏𝙊𝙒, Feedback: ⬜⬜🟩⬜⬜
Actual Info Gain: 2.3219 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 2.321928094887362, 'posterior_entropy': 0.0, 'actual_info_gain': 2.321928094887362, 'expected_info_gain': 1.9219280948873623}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('TATER', '0.000')]
Top entropy choice: 𝙏𝘼𝙏𝙀𝙍 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝙏𝘼𝙏𝙀𝙍 with entropy: 0.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟩🟩🟨🟩⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 10.689 bits, Posterior Entropy: 3.170, 
Guess: 𝘽𝙇𝙄𝙈𝙋, Feedback: ⬜⬜⬜⬜⬜, Prior Entropy: 3.170, Expected Info Gain: 1.880 bits, Actual Info Gain: 0.848 bits, Posterior Entropy: 2.322, 
Guess: 𝙆𝙊𝙏𝙊𝙒, Feedback: ⬜⬜🟩⬜⬜, Prior Entropy: 2.322, Expected Info Gain: 1.922 bits, Actual Info Gain: 2.322 bits, Posterior Entropy: 0.000, 
Guess: 𝙏𝘼𝙏𝙀𝙍, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'TATER': 0.4182 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙃𝙊𝙉𝙀𝘿 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜🟩⬜
Actual Info Gain: 4.9914 bits
Posterior entropy: 8.867278739709661
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 8.867278739709661, 'actual_info_gain': 4.991382245013115, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
467 candidate words remaining.
Top 10 guesses: [('INDOL', '4.933'), ('DINLO', '4.859'), ('LOWND', '4.836'), ('BLOND', '4.832'), ('LOUND', '4.773'), ('LINDY', '4.749'), ('DOLCI', '4.730'), ('DOILY', '4.707'), ('NOULD', '4.703'), ('MODIN', '4.688')]
Top entropy choice: 𝙄𝙉𝘿𝙊𝙇 with entropy: 4.9332
Guess count low, choosing the word with highest entropy: 𝙄𝙉𝘿𝙊𝙇 with entropy: 4.9332

Guess: 𝙄𝙉𝘿𝙊𝙇, Feedback: ⬜🟨🟨🟨⬜
Actual Info Gain: 5.4078 bits
Posterior entropy: 3.4594316186372973
entropy_info: {'prior_entropy': 8.867278739709661, 'posterior_entropy': 3.4594316186372973, 'actual_info_gain': 5.407847121072364, 'expected_info_gain': 4.933171607708794}
----------
The bot is making a guess...
11 candidate words remaining.
Top 10 guesses: [('ZONED', '2.664'), ('WANZE', '2.595'), ('WINZE', '2.595'), ('BONZE', '2.550'), ('DANCY', '2.550'), ('DONNY', '2.550'), ('DONSY', '2.550'), ('DOOZY', '2.550'), ('NONCY', '2.550'), ('VOZHD', '2.550')]
Top entropy choice: 𝙕𝙊𝙉𝙀𝘿 with entropy: 2.6635
Using common exploratory word: 𝙕𝙊𝙉𝙀𝘿 with entropy: 2.6635

Guess: 𝙕𝙊𝙉𝙀𝘿, Feedback: ⬜🟩🟩🟩🟩
Actual Info Gain: 1.8745 bits
Posterior entropy: 1.584962500721156
entropy_info: {'prior_entropy': 3.4594316186372973, 'posterior_entropy': 1.584962500721156, 'actual_info_gain': 1.8744691179161412, 'expected_info_gain': 4.933171607708794}
----------
The bot is making a guess...
3 candidate words remaining.
Top 10 guesses: [('BONED', '0.918'), ('CONED', '0.918'), ('HONED', '0.918')]
Top entropy choice: 𝘽𝙊𝙉𝙀𝘿 with entropy: 0.9183
Guess count high, choosing a candidate with the highest entropy: 𝘽𝙊𝙉𝙀𝘿 with entropy: 0.9183

Guess: 𝘽𝙊𝙉𝙀𝘿, Feedback: ⬜🟩🟩🟩🟩
Actual Info Gain: 0.5850 bits
Posterior entropy: 1.0
entropy_info: {'prior_entropy': 1.584962500721156, 'posterior_entropy': 1.0, 'actual_info_gain': 0.5849625007211561, 'expected_info_gain': 0.9182958340544896}
----------
The bot is making a guess...
2 candidate words remaining.
Top 10 guesses: [('CONED', '1.000'), ('HONED', '1.000')]
Top entropy choice: 𝘾𝙊𝙉𝙀𝘿 with entropy: 1.0000
Guess count high, choosing a candidate with the highest entropy: 𝙃𝙊𝙉𝙀𝘿 with entropy: 1.0000
You won! Amount of guesses: 5

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜🟩⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 4.991 bits, Posterior Entropy: 8.867, 
Guess: 𝙄𝙉𝘿𝙊𝙇, Feedback: ⬜🟨🟨🟨⬜, Prior Entropy: 8.867, Expected Info Gain: 4.933 bits, Actual Info Gain: 5.408 bits, Posterior Entropy: 3.459, 
Guess: 𝙕𝙊𝙉𝙀𝘿, Feedback: ⬜🟩🟩🟩🟩, Prior Entropy: 3.459, Expected Info Gain: 4.933 bits, Actual Info Gain: 1.874 bits, Posterior Entropy: 1.585, 
Guess: 𝘽𝙊𝙉𝙀𝘿, Feedback: ⬜🟩🟩🟩🟩, Prior Entropy: 1.585, Expected Info Gain: 0.918 bits, Actual Info Gain: 0.585 bits, Posterior Entropy: 1.000, 
Guess: 𝙃𝙊𝙉𝙀𝘿, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'HONED': 14.6858 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝘼𝙇𝙇𝘼𝙉 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟨⬜⬜⬜
Actual Info Gain: 4.3449 bits
Posterior entropy: 9.513727595952437
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 9.513727595952437, 'actual_info_gain': 4.34493338877034, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
731 candidate words remaining.
Top 10 guesses: [('COLIN', '5.613'), ('LIANA', '5.503'), ('ALOIN', '5.496'), ('PILON', '5.458'), ('PINOL', '5.439'), ('DINLO', '5.430'), ('LOGIN', '5.388'), ('NICOL', '5.363'), ('NMOLI', '5.336'), ('ULMIN', '5.322')]
Top entropy choice: 𝘾𝙊𝙇𝙄𝙉 with entropy: 5.6132
Guess count low, choosing the word with highest entropy: 𝘾𝙊𝙇𝙄𝙉 with entropy: 5.6132

Guess: 𝘾𝙊𝙇𝙄𝙉, Feedback: ⬜⬜🟩⬜🟩
Actual Info Gain: 7.1918 bits
Posterior entropy: 2.321928094887362
entropy_info: {'prior_entropy': 9.513727595952437, 'posterior_entropy': 2.321928094887362, 'actual_info_gain': 7.191799501065075, 'expected_info_gain': 5.613155296118865}
----------
The bot is making a guess...
5 candidate words remaining.
Top 10 guesses: [('AUNTY', '2.322'), ('AYAHS', '2.322'), ('BUBBY', '2.322'), ('BUDDY', '2.322'), ('BUFFY', '2.322'), ('BUFTY', '2.322'), ('BUGGY', '2.322'), ('BULGY', '2.322'), ('BULKY', '2.322'), ('BULLY', '2.322')]
Top entropy choice: 𝘼𝙐𝙉𝙏𝙔 with entropy: 2.3219
Using common exploratory word: 𝘼𝙐𝙉𝙏𝙔 with entropy: 2.3219

Guess: 𝘼𝙐𝙉𝙏𝙔, Feedback: 🟩⬜🟨⬜⬜
Actual Info Gain: 2.3219 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 2.321928094887362, 'posterior_entropy': 0.0, 'actual_info_gain': 2.321928094887362, 'expected_info_gain': 5.613155296118865}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('ALLAN', '0.000')]
Top entropy choice: 𝘼𝙇𝙇𝘼𝙉 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝘼𝙇𝙇𝘼𝙉 with entropy: 0.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟨⬜⬜⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 4.345 bits, Posterior Entropy: 9.514, 
Guess: 𝘾𝙊𝙇𝙄𝙉, Feedback: ⬜⬜🟩⬜🟩, Prior Entropy: 9.514, Expected Info Gain: 5.613 bits, Actual Info Gain: 7.192 bits, Posterior Entropy: 2.322, 
Guess: 𝘼𝙐𝙉𝙏𝙔, Feedback: 🟩⬜🟨⬜⬜, Prior Entropy: 2.322, Expected Info Gain: 5.613 bits, Actual Info Gain: 2.322 bits, Posterior Entropy: 0.000, 
Guess: 𝘼𝙇𝙇𝘼𝙉, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'ALLAN': 23.4716 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙎𝙌𝙐𝘼𝘿 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟨⬜⬜🟨
Actual Info Gain: 6.2737 bits
Posterior entropy: 7.584962500721156
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 7.584962500721156, 'actual_info_gain': 6.273698484001621, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
192 candidate words remaining.
Top 10 guesses: [('SPAIL', '4.682'), ('PLANH', '4.659'), ('SHALM', '4.648'), ('PILCH', '4.602'), ('PLAIN', '4.596'), ('SNAIL', '4.587'), ('MILCH', '4.581'), ('CHOIL', '4.568'), ('SCAIL', '4.568'), ('SHULN', '4.548')]
Top entropy choice: 𝙎𝙋𝘼𝙄𝙇 with entropy: 4.6818
Guess count low, choosing the word with highest entropy: 𝙎𝙋𝘼𝙄𝙇 with entropy: 4.6818

Guess: 𝙎𝙋𝘼𝙄𝙇, Feedback: 🟩⬜🟨⬜⬜
Actual Info Gain: 3.4150 bits
Posterior entropy: 4.169925001442312
entropy_info: {'prior_entropy': 7.584962500721156, 'posterior_entropy': 4.169925001442312, 'actual_info_gain': 3.415037499278844, 'expected_info_gain': 4.681813620578765}
----------
The bot is making a guess...
18 candidate words remaining.
Top 10 guesses: [('BHUNA', '3.725'), ('BUNDH', '3.725'), ('DUNAM', '3.725'), ('DUNCH', '3.725'), ('HUDNA', '3.725'), ('BUNCH', '3.684'), ('NUCHA', '3.684'), ('BUGAN', '3.614'), ('BUNAS', '3.614'), ('BUNIA', '3.614')]
Top entropy choice: 𝘽𝙃𝙐𝙉𝘼 with entropy: 3.7255
Using common exploratory word: 𝘽𝙐𝙉𝘾𝙃 with entropy: 3.6835

Guess: 𝘽𝙐𝙉𝘾𝙃, Feedback: ⬜🟨⬜⬜⬜
Actual Info Gain: 3.1699 bits
Posterior entropy: 1.0
entropy_info: {'prior_entropy': 4.169925001442312, 'posterior_entropy': 1.0, 'actual_info_gain': 3.169925001442312, 'expected_info_gain': 4.681813620578765}
----------
The bot is making a guess...
2 candidate words remaining.
Top 10 guesses: [('SQUAD', '1.000'), ('SQUAW', '1.000')]
Top entropy choice: 𝙎𝙌𝙐𝘼𝘿 with entropy: 1.0000
Guess count high, choosing a candidate with the highest entropy: 𝙎𝙌𝙐𝘼𝘿 with entropy: 1.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟨⬜⬜🟨, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 6.274 bits, Posterior Entropy: 7.585, 
Guess: 𝙎𝙋𝘼𝙄𝙇, Feedback: 🟩⬜🟨⬜⬜, Prior Entropy: 7.585, Expected Info Gain: 4.682 bits, Actual Info Gain: 3.415 bits, Posterior Entropy: 4.170, 
Guess: 𝘽𝙐𝙉𝘾𝙃, Feedback: ⬜🟨⬜⬜⬜, Prior Entropy: 4.170, Expected Info Gain: 4.682 bits, Actual Info Gain: 3.170 bits, Posterior Entropy: 1.000, 
Guess: 𝙎𝙌𝙐𝘼𝘿, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'SQUAD': 6.2136 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝘼𝙍𝙊𝙈𝘼 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟨🟨⬜⬜
Actual Info Gain: 5.6938 bits
Posterior entropy: 8.164906926675688
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 8.164906926675688, 'actual_info_gain': 5.693754058047089, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
287 candidate words remaining.
Top 10 guesses: [('BRAAI', '4.634'), ('BROIL', '4.601'), ('MOANA', '4.596'), ('LOGIN', '4.596'), ('COLIN', '4.581'), ('GROIN', '4.579'), ('GRAAL', '4.570'), ('CRAAL', '4.564'), ('LIANA', '4.563'), ('COALA', '4.560')]
Top entropy choice: 𝘽𝙍𝘼𝘼𝙄 with entropy: 4.6343
Guess count low, choosing the word with highest entropy: 𝘽𝙍𝘼𝘼𝙄 with entropy: 4.6343

Guess: 𝘽𝙍𝘼𝘼𝙄, Feedback: ⬜🟩🟨🟨⬜
Actual Info Gain: 7.1649 bits
Posterior entropy: 1.0
entropy_info: {'prior_entropy': 8.164906926675688, 'posterior_entropy': 1.0, 'actual_info_gain': 7.1649069266756875, 'expected_info_gain': 4.634280696834034}
----------
The bot is making a guess...
2 candidate words remaining.
Top 10 guesses: [('AROHA', '1.000'), ('AROMA', '1.000')]
Top entropy choice: 𝘼𝙍𝙊𝙃𝘼 with entropy: 1.0000
Few candidates left, going through them all to pick a common word...
Using common word: 𝘼𝙍𝙊𝙈𝘼 with entropy: 1.0000
You won! Amount of guesses: 3

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟨🟨⬜⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 5.694 bits, Posterior Entropy: 8.165, 
Guess: 𝘽𝙍𝘼𝘼𝙄, Feedback: ⬜🟩🟨🟨⬜, Prior Entropy: 8.165, Expected Info Gain: 4.634 bits, Actual Info Gain: 7.165 bits, Posterior Entropy: 1.000, 
Guess: 𝘼𝙍𝙊𝙈𝘼, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'AROMA': 8.4793 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙒𝙊𝙑𝙀𝙉 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜🟩⬜
Actual Info Gain: 4.9914 bits
Posterior entropy: 8.867278739709661
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 8.867278739709661, 'actual_info_gain': 4.991382245013115, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
467 candidate words remaining.
Top 10 guesses: [('INDOL', '4.933'), ('DINLO', '4.859'), ('LOWND', '4.836'), ('BLOND', '4.832'), ('LOUND', '4.773'), ('LINDY', '4.749'), ('DOLCI', '4.730'), ('DOILY', '4.707'), ('NOULD', '4.703'), ('MODIN', '4.688')]
Top entropy choice: 𝙄𝙉𝘿𝙊𝙇 with entropy: 4.9332
Guess count low, choosing the word with highest entropy: 𝙄𝙉𝘿𝙊𝙇 with entropy: 4.9332

Guess: 𝙄𝙉𝘿𝙊𝙇, Feedback: ⬜🟨⬜🟨⬜
Actual Info Gain: 4.4750 bits
Posterior entropy: 4.392317422778761
entropy_info: {'prior_entropy': 8.867278739709661, 'posterior_entropy': 4.392317422778761, 'actual_info_gain': 4.474961316930901, 'expected_info_gain': 4.933171607708794}
----------
The bot is making a guess...
21 candidate words remaining.
Top 10 guesses: [('MOCHY', '3.042'), ('MUNCH', '3.010'), ('GONCH', '3.010'), ('CHYME', '2.910'), ('CHEVY', '2.908'), ('CHIVY', '2.908'), ('VICHY', '2.908'), ('BENCH', '2.879'), ('BUNCH', '2.879'), ('WENCH', '2.856')]
Top entropy choice: 𝙈𝙊𝘾𝙃𝙔 with entropy: 3.0416
Using common exploratory word: 𝙈𝙐𝙉𝘾𝙃 with entropy: 3.0104

Guess: 𝙈𝙐𝙉𝘾𝙃, Feedback: ⬜⬜🟨⬜⬜
Actual Info Gain: 2.3923 bits
Posterior entropy: 2.0
entropy_info: {'prior_entropy': 4.392317422778761, 'posterior_entropy': 2.0, 'actual_info_gain': 2.3923174227787607, 'expected_info_gain': 4.933171607708794}
----------
The bot is making a guess...
4 candidate words remaining.
Top 10 guesses: [('VIBEX', '2.000'), ('ABACK', '1.500'), ('ABAKA', '1.500'), ('ABASK', '1.500'), ('ABOVE', '1.500'), ('ADVEW', '1.500'), ('ASKEW', '1.500'), ('AVOWS', '1.500'), ('AWAKE', '1.500'), ('AWAVE', '1.500')]
Top entropy choice: 𝙑𝙄𝘽𝙀𝙓 with entropy: 2.0000
Guess count high, choosing a candidate with the highest entropy: 𝙒𝙊𝙆𝙀𝙉 with entropy: 1.5000

Guess: 𝙒𝙊𝙆𝙀𝙉, Feedback: 🟩🟩⬜🟩🟩
Actual Info Gain: 1.0000 bits
Posterior entropy: 1.0
entropy_info: {'prior_entropy': 2.0, 'posterior_entropy': 1.0, 'actual_info_gain': 1.0, 'expected_info_gain': 1.5}
----------
The bot is making a guess...
2 candidate words remaining.
Top 10 guesses: [('WOVEN', '1.000'), ('WOXEN', '1.000')]
Top entropy choice: 𝙒𝙊𝙑𝙀𝙉 with entropy: 1.0000
Guess count high, choosing a candidate with the highest entropy: 𝙒𝙊𝙑𝙀𝙉 with entropy: 1.0000
You won! Amount of guesses: 5

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜🟩⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 4.991 bits, Posterior Entropy: 8.867, 
Guess: 𝙄𝙉𝘿𝙊𝙇, Feedback: ⬜🟨⬜🟨⬜, Prior Entropy: 8.867, Expected Info Gain: 4.933 bits, Actual Info Gain: 4.475 bits, Posterior Entropy: 4.392, 
Guess: 𝙈𝙐𝙉𝘾𝙃, Feedback: ⬜⬜🟨⬜⬜, Prior Entropy: 4.392, Expected Info Gain: 4.933 bits, Actual Info Gain: 2.392 bits, Posterior Entropy: 2.000, 
Guess: 𝙒𝙊𝙆𝙀𝙉, Feedback: 🟩🟩⬜🟩🟩, Prior Entropy: 2.000, Expected Info Gain: 1.500 bits, Actual Info Gain: 1.000 bits, Posterior Entropy: 1.000, 
Guess: 𝙒𝙊𝙑𝙀𝙉, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'WOVEN': 15.3400 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙈𝘼𝙏𝙏𝙔 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟨🟩⬜⬜⬜
Actual Info Gain: 6.8252 bits
Posterior entropy: 7.03342300153745
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 7.03342300153745, 'actual_info_gain': 6.8252379831853265, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
131 candidate words remaining.
Top 10 guesses: [('NITTY', '4.165'), ('NICHT', '4.110'), ('NITTO', '4.080'), ('NITTA', '4.074'), ('NOILY', '4.066'), ('CLINT', '4.064'), ('CUNIT', '3.984'), ('CUTIN', '3.975'), ('CHINO', '3.973'), ('LINTY', '3.971')]
Top entropy choice: 𝙉𝙄𝙏𝙏𝙔 with entropy: 4.1645
Guess count low, choosing the word with highest entropy: 𝙉𝙄𝙏𝙏𝙔 with entropy: 4.1645

Guess: 𝙉𝙄𝙏𝙏𝙔, Feedback: ⬜⬜🟩🟩🟩
Actual Info Gain: 4.4485 bits
Posterior entropy: 2.584962500721156
entropy_info: {'prior_entropy': 7.03342300153745, 'posterior_entropy': 2.584962500721156, 'actual_info_gain': 4.448460500816294, 'expected_info_gain': 4.164513109156485}
----------
The bot is making a guess...
6 candidate words remaining.
Top 10 guesses: [('BUMPH', '2.252'), ('CHAMP', '2.252'), ('CHIMB', '2.252'), ('CHIMP', '2.252'), ('CHOMP', '2.252'), ('CHUMP', '2.252'), ('ABAMP', '1.792'), ('ABMHO', '1.792'), ('ABOHM', '1.792'), ('BACHA', '1.792')]
Top entropy choice: 𝘽𝙐𝙈𝙋𝙃 with entropy: 2.2516
Using common exploratory word: 𝘾𝙃𝘼𝙈𝙋 with entropy: 2.2516

Guess: 𝘾𝙃𝘼𝙈𝙋, Feedback: ⬜⬜🟨🟨⬜
Actual Info Gain: 2.5850 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 2.584962500721156, 'posterior_entropy': 0.0, 'actual_info_gain': 2.584962500721156, 'expected_info_gain': 4.164513109156485}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('MATTY', '0.000')]
Top entropy choice: 𝙈𝘼𝙏𝙏𝙔 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝙈𝘼𝙏𝙏𝙔 with entropy: 0.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟨🟩⬜⬜⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 6.825 bits, Posterior Entropy: 7.033, 
Guess: 𝙉𝙄𝙏𝙏𝙔, Feedback: ⬜⬜🟩🟩🟩, Prior Entropy: 7.033, Expected Info Gain: 4.165 bits, Actual Info Gain: 4.448 bits, Posterior Entropy: 2.585, 
Guess: 𝘾𝙃𝘼𝙈𝙋, Feedback: ⬜⬜🟨🟨⬜, Prior Entropy: 2.585, Expected Info Gain: 4.165 bits, Actual Info Gain: 2.585 bits, Posterior Entropy: 0.000, 
Guess: 𝙈𝘼𝙏𝙏𝙔, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'MATTY': 4.0644 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙎𝙋𝙊𝙍𝙀 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜🟨🟨🟨
Actual Info Gain: 8.1307 bits
Posterior entropy: 5.727920454563199
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 5.727920454563199, 'actual_info_gain': 8.130740530159578, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
53 candidate words remaining.
Top 10 guesses: [('POIRE', '4.122'), ('PEISE', '4.002'), ('POISE', '3.981'), ('POWIE', '3.865'), ('SHOPE', '3.864'), ('POWRE', '3.842'), ('SPOIL', '3.827'), ('HOISE', '3.820'), ('SWIPE', '3.813'), ('SHISO', '3.812')]
Top entropy choice: 𝙋𝙊𝙄𝙍𝙀 with entropy: 4.1224
Guess count low, choosing the word with highest entropy: 𝙋𝙊𝙄𝙍𝙀 with entropy: 4.1224

Guess: 𝙋𝙊𝙄𝙍𝙀, Feedback: 🟨🟨⬜🟩🟩
Actual Info Gain: 5.7279 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 5.727920454563199, 'posterior_entropy': 0.0, 'actual_info_gain': 5.727920454563199, 'expected_info_gain': 4.122381135132994}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('SPORE', '0.000')]
Top entropy choice: 𝙎𝙋𝙊𝙍𝙀 with entropy: 0.0000
Few candidates left, going through them all to pick a common word...
Using common word: 𝙎𝙋𝙊𝙍𝙀 with entropy: 0.0000
You won! Amount of guesses: 3

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜🟨🟨🟨, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 8.131 bits, Posterior Entropy: 5.728, 
Guess: 𝙋𝙊𝙄𝙍𝙀, Feedback: 🟨🟨⬜🟩🟩, Prior Entropy: 5.728, Expected Info Gain: 4.122 bits, Actual Info Gain: 5.728 bits, Posterior Entropy: 0.000, 
Guess: 𝙎𝙋𝙊𝙍𝙀, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'SPORE': 1.6364 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝘽𝙍𝙐𝙏𝙀 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟨⬜🟨🟨⬜
Actual Info Gain: 7.8363 bits
Posterior entropy: 6.022367813028454
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 6.022367813028454, 'actual_info_gain': 7.836293171694322, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
65 candidate words remaining.
Top 10 guesses: [('RIONE', '4.234'), ('RUTIN', '4.195'), ('RUICE', '4.172'), ('OUTIE', '4.156'), ('UNTIE', '4.147'), ('PEINE', '4.142'), ('RINCE', '4.134'), ('CUTIE', '4.132'), ('RETIE', '4.128'), ('NIECE', '4.125')]
Top entropy choice: 𝙍𝙄𝙊𝙉𝙀 with entropy: 4.2338
Guess count low, choosing the word with highest entropy: 𝙍𝙄𝙊𝙉𝙀 with entropy: 4.2338

Guess: 𝙍𝙄𝙊𝙉𝙀, Feedback: 🟨⬜⬜⬜🟩
Actual Info Gain: 4.4374 bits
Posterior entropy: 1.584962500721156
entropy_info: {'prior_entropy': 6.022367813028454, 'posterior_entropy': 1.584962500721156, 'actual_info_gain': 4.437405312307298, 'expected_info_gain': 4.233831760897209}
----------
The bot is making a guess...
3 candidate words remaining.
Top 10 guesses: [('METRE', '1.585'), ('PETRE', '1.585'), ('BRUTE', '0.918')]
Top entropy choice: 𝙈𝙀𝙏𝙍𝙀 with entropy: 1.5850
Few candidates left, going through them all to pick a common word...
Using common word: 𝙈𝙀𝙏𝙍𝙀 with entropy: 1.5850

Guess: 𝙈𝙀𝙏𝙍𝙀, Feedback: ⬜⬜🟨🟨🟩
Actual Info Gain: 1.5850 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 1.584962500721156, 'posterior_entropy': 0.0, 'actual_info_gain': 1.584962500721156, 'expected_info_gain': 4.233831760897209}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('BRUTE', '0.000')]
Top entropy choice: 𝘽𝙍𝙐𝙏𝙀 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝘽𝙍𝙐𝙏𝙀 with entropy: 0.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟨⬜🟨🟨⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 7.836 bits, Posterior Entropy: 6.022, 
Guess: 𝙍𝙄𝙊𝙉𝙀, Feedback: 🟨⬜⬜⬜🟩, Prior Entropy: 6.022, Expected Info Gain: 4.234 bits, Actual Info Gain: 4.437 bits, Posterior Entropy: 1.585, 
Guess: 𝙈𝙀𝙏𝙍𝙀, Feedback: ⬜⬜🟨🟨🟩, Prior Entropy: 1.585, Expected Info Gain: 4.234 bits, Actual Info Gain: 1.585 bits, Posterior Entropy: 0.000, 
Guess: 𝘽𝙍𝙐𝙏𝙀, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'BRUTE': 1.8413 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙃𝙀𝙍𝙏𝙕 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟨⬜🟩🟨⬜
Actual Info Gain: 10.3992 bits
Posterior entropy: 3.4594316186372973
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 3.4594316186372973, 'actual_info_gain': 10.399229366085478, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
11 candidate words remaining.
Top 10 guesses: [('COUTH', '2.914'), ('HENCE', '2.914'), ('MOUCH', '2.914'), ('VOUCH', '2.914'), ('VOZHD', '2.914'), ('YOUTH', '2.914'), ('FOUTH', '2.845'), ('HEFTE', '2.845'), ('HEFTY', '2.845'), ('POUCH', '2.845')]
Top entropy choice: 𝘾𝙊𝙐𝙏𝙃 with entropy: 2.9140
Guess count low, choosing the word with highest entropy: 𝘾𝙊𝙐𝙏𝙃 with entropy: 2.9140

Guess: 𝘾𝙊𝙐𝙏𝙃, Feedback: ⬜⬜⬜🟩🟨
Actual Info Gain: 3.4594 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 3.4594316186372973, 'posterior_entropy': 0.0, 'actual_info_gain': 3.4594316186372973, 'expected_info_gain': 2.9139770731827523}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('HERTZ', '0.000')]
Top entropy choice: 𝙃𝙀𝙍𝙏𝙕 with entropy: 0.0000
Few candidates left, going through them all to pick a common word...
Using common word: 𝙃𝙀𝙍𝙏𝙕 with entropy: 0.0000
You won! Amount of guesses: 3

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟨⬜🟩🟨⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 10.399 bits, Posterior Entropy: 3.459, 
Guess: 𝘾𝙊𝙐𝙏𝙃, Feedback: ⬜⬜⬜🟩🟨, Prior Entropy: 3.459, Expected Info Gain: 2.914 bits, Actual Info Gain: 3.459 bits, Posterior Entropy: 0.000, 
Guess: 𝙃𝙀𝙍𝙏𝙕, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'HERTZ': 0.3188 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙄𝙏𝘾𝙃𝙔 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟨⬜⬜⬜⬜
Actual Info Gain: 5.5779 bits
Posterior entropy: 8.280770770130603
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 8.280770770130603, 'actual_info_gain': 5.577890214592173, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
311 candidate words remaining.
Top 10 guesses: [('COLIN', '5.317'), ('CUNIT', '5.247'), ('PIONY', '5.209'), ('COUNT', '5.200'), ('NICOL', '5.186'), ('NITTO', '5.181'), ('PITOT', '5.173'), ('LITHO', '5.162'), ('PILOT', '5.155'), ('POTIN', '5.150')]
Top entropy choice: 𝘾𝙊𝙇𝙄𝙉 with entropy: 5.3169
Guess count low, choosing the word with highest entropy: 𝘾𝙊𝙇𝙄𝙉 with entropy: 5.3169

Guess: 𝘾𝙊𝙇𝙄𝙉, Feedback: 🟨⬜⬜🟨⬜
Actual Info Gain: 4.4734 bits
Posterior entropy: 3.807354922057604
entropy_info: {'prior_entropy': 8.280770770130603, 'posterior_entropy': 3.807354922057604, 'actual_info_gain': 4.4734158480729995, 'expected_info_gain': 5.316896712755155}
----------
The bot is making a guess...
14 candidate words remaining.
Top 10 guesses: [('BUMPH', '2.835'), ('HUMPH', '2.835'), ('DUMPY', '2.692'), ('DIGHT', '2.557'), ('FUDGY', '2.557'), ('LYMPH', '2.557'), ('MIGHT', '2.557'), ('MUCID', '2.557'), ('MUGHO', '2.557'), ('MUMPH', '2.557')]
Top entropy choice: 𝘽𝙐𝙈𝙋𝙃 with entropy: 2.8352
Using common exploratory word: 𝙇𝙔𝙈𝙋𝙃 with entropy: 2.5567

Guess: 𝙇𝙔𝙈𝙋𝙃, Feedback: ⬜🟨⬜⬜🟨
Actual Info Gain: 3.8074 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 3.807354922057604, 'posterior_entropy': 0.0, 'actual_info_gain': 3.807354922057604, 'expected_info_gain': 5.316896712755155}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('ITCHY', '0.000')]
Top entropy choice: 𝙄𝙏𝘾𝙃𝙔 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝙄𝙏𝘾𝙃𝙔 with entropy: 0.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟨⬜⬜⬜⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 5.578 bits, Posterior Entropy: 8.281, 
Guess: 𝘾𝙊𝙇𝙄𝙉, Feedback: 🟨⬜⬜🟨⬜, Prior Entropy: 8.281, Expected Info Gain: 5.317 bits, Actual Info Gain: 4.473 bits, Posterior Entropy: 3.807, 
Guess: 𝙇𝙔𝙈𝙋𝙃, Feedback: ⬜🟨⬜⬜🟨, Prior Entropy: 3.807, Expected Info Gain: 5.317 bits, Actual Info Gain: 3.807 bits, Posterior Entropy: 0.000, 
Guess: 𝙄𝙏𝘾𝙃𝙔, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'ITCHY': 9.9034 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙑𝙄𝙂𝙊𝙍 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜🟨⬜⬜
Actual Info Gain: 5.5277 bits
Posterior entropy: 8.330916878114618
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 8.330916878114618, 'actual_info_gain': 5.527744106608159, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
322 candidate words remaining.
Top 10 guesses: [('BOUND', '5.238'), ('POUND', '5.185'), ('LOUND', '5.137'), ('POIND', '5.117'), ('PIONY', '5.107'), ('PRION', '5.099'), ('MOUND', '5.083'), ('COULD', '5.079'), ('BIDON', '5.077'), ('COIGN', '5.062')]
Top entropy choice: 𝘽𝙊𝙐𝙉𝘿 with entropy: 5.2375
Guess count low, choosing the word with highest entropy: 𝘽𝙊𝙐𝙉𝘿 with entropy: 5.2375

Guess: 𝘽𝙊𝙐𝙉𝘿, Feedback: ⬜🟨⬜⬜⬜
Actual Info Gain: 2.7162 bits
Posterior entropy: 5.614709844115208
entropy_info: {'prior_entropy': 8.330916878114618, 'posterior_entropy': 5.614709844115208, 'actual_info_gain': 2.7162070339994093, 'expected_info_gain': 5.237543649847127}
----------
The bot is making a guess...
49 candidate words remaining.
Top 10 guesses: [('CROOL', '4.446'), ('CLOOP', '4.395'), ('GLOOM', '4.374'), ('GLOOP', '4.333'), ('PIGLY', '4.293'), ('PLOOK', '4.245'), ('OLIGO', '4.243'), ('FILMY', '4.232'), ('CRIMP', '4.201'), ('GILPY', '4.196')]
Top entropy choice: 𝘾𝙍𝙊𝙊𝙇 with entropy: 4.4456
Using common exploratory word: 𝙂𝙇𝙊𝙊𝙈 with entropy: 4.3740

Guess: 𝙂𝙇𝙊𝙊𝙈, Feedback: 🟨⬜⬜🟩⬜
Actual Info Gain: 4.6147 bits
Posterior entropy: 1.0
entropy_info: {'prior_entropy': 5.614709844115208, 'posterior_entropy': 1.0, 'actual_info_gain': 4.614709844115208, 'expected_info_gain': 5.237543649847127}
----------
The bot is making a guess...
2 candidate words remaining.
Top 10 guesses: [('RIGOR', '1.000'), ('VIGOR', '1.000')]
Top entropy choice: 𝙍𝙄𝙂𝙊𝙍 with entropy: 1.0000
Guess count high, choosing a candidate with the highest entropy: 𝙍𝙄𝙂𝙊𝙍 with entropy: 1.0000

Guess: 𝙍𝙄𝙂𝙊𝙍, Feedback: ⬜🟩🟩🟩🟩
Actual Info Gain: 1.0000 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 1.0, 'posterior_entropy': 0.0, 'actual_info_gain': 1.0, 'expected_info_gain': 1.0}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('VIGOR', '0.000')]
Top entropy choice: 𝙑𝙄𝙂𝙊𝙍 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝙑𝙄𝙂𝙊𝙍 with entropy: 0.0000
You won! Amount of guesses: 5

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜🟨⬜⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 5.528 bits, Posterior Entropy: 8.331, 
Guess: 𝘽𝙊𝙐𝙉𝘿, Feedback: ⬜🟨⬜⬜⬜, Prior Entropy: 8.331, Expected Info Gain: 5.238 bits, Actual Info Gain: 2.716 bits, Posterior Entropy: 5.615, 
Guess: 𝙂𝙇𝙊𝙊𝙈, Feedback: 🟨⬜⬜🟩⬜, Prior Entropy: 5.615, Expected Info Gain: 5.238 bits, Actual Info Gain: 4.615 bits, Posterior Entropy: 1.000, 
Guess: 𝙍𝙄𝙂𝙊𝙍, Feedback: ⬜🟩🟩🟩🟩, Prior Entropy: 1.000, Expected Info Gain: 1.000 bits, Actual Info Gain: 1.000 bits, Posterior Entropy: 0.000, 
Guess: 𝙑𝙄𝙂𝙊𝙍, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'VIGOR': 11.1293 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝘾𝙇𝙊𝘾𝙆 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜⬜⬜
Actual Info Gain: 3.8615 bits
Posterior entropy: 9.997179480937621
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 9.997179480937621, 'actual_info_gain': 3.8614815037851553, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
1022 candidate words remaining.
Top 10 guesses: [('COLIN', '5.938'), ('NOILY', '5.864'), ('DINLO', '5.841'), ('PIONY', '5.791'), ('LINGO', '5.749'), ('PILON', '5.742'), ('PINOL', '5.704'), ('LOGIN', '5.699'), ('DOILY', '5.681'), ('NICOL', '5.680')]
Top entropy choice: 𝘾𝙊𝙇𝙄𝙉 with entropy: 5.9379
Guess count low, choosing the word with highest entropy: 𝘾𝙊𝙇𝙄𝙉 with entropy: 5.9379

Guess: 𝘾𝙊𝙇𝙄𝙉, Feedback: 🟩🟨🟨⬜⬜
Actual Info Gain: 6.8273 bits
Posterior entropy: 3.169925001442312
entropy_info: {'prior_entropy': 9.997179480937621, 'posterior_entropy': 3.169925001442312, 'actual_info_gain': 6.827254479495309, 'expected_info_gain': 5.937896650379372}
----------
The bot is making a guess...
9 candidate words remaining.
Top 10 guesses: [('FLUMP', '2.948'), ('HUMPY', '2.948'), ('PLUMY', '2.948'), ('CLUMP', '2.725'), ('DSOMO', '2.725'), ('DUOMO', '2.725'), ('FLIMP', '2.725'), ('FROOM', '2.725'), ('KHOUM', '2.725'), ('MOOCH', '2.725')]
Top entropy choice: 𝙁𝙇𝙐𝙈𝙋 with entropy: 2.9477
Using common exploratory word: 𝘾𝙇𝙐𝙈𝙋 with entropy: 2.7255

Guess: 𝘾𝙇𝙐𝙈𝙋, Feedback: 🟩🟩⬜⬜⬜
Actual Info Gain: 2.1699 bits
Posterior entropy: 1.0
entropy_info: {'prior_entropy': 3.169925001442312, 'posterior_entropy': 1.0, 'actual_info_gain': 2.169925001442312, 'expected_info_gain': 5.937896650379372}
----------
The bot is making a guess...
2 candidate words remaining.
Top 10 guesses: [('CLOCK', '1.000'), ('CLOFF', '1.000')]
Top entropy choice: 𝘾𝙇𝙊𝘾𝙆 with entropy: 1.0000
Guess count high, choosing a candidate with the highest entropy: 𝘾𝙇𝙊𝘾𝙆 with entropy: 1.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜⬜⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 3.861 bits, Posterior Entropy: 9.997, 
Guess: 𝘾𝙊𝙇𝙄𝙉, Feedback: 🟩🟨🟨⬜⬜, Prior Entropy: 9.997, Expected Info Gain: 5.938 bits, Actual Info Gain: 6.827 bits, Posterior Entropy: 3.170, 
Guess: 𝘾𝙇𝙐𝙈𝙋, Feedback: 🟩🟩⬜⬜⬜, Prior Entropy: 3.170, Expected Info Gain: 5.938 bits, Actual Info Gain: 2.170 bits, Posterior Entropy: 1.000, 
Guess: 𝘾𝙇𝙊𝘾𝙆, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'CLOCK': 31.2042 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙎𝙃𝘼𝙆𝘼 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟨⬜⬜🟨
Actual Info Gain: 6.2737 bits
Posterior entropy: 7.584962500721156
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 7.584962500721156, 'actual_info_gain': 6.273698484001621, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
192 candidate words remaining.
Top 10 guesses: [('SPAIL', '4.682'), ('PLANH', '4.659'), ('SHALM', '4.648'), ('PILCH', '4.602'), ('PLAIN', '4.596'), ('SNAIL', '4.587'), ('MILCH', '4.581'), ('CHOIL', '4.568'), ('SCAIL', '4.568'), ('SHULN', '4.548')]
Top entropy choice: 𝙎𝙋𝘼𝙄𝙇 with entropy: 4.6818
Guess count low, choosing the word with highest entropy: 𝙎𝙋𝘼𝙄𝙇 with entropy: 4.6818

Guess: 𝙎𝙋𝘼𝙄𝙇, Feedback: 🟩⬜🟩⬜⬜
Actual Info Gain: 2.7270 bits
Posterior entropy: 4.857980995127572
entropy_info: {'prior_entropy': 7.584962500721156, 'posterior_entropy': 4.857980995127572, 'actual_info_gain': 2.726981505593584, 'expected_info_gain': 4.681813620578765}
----------
The bot is making a guess...
29 candidate words remaining.
Top 10 guesses: [('CHUNK', '4.087'), ('CHYND', '3.992'), ('CHANK', '3.949'), ('CHINK', '3.949'), ('KHANA', '3.883'), ('THUNK', '3.854'), ('WHINY', '3.837'), ('HUNKY', '3.828'), ('CHOWK', '3.811'), ('SHONK', '3.785')]
Top entropy choice: 𝘾𝙃𝙐𝙉𝙆 with entropy: 4.0868
Using common exploratory word: 𝘾𝙃𝙐𝙉𝙆 with entropy: 4.0868

Guess: 𝘾𝙃𝙐𝙉𝙆, Feedback: ⬜🟩⬜⬜🟨
Actual Info Gain: 3.2730 bits
Posterior entropy: 1.584962500721156
entropy_info: {'prior_entropy': 4.857980995127572, 'posterior_entropy': 1.584962500721156, 'actual_info_gain': 3.273018494406416, 'expected_info_gain': 4.681813620578765}
----------
The bot is making a guess...
3 candidate words remaining.
Top 10 guesses: [('SHAKA', '0.918'), ('SHAKO', '0.918'), ('SHAKY', '0.918')]
Top entropy choice: 𝙎𝙃𝘼𝙆𝘼 with entropy: 0.9183
Guess count high, choosing a candidate with the highest entropy: 𝙎𝙃𝘼𝙆𝘼 with entropy: 0.9183
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟨⬜⬜🟨, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 6.274 bits, Posterior Entropy: 7.585, 
Guess: 𝙎𝙋𝘼𝙄𝙇, Feedback: 🟩⬜🟩⬜⬜, Prior Entropy: 7.585, Expected Info Gain: 4.682 bits, Actual Info Gain: 2.727 bits, Posterior Entropy: 4.858, 
Guess: 𝘾𝙃𝙐𝙉𝙆, Feedback: ⬜🟩⬜⬜🟨, Prior Entropy: 4.858, Expected Info Gain: 4.682 bits, Actual Info Gain: 3.273 bits, Posterior Entropy: 1.585, 
Guess: 𝙎𝙃𝘼𝙆𝘼, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'SHAKA': 6.4765 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙅𝙊𝙇𝙇𝙔 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜⬜⬜
Actual Info Gain: 3.8615 bits
Posterior entropy: 9.997179480937621
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 9.997179480937621, 'actual_info_gain': 3.8614815037851553, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
1022 candidate words remaining.
Top 10 guesses: [('COLIN', '5.938'), ('NOILY', '5.864'), ('DINLO', '5.841'), ('PIONY', '5.791'), ('LINGO', '5.749'), ('PILON', '5.742'), ('PINOL', '5.704'), ('LOGIN', '5.699'), ('DOILY', '5.681'), ('NICOL', '5.680')]
Top entropy choice: 𝘾𝙊𝙇𝙄𝙉 with entropy: 5.9379
Guess count low, choosing the word with highest entropy: 𝘾𝙊𝙇𝙄𝙉 with entropy: 5.9379

Guess: 𝘾𝙊𝙇𝙄𝙉, Feedback: ⬜🟩🟩⬜⬜
Actual Info Gain: 5.6049 bits
Posterior entropy: 4.392317422778761
entropy_info: {'prior_entropy': 9.997179480937621, 'posterior_entropy': 4.392317422778761, 'actual_info_gain': 5.604862058158861, 'expected_info_gain': 5.937896650379372}
----------
The bot is making a guess...
21 candidate words remaining.
Top 10 guesses: [('DIMLY', '2.980'), ('MADLY', '2.980'), ('AMPLY', '2.908'), ('IMPLY', '2.908'), ('MUHLY', '2.908'), ('DUPLY', '2.885'), ('PYGMY', '2.885'), ('LYMPH', '2.869'), ('DAMPY', '2.869'), ('DUMPY', '2.869')]
Top entropy choice: 𝘿𝙄𝙈𝙇𝙔 with entropy: 2.9803
Using common exploratory word: 𝘿𝙄𝙈𝙇𝙔 with entropy: 2.9803

Guess: 𝘿𝙄𝙈𝙇𝙔, Feedback: ⬜⬜⬜🟩🟩
Actual Info Gain: 1.5850 bits
Posterior entropy: 2.807354922057604
entropy_info: {'prior_entropy': 4.392317422778761, 'posterior_entropy': 2.807354922057604, 'actual_info_gain': 1.5849625007211565, 'expected_info_gain': 5.937896650379372}
----------
The bot is making a guess...
7 candidate words remaining.
Top 10 guesses: [('FOWTH', '1.664'), ('GAWPS', '1.664'), ('GLYPH', '1.664'), ('GOWFS', '1.664'), ('GRAPH', '1.664'), ('GULPH', '1.664'), ('HEWGH', '1.664'), ('HOWFF', '1.664'), ('HOWFS', '1.664'), ('LAIGH', '1.664')]
Top entropy choice: 𝙁𝙊𝙒𝙏𝙃 with entropy: 1.6645
Guess count high, choosing a candidate with the highest entropy: 𝙁𝙊𝙇𝙇𝙔 with entropy: 0.5917

Guess: 𝙁𝙊𝙇𝙇𝙔, Feedback: ⬜🟩🟩🟩🟩
Actual Info Gain: 0.2224 bits
Posterior entropy: 2.584962500721156
entropy_info: {'prior_entropy': 2.807354922057604, 'posterior_entropy': 2.584962500721156, 'actual_info_gain': 0.2223924213364481, 'expected_info_gain': 0.5916727785823275}
----------
The bot is making a guess...
6 candidate words remaining.
Top 10 guesses: [('GAWPS', '1.792'), ('GLYPH', '1.792'), ('GRAPH', '1.792'), ('GULPH', '1.792'), ('HEWGH', '1.792'), ('LAIGH', '1.792'), ('LAPJE', '1.792'), ('LAUGH', '1.792'), ('LEUGH', '1.792'), ('LIGHT', '1.792')]
Top entropy choice: 𝙂𝘼𝙒𝙋𝙎 with entropy: 1.7925
Guess count high, choosing a candidate with the highest entropy: 𝙂𝙊𝙇𝙇𝙔 with entropy: 0.6500

Guess: 𝙂𝙊𝙇𝙇𝙔, Feedback: ⬜🟩🟩🟩🟩
Actual Info Gain: 0.2630 bits
Posterior entropy: 2.321928094887362
entropy_info: {'prior_entropy': 2.584962500721156, 'posterior_entropy': 2.321928094887362, 'actual_info_gain': 0.2630344058337939, 'expected_info_gain': 0.6500224216483541}
----------
The bot is making a guess...
5 candidate words remaining.
Top 10 guesses: [('LAPJE', '1.922'), ('LOWPS', '1.922'), ('LOWTH', '1.922'), ('LYMPH', '1.922'), ('PHWAT', '1.922'), ('PSHAW', '1.922'), ('PUJAH', '1.922'), ('WHAPS', '1.922'), ('WHAUP', '1.922'), ('WHEEP', '1.922')]
Top entropy choice: 𝙇𝘼𝙋𝙅𝙀 with entropy: 1.9219
Guess count high, choosing a candidate with the highest entropy: 𝙃𝙊𝙇𝙇𝙔 with entropy: 0.7219
Game Over! You ran out of guesses.

Guess: 𝙃𝙊𝙇𝙇𝙔, Feedback: ⬜🟩🟩🟩🟩
Actual Info Gain: 0.3219 bits
Posterior entropy: 2.0
entropy_info: {'prior_entropy': 2.321928094887362, 'posterior_entropy': 2.0, 'actual_info_gain': 0.3219280948873622, 'expected_info_gain': 0.7219280948873623}
----------

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜⬜⬜⬜⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 3.861 bits, Posterior Entropy: 9.997, 
Guess: 𝘾𝙊𝙇𝙄𝙉, Feedback: ⬜🟩🟩⬜⬜, Prior Entropy: 9.997, Expected Info Gain: 5.938 bits, Actual Info Gain: 5.605 bits, Posterior Entropy: 4.392, 
Guess: 𝘿𝙄𝙈𝙇𝙔, Feedback: ⬜⬜⬜🟩🟩, Prior Entropy: 4.392, Expected Info Gain: 5.938 bits, Actual Info Gain: 1.585 bits, Posterior Entropy: 2.807, 
Guess: 𝙁𝙊𝙇𝙇𝙔, Feedback: ⬜🟩🟩🟩🟩, Prior Entropy: 2.807, Expected Info Gain: 0.592 bits, Actual Info Gain: 0.222 bits, Posterior Entropy: 2.585, 
Guess: 𝙂𝙊𝙇𝙇𝙔, Feedback: ⬜🟩🟩🟩🟩, Prior Entropy: 2.585, Expected Info Gain: 0.650 bits, Actual Info Gain: 0.263 bits, Posterior Entropy: 2.322, 
Guess: 𝙃𝙊𝙇𝙇𝙔, Feedback: ⬜🟩🟩🟩🟩
===================================

Time taken for 'JOLLY': 31.9512 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙎𝘼𝙂𝘼𝙍 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟩🟨⬜🟨
Actual Info Gain: 10.0513 bits
Posterior entropy: 3.807354922057604
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 3.807354922057604, 'actual_info_gain': 10.051306062665173, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
14 candidate words remaining.
Top 10 guesses: [('MOPSY', '3.236'), ('ROMPU', '3.236'), ('ROMPY', '3.236'), ('RUMPO', '3.236'), ('RUPIA', '3.236'), ('MAPAU', '3.182'), ('PASAR', '3.182'), ('SIMAR', '3.182'), ('SYMAR', '3.182'), ('YAMPA', '3.182')]
Top entropy choice: 𝙈𝙊𝙋𝙎𝙔 with entropy: 3.2359
Guess count low, choosing the word with highest entropy: 𝙈𝙊𝙋𝙎𝙔 with entropy: 3.2359

Guess: 𝙈𝙊𝙋𝙎𝙔, Feedback: ⬜⬜⬜🟨⬜
Actual Info Gain: 1.8074 bits
Posterior entropy: 2.0
entropy_info: {'prior_entropy': 3.807354922057604, 'posterior_entropy': 2.0, 'actual_info_gain': 1.8073549220576042, 'expected_info_gain': 3.2359263506290334}
----------
The bot is making a guess...
4 candidate words remaining.
Top 10 guesses: [('ABACA', '2.000'), ('ABACI', '2.000'), ('ABACK', '2.000'), ('ABACS', '2.000'), ('ABAKA', '2.000'), ('ABAYA', '2.000'), ('ABBAS', '2.000'), ('ABEAM', '2.000'), ('ABEAR', '2.000'), ('ABEAT', '2.000')]
Top entropy choice: 𝘼𝘽𝘼𝘾𝘼 with entropy: 2.0000
Using common exploratory word: 𝘼𝘽𝘼𝘾𝙆 with entropy: 2.0000

Guess: 𝘼𝘽𝘼𝘾𝙆, Feedback: 🟨⬜🟨⬜⬜
Actual Info Gain: 2.0000 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 2.0, 'posterior_entropy': 0.0, 'actual_info_gain': 2.0, 'expected_info_gain': 3.2359263506290334}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('SAGAR', '0.000')]
Top entropy choice: 𝙎𝘼𝙂𝘼𝙍 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝙎𝘼𝙂𝘼𝙍 with entropy: 0.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟩🟨⬜🟨, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 10.051 bits, Posterior Entropy: 3.807, 
Guess: 𝙈𝙊𝙋𝙎𝙔, Feedback: ⬜⬜⬜🟨⬜, Prior Entropy: 3.807, Expected Info Gain: 3.236 bits, Actual Info Gain: 1.807 bits, Posterior Entropy: 2.000, 
Guess: 𝘼𝘽𝘼𝘾𝙆, Feedback: 🟨⬜🟨⬜⬜, Prior Entropy: 2.000, Expected Info Gain: 3.236 bits, Actual Info Gain: 2.000 bits, Posterior Entropy: 0.000, 
Guess: 𝙎𝘼𝙂𝘼𝙍, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'SAGAR': 0.5807 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙏𝘼𝙄𝙂𝘼 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟩🟩⬜⬜⬜
Actual Info Gain: 7.8814 bits
Posterior entropy: 5.977279923499917
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 5.977279923499917, 'actual_info_gain': 7.88138106122286, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
63 candidate words remaining.
Top 10 guesses: [('KYLIN', '4.106'), ('LINKY', '3.945'), ('LANAI', '3.896'), ('LIANA', '3.856'), ('COLIN', '3.852'), ('YONIC', '3.810'), ('CONIA', '3.793'), ('LYING', '3.787'), ('INULA', '3.776'), ('CLINK', '3.763')]
Top entropy choice: 𝙆𝙔𝙇𝙄𝙉 with entropy: 4.1056
Guess count low, choosing the word with highest entropy: 𝙆𝙔𝙇𝙄𝙉 with entropy: 4.1056

Guess: 𝙆𝙔𝙇𝙄𝙉, Feedback: ⬜⬜⬜🟨⬜
Actual Info Gain: 4.3923 bits
Posterior entropy: 1.584962500721156
entropy_info: {'prior_entropy': 5.977279923499917, 'posterior_entropy': 1.584962500721156, 'actual_info_gain': 4.392317422778761, 'expected_info_gain': 4.105638606067614}
----------
The bot is making a guess...
3 candidate words remaining.
Top 10 guesses: [('TACHI', '1.585'), ('TAIGA', '1.585'), ('TAWAI', '1.585')]
Top entropy choice: 𝙏𝘼𝘾𝙃𝙄 with entropy: 1.5850
Few candidates left, going through them all to pick a common word...
Using common word: 𝙏𝘼𝙄𝙂𝘼 with entropy: 1.5850
You won! Amount of guesses: 3

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟩🟩⬜⬜⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 7.881 bits, Posterior Entropy: 5.977, 
Guess: 𝙆𝙔𝙇𝙄𝙉, Feedback: ⬜⬜⬜🟨⬜, Prior Entropy: 5.977, Expected Info Gain: 4.106 bits, Actual Info Gain: 4.392 bits, Posterior Entropy: 1.585, 
Guess: 𝙏𝘼𝙄𝙂𝘼, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'TAIGA': 1.9168 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙎𝙏𝙀𝘼𝙈 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟨🟨⬜🟨🟨
Actual Info Gain: 8.7712 bits
Posterior entropy: 5.087462841250339
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 5.087462841250339, 'actual_info_gain': 8.771198143472438, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
34 candidate words remaining.
Top 10 guesses: [('PLATT', '3.540'), ('STALK', '3.478'), ('SPALT', '3.465'), ('PLEAT', '3.463'), ('PLATE', '3.440'), ('KLETT', '3.436'), ('SPALD', '3.418'), ('SPELT', '3.414'), ('STALE', '3.410'), ('STEAL', '3.410')]
Top entropy choice: 𝙋𝙇𝘼𝙏𝙏 with entropy: 3.5395
Guess count low, choosing the word with highest entropy: 𝙋𝙇𝘼𝙏𝙏 with entropy: 3.5395

Guess: 𝙋𝙇𝘼𝙏𝙏, Feedback: ⬜⬜🟨🟨⬜
Actual Info Gain: 2.5025 bits
Posterior entropy: 2.584962500721156
entropy_info: {'prior_entropy': 5.087462841250339, 'posterior_entropy': 2.584962500721156, 'actual_info_gain': 2.502500340529183, 'expected_info_gain': 3.5395454741006125}
----------
The bot is making a guess...
6 candidate words remaining.
Top 10 guesses: [('DEMON', '2.585'), ('EMEND', '2.585'), ('KENDO', '2.585'), ('KNEED', '2.585'), ('MONDE', '2.585'), ('DECKO', '2.252'), ('DEINK', '2.252'), ('DEKKO', '2.252'), ('DOEKS', '2.252'), ('DROKE', '2.252')]
Top entropy choice: 𝘿𝙀𝙈𝙊𝙉 with entropy: 2.5850
Using common exploratory word: 𝘿𝙀𝙈𝙊𝙉 with entropy: 2.5850

Guess: 𝘿𝙀𝙈𝙊𝙉, Feedback: ⬜🟨🟨⬜⬜
Actual Info Gain: 2.5850 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 2.584962500721156, 'posterior_entropy': 0.0, 'actual_info_gain': 2.584962500721156, 'expected_info_gain': 3.5395454741006125}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('STEAM', '0.000')]
Top entropy choice: 𝙎𝙏𝙀𝘼𝙈 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝙎𝙏𝙀𝘼𝙈 with entropy: 0.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟨🟨⬜🟨🟨, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 8.771 bits, Posterior Entropy: 5.087, 
Guess: 𝙋𝙇𝘼𝙏𝙏, Feedback: ⬜⬜🟨🟨⬜, Prior Entropy: 5.087, Expected Info Gain: 3.540 bits, Actual Info Gain: 2.503 bits, Posterior Entropy: 2.585, 
Guess: 𝘿𝙀𝙈𝙊𝙉, Feedback: ⬜🟨🟨⬜⬜, Prior Entropy: 2.585, Expected Info Gain: 3.540 bits, Actual Info Gain: 2.585 bits, Posterior Entropy: 0.000, 
Guess: 𝙎𝙏𝙀𝘼𝙈, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'STEAM': 0.9664 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙋𝘼𝙍𝙀𝘿 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟩🟩🟩⬜
Actual Info Gain: 8.9518 bits
Posterior entropy: 4.906890595608519
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 4.906890595608519, 'actual_info_gain': 8.951770389114259, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
30 candidate words remaining.
Top 10 guesses: [('DOPER', '2.906'), ('PUDOR', '2.853'), ('DECOR', '2.847'), ('COPED', '2.811'), ('CEDER', '2.776'), ('CODER', '2.769'), ('CHODE', '2.708'), ('DOUCE', '2.694'), ('EPHOD', '2.689'), ('POWND', '2.659')]
Top entropy choice: 𝘿𝙊𝙋𝙀𝙍 with entropy: 2.9064
Guess count low, choosing the word with highest entropy: 𝘿𝙊𝙋𝙀𝙍 with entropy: 2.9064

Guess: 𝘿𝙊𝙋𝙀𝙍, Feedback: 🟨⬜🟨🟩🟨
Actual Info Gain: 4.9069 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 4.906890595608519, 'posterior_entropy': 0.0, 'actual_info_gain': 4.906890595608519, 'expected_info_gain': 2.9063568479084014}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('PARED', '0.000')]
Top entropy choice: 𝙋𝘼𝙍𝙀𝘿 with entropy: 0.0000
Few candidates left, going through them all to pick a common word...
Using common word: 𝙋𝘼𝙍𝙀𝘿 with entropy: 0.0000
You won! Amount of guesses: 3

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟩🟩🟩⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 8.952 bits, Posterior Entropy: 4.907, 
Guess: 𝘿𝙊𝙋𝙀𝙍, Feedback: 🟨⬜🟨🟩🟨, Prior Entropy: 4.907, Expected Info Gain: 2.906 bits, Actual Info Gain: 4.907 bits, Posterior Entropy: 0.000, 
Guess: 𝙋𝘼𝙍𝙀𝘿, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'PARED': 0.9092 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝘾𝘼𝙉𝙊𝙀 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟩⬜🟨⬜
Actual Info Gain: 7.0773 bits
Posterior entropy: 6.78135971352466
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 6.78135971352466, 'actual_info_gain': 7.077301271198117, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
110 candidate words remaining.
Top 10 guesses: [('CLING', '4.230'), ('BLIND', '4.090'), ('CHILD', '4.083'), ('CLINE', '4.048'), ('DINLO', '4.036'), ('LUNGI', '4.014'), ('CLIMB', '4.012'), ('COLIN', '4.001'), ('DOLCI', '3.991'), ('LUCID', '3.987')]
Top entropy choice: 𝘾𝙇𝙄𝙉𝙂 with entropy: 4.2296
Guess count low, choosing the word with highest entropy: 𝘾𝙇𝙄𝙉𝙂 with entropy: 4.2296

Guess: 𝘾𝙇𝙄𝙉𝙂, Feedback: 🟩⬜⬜🟨⬜
Actual Info Gain: 6.7814 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 6.78135971352466, 'posterior_entropy': 0.0, 'actual_info_gain': 6.78135971352466, 'expected_info_gain': 4.229577516458548}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('CANOE', '0.000')]
Top entropy choice: 𝘾𝘼𝙉𝙊𝙀 with entropy: 0.0000
Few candidates left, going through them all to pick a common word...
Using common word: 𝘾𝘼𝙉𝙊𝙀 with entropy: 0.0000
You won! Amount of guesses: 3

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟩⬜🟨⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 7.077 bits, Posterior Entropy: 6.781, 
Guess: 𝘾𝙇𝙄𝙉𝙂, Feedback: 🟩⬜⬜🟨⬜, Prior Entropy: 6.781, Expected Info Gain: 4.230 bits, Actual Info Gain: 6.781 bits, Posterior Entropy: 0.000, 
Guess: 𝘾𝘼𝙉𝙊𝙀, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'CANOE': 3.4775 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙈𝙊𝙐𝙏𝙃 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟨⬜⬜⬜⬜
Actual Info Gain: 5.5779 bits
Posterior entropy: 8.280770770130603
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 8.280770770130603, 'actual_info_gain': 5.577890214592173, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
311 candidate words remaining.
Top 10 guesses: [('COLIN', '5.317'), ('CUNIT', '5.247'), ('PIONY', '5.209'), ('COUNT', '5.200'), ('NICOL', '5.186'), ('NITTO', '5.181'), ('PITOT', '5.173'), ('LITHO', '5.162'), ('PILOT', '5.155'), ('POTIN', '5.150')]
Top entropy choice: 𝘾𝙊𝙇𝙄𝙉 with entropy: 5.3169
Guess count low, choosing the word with highest entropy: 𝘾𝙊𝙇𝙄𝙉 with entropy: 5.3169

Guess: 𝘾𝙊𝙇𝙄𝙉, Feedback: ⬜🟩⬜⬜⬜
Actual Info Gain: 3.2808 bits
Posterior entropy: 5.0
entropy_info: {'prior_entropy': 8.280770770130603, 'posterior_entropy': 5.0, 'actual_info_gain': 3.2807707701306033, 'expected_info_gain': 5.316896712755155}
----------
The bot is making a guess...
32 candidate words remaining.
Top 10 guesses: [('HUMPY', '3.948'), ('BUTTY', '3.762'), ('POOHY', '3.757'), ('BUMPH', '3.742'), ('BOOTH', '3.679'), ('BOMOH', '3.668'), ('PHOTY', '3.656'), ('BUMPY', '3.636'), ('PUTTY', '3.614'), ('UMPTY', '3.608')]
Top entropy choice: 𝙃𝙐𝙈𝙋𝙔 with entropy: 3.9484
Using common exploratory word: 𝘽𝙊𝙊𝙏𝙃 with entropy: 3.6792

Guess: 𝘽𝙊𝙊𝙏𝙃, Feedback: ⬜🟩⬜🟩🟩
Actual Info Gain: 3.0000 bits
Posterior entropy: 2.0
entropy_info: {'prior_entropy': 5.0, 'posterior_entropy': 2.0, 'actual_info_gain': 3.0, 'expected_info_gain': 5.316896712755155}
----------
The bot is making a guess...
4 candidate words remaining.
Top 10 guesses: [('ALUMY', '2.000'), ('AWMRY', '2.000'), ('BUFFY', '2.000'), ('BUFTY', '2.000'), ('BUMFS', '2.000'), ('BUMPY', '2.000'), ('DUMKY', '2.000'), ('DUMMY', '2.000'), ('DUMPY', '2.000'), ('DWAMY', '2.000')]
Top entropy choice: 𝘼𝙇𝙐𝙈𝙔 with entropy: 2.0000
Guess count high, choosing a candidate with the highest entropy: 𝙈𝙊𝙐𝙏𝙃 with entropy: 1.5000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟨⬜⬜⬜⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 5.578 bits, Posterior Entropy: 8.281, 
Guess: 𝘾𝙊𝙇𝙄𝙉, Feedback: ⬜🟩⬜⬜⬜, Prior Entropy: 8.281, Expected Info Gain: 5.317 bits, Actual Info Gain: 3.281 bits, Posterior Entropy: 5.000, 
Guess: 𝘽𝙊𝙊𝙏𝙃, Feedback: ⬜🟩⬜🟩🟩, Prior Entropy: 5.000, Expected Info Gain: 5.317 bits, Actual Info Gain: 3.000 bits, Posterior Entropy: 2.000, 
Guess: 𝙈𝙊𝙐𝙏𝙃, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'MOUTH': 10.4914 seconds

----------------------------------------------------------------------------------------------------
___ Testing word: 𝙏𝙊𝙋𝘼𝙕 ___

The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟩🟨⬜⬜⬜
Actual Info Gain: 8.5367 bits
Posterior entropy: 5.321928094887363
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 5.321928094887363, 'actual_info_gain': 8.536732889835413, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
40 candidate words remaining.
Top 10 guesses: [('HINKY', '4.196'), ('LINKY', '4.194'), ('COLIN', '4.125'), ('NIKAH', '4.125'), ('KYLIN', '4.106'), ('WINKY', '4.044'), ('LINCH', '4.034'), ('PILON', '4.028'), ('KINDA', '4.022'), ('GONIA', '4.006')]
Top entropy choice: 𝙃𝙄𝙉𝙆𝙔 with entropy: 4.1964
Guess count low, choosing the word with highest entropy: 𝙃𝙄𝙉𝙆𝙔 with entropy: 4.1964

Guess: 𝙃𝙄𝙉𝙆𝙔, Feedback: ⬜⬜⬜⬜⬜
Actual Info Gain: 3.3219 bits
Posterior entropy: 2.0
entropy_info: {'prior_entropy': 5.321928094887363, 'posterior_entropy': 2.0, 'actual_info_gain': 3.3219280948873626, 'expected_info_gain': 4.196439344671015}
----------
The bot is making a guess...
4 candidate words remaining.
Top 10 guesses: [('ABLED', '2.000'), ('ABLER', '2.000'), ('ABLES', '2.000'), ('ABLET', '2.000'), ('ABLOW', '2.000'), ('ABOIL', '2.000'), ('ABUZZ', '2.000'), ('ACOEL', '2.000'), ('ADLIB', '2.000'), ('ADOPT', '2.000')]
Top entropy choice: 𝘼𝘽𝙇𝙀𝘿 with entropy: 2.0000
Using common exploratory word: 𝘼𝘿𝙊𝙋𝙏 with entropy: 2.0000

Guess: 𝘼𝘿𝙊𝙋𝙏, Feedback: 🟨⬜🟨🟨🟨
Actual Info Gain: 2.0000 bits
Posterior entropy: 0.0
entropy_info: {'prior_entropy': 2.0, 'posterior_entropy': 0.0, 'actual_info_gain': 2.0, 'expected_info_gain': 4.196439344671015}
----------
The bot is making a guess...
1 candidate words remaining.
Top 10 guesses: [('TOPAZ', '0.000')]
Top entropy choice: 𝙏𝙊𝙋𝘼𝙕 with entropy: 0.0000
Guess count high, choosing a candidate with the highest entropy: 𝙏𝙊𝙋𝘼𝙕 with entropy: 0.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: 🟩🟨⬜⬜⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 8.537 bits, Posterior Entropy: 5.322, 
Guess: 𝙃𝙄𝙉𝙆𝙔, Feedback: ⬜⬜⬜⬜⬜, Prior Entropy: 5.322, Expected Info Gain: 4.196 bits, Actual Info Gain: 3.322 bits, Posterior Entropy: 2.000, 
Guess: 𝘼𝘿𝙊𝙋𝙏, Feedback: 🟨⬜🟨🟨🟨, Prior Entropy: 2.000, Expected Info Gain: 4.196 bits, Actual Info Gain: 2.000 bits, Posterior Entropy: 0.000, 
Guess: 𝙏𝙊𝙋𝘼𝙕, Feedback: 🟩🟩🟩🟩🟩
===================================

Time taken for 'TOPAZ': 1.3935 seconds

Total time for 3000 words: 1191.78 seconds
Average time per word: 0.3906 seconds
Testing bot class: NonGreedyCachedEntropyWordleBot complete.
