Testing bot: WordleBot
___ Testing word: 𝘽𝘼𝙍𝙍𝙀 ___

First Starting word: 'TARES' (Best starting word for catching common words)
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟩🟩🟨⬜
entropy_info: None
----------
The bot originally chose: BARGE 
The top ten guesses and scores: [('BARGE', 102), ('GARBE', 102), ('LARGE', 101), ('PARGE', 100), ('CARLE', 99), ('PARLE', 99), ('BARDE', 98), ('CARPE', 98), ('GARDE', 98), ('MARGE', 98)]
The bot finally chose: BARGE 
Guess: 𝘽𝘼𝙍𝙂𝙀, Feedback: 🟩🟩🟩⬜🟩
entropy_info: None
----------
The bot originally chose: BARDE 
The top ten guesses and scores: [('BARDE', 19), ('BARYE', 19), ('BARBE', 18), ('BARRE', 18)]
The bot finally chose: BARRE 
You won! Amount of guesses: 3

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟩🟩🟨⬜
Guess: 𝘽𝘼𝙍𝙂𝙀, Feedback: 🟩🟩🟩⬜🟩
Guess: 𝘽𝘼𝙍𝙍𝙀, Feedback: 🟩🟩🟩🟩🟩
===================================

Testing bot class: WordleBot complete.

----------------------------------------------------------------------------------------------------
Testing bot: EntropyWordleBot
___ Testing word: 𝘽𝘼𝙍𝙍𝙀 ___


The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟩🟩🟨⬜
Actual Info Gain: 9.1038 bits
Posterior entropy: 4.754887502163468
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 4.754887502163468, 'actual_info_gain': 9.103773482559308, 'expected_info_gain': 6.15937645579268}
----------

The bot is making a guess...
Initial guess by the bot: GARBE with entropy: 2.1581
Bot chose: BARGE with entropy: 2.0379
Top ten guesses: [('GARBE', 2.1581047637021826), ('PARGE', 2.056071893251684), ('BARGE', 2.0379154119304594), ('LARGE', 1.9358825414799603), ('GARRE', 1.9177260601587358), ('CARLE', 1.8290858982529976), ('PARLE', 1.8290858982529976), ('GARDE', 1.8109294169317731), ('BARRE', 1.7919245392005054), ('MARGE', 1.7550118241789237)]

Guess: 𝘽𝘼𝙍𝙂𝙀, Feedback: 🟩🟩🟩⬜🟩
Actual Info Gain: 2.7549 bits
Posterior entropy: 2.0
entropy_info: {'prior_entropy': 4.754887502163468, 'posterior_entropy': 2.0, 'actual_info_gain': 2.7548875021634682, 'expected_info_gain': 2.0379154119304594}
----------

The bot is making a guess...
Initial guess by the bot: BARBE with entropy: 0.8113
Bot chose: BARRE with entropy: 0.8113
Top ten guesses: [('BARBE', 0.8112781244591328), ('BARDE', 0.8112781244591328), ('BARRE', 0.8112781244591328), ('BARYE', 0.8112781244591328)]
You won! Amount of guesses: 3

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟩🟩🟨⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 9.104 bits, Posterior Entropy: 4.755, 
Guess: 𝘽𝘼𝙍𝙂𝙀, Feedback: 🟩🟩🟩⬜🟩, Prior Entropy: 4.755, Expected Info Gain: 2.038 bits, Actual Info Gain: 2.755 bits, Posterior Entropy: 2.000, 
Guess: 𝘽𝘼𝙍𝙍𝙀, Feedback: 🟩🟩🟩🟩🟩
===================================

Testing bot class: EntropyWordleBot complete.

----------------------------------------------------------------------------------------------------
Testing bot: FastEntropyWordleBot
___ Testing word: 𝘽𝘼𝙍𝙍𝙀 ___


The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟩🟩🟨⬜
Actual Info Gain: 9.1038 bits
Posterior entropy: 4.754887502163468
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 4.754887502163468, 'actual_info_gain': 9.103773482559308, 'expected_info_gain': 6.15937645579268}
----------

The bot is making a guess...
Initial guess by the bot: GARBE with entropy: 2.1581
Bot chose: BARGE with entropy: 2.0379
Top ten guesses: [('GARBE', 2.1581047637021826), ('PARGE', 2.056071893251684), ('BARGE', 2.0379154119304594), ('LARGE', 1.9358825414799603), ('GARRE', 1.9177260601587358), ('CARLE', 1.8290858982529976), ('PARLE', 1.8290858982529976), ('GARDE', 1.8109294169317731), ('BARRE', 1.7919245392005054), ('MARGE', 1.7550118241789237)]

Guess: 𝘽𝘼𝙍𝙂𝙀, Feedback: 🟩🟩🟩⬜🟩
Actual Info Gain: 2.7549 bits
Posterior entropy: 2.0
entropy_info: {'prior_entropy': 4.754887502163468, 'posterior_entropy': 2.0, 'actual_info_gain': 2.7548875021634682, 'expected_info_gain': 2.0379154119304594}
----------

The bot is making a guess...
Initial guess by the bot: BARBE with entropy: 0.8113
Bot chose: BARRE with entropy: 0.8113
Top ten guesses: [('BARBE', 0.8112781244591328), ('BARDE', 0.8112781244591328), ('BARRE', 0.8112781244591328), ('BARYE', 0.8112781244591328)]
You won! Amount of guesses: 3

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟩🟩🟨⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 9.104 bits, Posterior Entropy: 4.755, 
Guess: 𝘽𝘼𝙍𝙂𝙀, Feedback: 🟩🟩🟩⬜🟩, Prior Entropy: 4.755, Expected Info Gain: 2.038 bits, Actual Info Gain: 2.755 bits, Posterior Entropy: 2.000, 
Guess: 𝘽𝘼𝙍𝙍𝙀, Feedback: 🟩🟩🟩🟩🟩
===================================

Testing bot class: FastEntropyWordleBot complete.

----------------------------------------------------------------------------------------------------
Testing bot: CachedEntropyWordleBot
___ Testing word: 𝘽𝘼𝙍𝙍𝙀 ___

Loading letter frequencies...
Loaded letter frequency cache with 26 letters
The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟩🟩🟨⬜
Actual Info Gain: 9.1038 bits
Posterior entropy: 4.754887502163468
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 4.754887502163468, 'actual_info_gain': 9.103773482559308, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
Evaluating 27 potential guesses from 27 remaining candidates...
Top 5 guesses: [('GARBE', '2.158'), ('PARGE', '2.056'), ('BARGE', '2.038'), ('LARGE', '1.936'), ('GARRE', '1.918')]
Top entropy choice: 𝙂𝘼𝙍𝘽𝙀 with entropy: 2.1581
Ended up choosing COMMON word: 𝘽𝘼𝙍𝙂𝙀 with entropy: 2.0379

Guess: 𝘽𝘼𝙍𝙂𝙀, Feedback: 🟩🟩🟩⬜🟩
Actual Info Gain: 2.7549 bits
Posterior entropy: 2.0
entropy_info: {'prior_entropy': 4.754887502163468, 'posterior_entropy': 2.0, 'actual_info_gain': 2.7548875021634682, 'expected_info_gain': 2.0379154119304594}
----------
The bot is making a guess...
Evaluating 4 potential guesses from 4 remaining candidates...
Top 5 guesses: [('BARBE', '0.811'), ('BARDE', '0.811'), ('BARRE', '0.811'), ('BARYE', '0.811')]
Top entropy choice: 𝘽𝘼𝙍𝘽𝙀 with entropy: 0.8113
Ended up choosing COMMON word: 𝘽𝘼𝙍𝙍𝙀 with entropy: 0.8113
You won! Amount of guesses: 3

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟩🟩🟨⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 9.104 bits, Posterior Entropy: 4.755, 
Guess: 𝘽𝘼𝙍𝙂𝙀, Feedback: 🟩🟩🟩⬜🟩, Prior Entropy: 4.755, Expected Info Gain: 2.038 bits, Actual Info Gain: 2.755 bits, Posterior Entropy: 2.000, 
Guess: 𝘽𝘼𝙍𝙍𝙀, Feedback: 🟩🟩🟩🟩🟩
===================================

Testing bot class: CachedEntropyWordleBot complete.

----------------------------------------------------------------------------------------------------
Testing bot: NonGreedyCachedEntropyWordleBot
___ Testing word: 𝘽𝘼𝙍𝙍𝙀 ___

Loading letter frequencies...
Loaded letter frequency cache with 26 letters
The bot is making a guess...
First Starting word: TARES (Best starting word for catching common words)

Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟩🟩🟨⬜
Actual Info Gain: 9.1038 bits
Posterior entropy: 4.754887502163468
entropy_info: {'prior_entropy': 13.858660984722777, 'posterior_entropy': 4.754887502163468, 'actual_info_gain': 9.103773482559308, 'expected_info_gain': 6.15937645579268}
----------
The bot is making a guess...
THe possible candidates are: ['BARBE', 'BARDE', 'BARGE', 'BARRE', 'BARYE', 'CARLE', 'CARNE', 'CARPE', 'CARVE', 'DARRE', 'EARLY', 'FARCE', 'FARLE', 'GARBE', 'GARDE', 'GARRE', 'LARGE', 'MARAE', 'MARGE', 'MARLE', 'NARRE', 'PARAE', 'PARGE', 'PARLE', 'PARVE', 'VARVE', 'WARRE']
27 candidate words remaining.
Top 10 guesses: [('PYGAL', '2.918'), ('GILPY', '2.845'), ('GLYPH', '2.845'), ('GULPY', '2.845'), ('GLEBY', '2.827'), ('GLOBY', '2.827'), ('GLAMP', '2.817'), ('PLAGA', '2.799'), ('GULAB', '2.750'), ('BILGY', '2.725')]
Top entropy choice: 𝙋𝙔𝙂𝘼𝙇 with entropy: 2.9185
Guess count low, choosing the word with highest entropy: 𝙋𝙔𝙂𝘼𝙇 with entropy: 2.9185

Guess: 𝙋𝙔𝙂𝘼𝙇, Feedback: ⬜⬜⬜🟨⬜
Actual Info Gain: 1.4330 bits
Posterior entropy: 3.321928094887362
entropy_info: {'prior_entropy': 4.754887502163468, 'posterior_entropy': 3.321928094887362, 'actual_info_gain': 1.432959407276106, 'expected_info_gain': 2.918450134553325}
----------
The bot is making a guess...
THe possible candidates are: ['BARBE', 'BARDE', 'BARRE', 'CARNE', 'CARVE', 'DARRE', 'FARCE', 'NARRE', 'VARVE', 'WARRE']
10 candidate words remaining.
Top 10 guesses: [('CANDO', '2.522'), ('CANDY', '2.522'), ('CNIDA', '2.522'), ('CONDO', '2.522'), ('CUNDY', '2.522'), ('DANCE', '2.522'), ('DANCY', '2.522'), ('DENCH', '2.522'), ('DUNCE', '2.522'), ('DUNCH', '2.522')]
Top entropy choice: 𝘾𝘼𝙉𝘿𝙊 with entropy: 2.5219
Using common exploratory word: 𝘾𝘼𝙉𝘿𝙔 with entropy: 2.5219

Guess: 𝘾𝘼𝙉𝘿𝙔, Feedback: ⬜🟩⬜⬜⬜
Actual Info Gain: 1.3219 bits
Posterior entropy: 2.0
entropy_info: {'prior_entropy': 3.321928094887362, 'posterior_entropy': 2.0, 'actual_info_gain': 1.3219280948873622, 'expected_info_gain': 2.918450134553325}
----------
The bot is making a guess...
THe possible candidates are: ['BARBE', 'BARRE', 'VARVE', 'WARRE']
4 candidate words remaining.
Top 10 guesses: [('ABERS', '2.000'), ('ABORD', '2.000'), ('ABORE', '2.000'), ('ABORN', '2.000'), ('ABORT', '2.000'), ('ABURA', '2.000'), ('ABURN', '2.000'), ('ACERB', '2.000'), ('ADORB', '2.000'), ('AMBRY', '2.000')]
Top entropy choice: 𝘼𝘽𝙀𝙍𝙎 with entropy: 2.0000
Guess count high, choosing a candidate with the highest entropy: 𝘽𝘼𝙍𝙍𝙀 with entropy: 2.0000
You won! Amount of guesses: 4

===================================
History:
Guess: 𝙏𝘼𝙍𝙀𝙎, Feedback: ⬜🟩🟩🟨⬜, Prior Entropy: 13.859, Expected Info Gain: 6.159 bits, Actual Info Gain: 9.104 bits, Posterior Entropy: 4.755, 
Guess: 𝙋𝙔𝙂𝘼𝙇, Feedback: ⬜⬜⬜🟨⬜, Prior Entropy: 4.755, Expected Info Gain: 2.918 bits, Actual Info Gain: 1.433 bits, Posterior Entropy: 3.322, 
Guess: 𝘾𝘼𝙉𝘿𝙔, Feedback: ⬜🟩⬜⬜⬜, Prior Entropy: 3.322, Expected Info Gain: 2.918 bits, Actual Info Gain: 1.322 bits, Posterior Entropy: 2.000, 
Guess: 𝘽𝘼𝙍𝙍𝙀, Feedback: 🟩🟩🟩🟩🟩
===================================

Testing bot class: NonGreedyCachedEntropyWordleBot complete.

----------------------------------------------------------------------------------------------------
